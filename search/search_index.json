{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The BERBL library This is an implementation of Drugowitsch's Bayesian Learning Classifier System 1 2 . BERBL stands for Bayesian Evolutionary Rule-based Learner. Note that documentation for this library is still underway. If you have any questions, feel free to open an issue . Usage examples You could use defaults everywhere but in the population size and the number of iterations to run: import numpy as np # type: ignore from berbl import BERBL from berbl.search.operators.drugowitsch import DefaultToolbox from sklearn.compose import TransformedTargetRegressor # type: ignore from sklearn.metrics import mean_absolute_error # type: ignore from sklearn.pipeline import make_pipeline # type: ignore from sklearn.preprocessing import StandardScaler # type: ignore from sklearn.utils import check_random_state # type: ignore # Generate some data for the identity function. X = np . arange ( 300 ) . reshape (( - 1 , 1 )) y = np . arange ( 300 ) . reshape (( - 1 , 1 )) random_state = check_random_state ( 2 ) # Initialize toolbox, let's assume that a population size of 10 suffices for # this task. toolbox = DefaultToolbox ( n = 10 , random_state = random_state ) # Instantiate BERBL. regressor = BERBL ( toolbox = toolbox , n_iter = 20 ) # Next, just some standard scikit-learn stuff: Create a pipeline that # standardizes inputs. pipe = make_pipeline ( StandardScaler (), TransformedTargetRegressor ( regressor = regressor , transformer = StandardScaler ())) # Fit the pipeline. estimator = pipe . fit ( X , y ) # Make predictions (here, on the training data for simplicities sake). y_pred = estimator . predict ( X ) # Get some metrics. print ( \"MAE on training data: \" , mean_absolute_error ( y , y_pred )) You can also override certain operators of the evolutionary algorithm similarly to the DEAP API : import numpy as np # type: ignore from berbl import BERBL from berbl.search.operators.drugowitsch import DefaultToolbox from sklearn.compose import TransformedTargetRegressor # type: ignore from sklearn.metrics import mean_absolute_error # type: ignore from sklearn.pipeline import make_pipeline # type: ignore from sklearn.preprocessing import StandardScaler # type: ignore from sklearn.utils import check_random_state # type: ignore # Generate some data for the identity function. X = np . arange ( 300 ) . reshape (( - 1 , 1 )) y = np . arange ( 300 ) . reshape (( - 1 , 1 )) random_state = check_random_state ( 2 ) # Initialize toolbox, let's assume that a population size of 10 suffices for # this task. toolbox = DefaultToolbox ( n = 10 , random_state = random_state ) # Let's assume we want to use a custom mutate that does nothing (i.e. disable # mutation in a slightly awkward fashion). def custom_mutate ( genotype , random_state ): return genotype # Override mutation operator in toolbox with our custom one. toolbox . register ( \"mutate\" , custom_mutate ) # Instantiate BERBL. regressor = BERBL ( toolbox = toolbox , n_iter = 20 ) # Next, just some standard scikit-learn stuff: Create a pipeline that # standardizes inputs. pipe = make_pipeline ( StandardScaler (), TransformedTargetRegressor ( regressor = regressor , transformer = StandardScaler ())) # Fit the pipeline. estimator = pipe . fit ( X , y ) # Make predictions (here, on the training data for simplicities sake). y_pred = estimator . predict ( X ) # Get some metrics. print ( \"MAE on training data: \" , mean_absolute_error ( y , y_pred )) Note on nomenclature In the implementation we try to avoid the overloaded term classifier and instead use rule . A rule consists of a matching function and a (local) submodel . In addition, there is a mixing weight associated with each rule that comes in to play when rules overlap (i.e. when an input is matched by the matching functions of more than one rule). Jan Drugowitsch. 2007. Learning Classifier Systems from first principles. PDF . \u21a9 Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. PDF . \u21a9","title":"The BERBL library"},{"location":"#the-berbl-library","text":"This is an implementation of Drugowitsch's Bayesian Learning Classifier System 1 2 . BERBL stands for Bayesian Evolutionary Rule-based Learner. Note that documentation for this library is still underway. If you have any questions, feel free to open an issue .","title":"The BERBL library"},{"location":"#usage-examples","text":"You could use defaults everywhere but in the population size and the number of iterations to run: import numpy as np # type: ignore from berbl import BERBL from berbl.search.operators.drugowitsch import DefaultToolbox from sklearn.compose import TransformedTargetRegressor # type: ignore from sklearn.metrics import mean_absolute_error # type: ignore from sklearn.pipeline import make_pipeline # type: ignore from sklearn.preprocessing import StandardScaler # type: ignore from sklearn.utils import check_random_state # type: ignore # Generate some data for the identity function. X = np . arange ( 300 ) . reshape (( - 1 , 1 )) y = np . arange ( 300 ) . reshape (( - 1 , 1 )) random_state = check_random_state ( 2 ) # Initialize toolbox, let's assume that a population size of 10 suffices for # this task. toolbox = DefaultToolbox ( n = 10 , random_state = random_state ) # Instantiate BERBL. regressor = BERBL ( toolbox = toolbox , n_iter = 20 ) # Next, just some standard scikit-learn stuff: Create a pipeline that # standardizes inputs. pipe = make_pipeline ( StandardScaler (), TransformedTargetRegressor ( regressor = regressor , transformer = StandardScaler ())) # Fit the pipeline. estimator = pipe . fit ( X , y ) # Make predictions (here, on the training data for simplicities sake). y_pred = estimator . predict ( X ) # Get some metrics. print ( \"MAE on training data: \" , mean_absolute_error ( y , y_pred )) You can also override certain operators of the evolutionary algorithm similarly to the DEAP API : import numpy as np # type: ignore from berbl import BERBL from berbl.search.operators.drugowitsch import DefaultToolbox from sklearn.compose import TransformedTargetRegressor # type: ignore from sklearn.metrics import mean_absolute_error # type: ignore from sklearn.pipeline import make_pipeline # type: ignore from sklearn.preprocessing import StandardScaler # type: ignore from sklearn.utils import check_random_state # type: ignore # Generate some data for the identity function. X = np . arange ( 300 ) . reshape (( - 1 , 1 )) y = np . arange ( 300 ) . reshape (( - 1 , 1 )) random_state = check_random_state ( 2 ) # Initialize toolbox, let's assume that a population size of 10 suffices for # this task. toolbox = DefaultToolbox ( n = 10 , random_state = random_state ) # Let's assume we want to use a custom mutate that does nothing (i.e. disable # mutation in a slightly awkward fashion). def custom_mutate ( genotype , random_state ): return genotype # Override mutation operator in toolbox with our custom one. toolbox . register ( \"mutate\" , custom_mutate ) # Instantiate BERBL. regressor = BERBL ( toolbox = toolbox , n_iter = 20 ) # Next, just some standard scikit-learn stuff: Create a pipeline that # standardizes inputs. pipe = make_pipeline ( StandardScaler (), TransformedTargetRegressor ( regressor = regressor , transformer = StandardScaler ())) # Fit the pipeline. estimator = pipe . fit ( X , y ) # Make predictions (here, on the training data for simplicities sake). y_pred = estimator . predict ( X ) # Get some metrics. print ( \"MAE on training data: \" , mean_absolute_error ( y , y_pred ))","title":"Usage examples"},{"location":"#note-on-nomenclature","text":"In the implementation we try to avoid the overloaded term classifier and instead use rule . A rule consists of a matching function and a (local) submodel . In addition, there is a mixing weight associated with each rule that comes in to play when rules overlap (i.e. when an input is matched by the matching functions of more than one rule). Jan Drugowitsch. 2007. Learning Classifier Systems from first principles. PDF . \u21a9 Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. PDF . \u21a9","title":"Note on nomenclature"},{"location":"reference/SUMMARY/","text":"berbl literal hyperparams model match allmatch init nomatch radial1d_drugowitsch softinterval1d_drugowitsch mixing mixing_laplace mixture rule search ga drugowitsch operators drugowitsch utils","title":"SUMMARY"},{"location":"reference/berbl/","text":"BERBL ( toolbox = DefaultToolbox ( random_state = None ), search = 'drugowitsch' , n_iter = 100 ) Bases: BaseEstimator , RegressorMixin An implementation of the Bayesian Learning Classifier System. Based on (but also extending) the book \u2018Design and Analysis of Learning Classifier Systems \u2013 A Probabilistic Approach\u2019 by Jan Drugowitsch. Follows the scikit-learn estimator pattern . Parameters: Name Type Description Default toolbox object A DEAP Toolbox object that specifies all the operators required by the selected search algorithm ( search parameter). By default, DefaultToolbox(random_state=None) . DefaultToolbox(random_state=None) search str Which search algorithm to use to perform model selection. Also see toolbox parameter. For now, only 'drugowitsch' (denoting the simplistic genetic algorithm from Drugowitsch's book ) is supported. 'drugowitsch' n_iter int Number of iterations to run the search. 100 Source code in berbl/__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , toolbox = DefaultToolbox ( random_state = None ), search = \"drugowitsch\" , n_iter = 100 ): \"\"\" Parameters ---------- toolbox : object A DEAP ``Toolbox`` object that specifies all the operators required by the selected search algorithm (``search`` parameter). By default, [``DefaultToolbox(random_state=None)``](search/operators/#berbl.search.operators.drugowitsch.DefaultToolbox). search : str Which search algorithm to use to perform model selection. Also see ``toolbox`` parameter. For now, only ``'drugowitsch'`` (denoting the simplistic genetic algorithm from [Drugowitsch's book](/)) is supported. n_iter : int Number of iterations to run the search. \"\"\" self . toolbox = toolbox self . search = search self . n_iter = n_iter fit ( X , y ) Fit BERBL to the data. Note: Input to this function (each of X and y ) is assumed to be standardized. Parameters: Name Type Description Default X array of shape (N, DX) Training data. required y : array of shape (N, Dy) Target values. Returns: Name Type Description self object Fitted estimator. Source code in berbl/__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def fit ( self , X , y ): \"\"\" Fit BERBL to the data. Note: Input to this function (each of ``X`` and ``y``) is assumed to be standardized. Parameters ---------- X : array of shape (N, DX) Training data. y : array of shape (N, Dy) Target values. Returns ------- self : object Fitted estimator. \"\"\" # TODO Link to sklearn Sphinx-generated inventory # See SLEP010. X , y = self . _validate_data ( X , y , multi_output = True ) searchcls = search_methods [ self . search ] self . search_ = searchcls ( self . toolbox , n_iter = self . n_iter , random_state = randseed ( self . toolbox . random_state )) self . search_ = self . search_ . fit ( X , y ) return self predict ( X ) Predict using BERBL. Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Name Type Description y array of shape (n, Dy) Predictions for the inputs (i.e. BERBL's predicitive distributions' means). Source code in berbl/__init__.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def predict ( self , X ): \"\"\" Predict using BERBL. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- y : array of shape (n, Dy) Predictions for the inputs (i.e. BERBL's predicitive distributions' means). \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predict ( X ) predict_distribution ( x ) The predictive distribution for the given input (i.e. the distribution over all outputs for that input). Parameters: Name Type Description Default x array of shape (DX,) The input to compute the predictive distribution for. required Returns: Name Type Description pdf callable A callable that, when given a possible output value y (an array of shape (Dy, ) ) returns the value of the predictive distribution at that point. Source code in berbl/__init__.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def predict_distribution ( self , x ): \"\"\" The predictive distribution for the given input (i.e. the distribution over all outputs for that input). Parameters ---------- x : array of shape (DX,) The input to compute the predictive distribution for. Returns ------- pdf : callable A callable that, when given a possible output value ``y`` (an array of shape ``(Dy, )``) returns the value of the predictive distribution at that point. \"\"\" # TODO Properly validate input here check_is_fitted ( self ) return self . search_ . predict_distribution ( x ) predict_mean_var ( X ) Get the mean and variance of BERBL's predictive density for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d 1 Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. \u21a9 Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Type Description y , y_var Means and variances. Source code in berbl/__init__.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def predict_mean_var ( self , X ): \"\"\" Get the mean and variance of BERBL's predictive density for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d[^1] [^1]: Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- y, y_var : tuple of two arrays of shape (N, Dy) Means and variances. \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predict_mean_var ( X ) predicts ( X ) Get each submodel's predictions, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/__init__.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def predicts ( self , X ): \"\"\" Get each submodel's predictions, one by one, without mixing them. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predicts ( X )","title":"berbl"},{"location":"reference/berbl/#berbl.BERBL","text":"Bases: BaseEstimator , RegressorMixin An implementation of the Bayesian Learning Classifier System. Based on (but also extending) the book \u2018Design and Analysis of Learning Classifier Systems \u2013 A Probabilistic Approach\u2019 by Jan Drugowitsch. Follows the scikit-learn estimator pattern . Parameters: Name Type Description Default toolbox object A DEAP Toolbox object that specifies all the operators required by the selected search algorithm ( search parameter). By default, DefaultToolbox(random_state=None) . DefaultToolbox(random_state=None) search str Which search algorithm to use to perform model selection. Also see toolbox parameter. For now, only 'drugowitsch' (denoting the simplistic genetic algorithm from Drugowitsch's book ) is supported. 'drugowitsch' n_iter int Number of iterations to run the search. 100 Source code in berbl/__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , toolbox = DefaultToolbox ( random_state = None ), search = \"drugowitsch\" , n_iter = 100 ): \"\"\" Parameters ---------- toolbox : object A DEAP ``Toolbox`` object that specifies all the operators required by the selected search algorithm (``search`` parameter). By default, [``DefaultToolbox(random_state=None)``](search/operators/#berbl.search.operators.drugowitsch.DefaultToolbox). search : str Which search algorithm to use to perform model selection. Also see ``toolbox`` parameter. For now, only ``'drugowitsch'`` (denoting the simplistic genetic algorithm from [Drugowitsch's book](/)) is supported. n_iter : int Number of iterations to run the search. \"\"\" self . toolbox = toolbox self . search = search self . n_iter = n_iter","title":"BERBL"},{"location":"reference/berbl/#berbl.BERBL.fit","text":"Fit BERBL to the data. Note: Input to this function (each of X and y ) is assumed to be standardized. Parameters: Name Type Description Default X array of shape (N, DX) Training data. required y : array of shape (N, Dy) Target values. Returns: Name Type Description self object Fitted estimator. Source code in berbl/__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def fit ( self , X , y ): \"\"\" Fit BERBL to the data. Note: Input to this function (each of ``X`` and ``y``) is assumed to be standardized. Parameters ---------- X : array of shape (N, DX) Training data. y : array of shape (N, Dy) Target values. Returns ------- self : object Fitted estimator. \"\"\" # TODO Link to sklearn Sphinx-generated inventory # See SLEP010. X , y = self . _validate_data ( X , y , multi_output = True ) searchcls = search_methods [ self . search ] self . search_ = searchcls ( self . toolbox , n_iter = self . n_iter , random_state = randseed ( self . toolbox . random_state )) self . search_ = self . search_ . fit ( X , y ) return self","title":"fit()"},{"location":"reference/berbl/#berbl.BERBL.predict","text":"Predict using BERBL. Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Name Type Description y array of shape (n, Dy) Predictions for the inputs (i.e. BERBL's predicitive distributions' means). Source code in berbl/__init__.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def predict ( self , X ): \"\"\" Predict using BERBL. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- y : array of shape (n, Dy) Predictions for the inputs (i.e. BERBL's predicitive distributions' means). \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predict ( X )","title":"predict()"},{"location":"reference/berbl/#berbl.BERBL.predict_distribution","text":"The predictive distribution for the given input (i.e. the distribution over all outputs for that input). Parameters: Name Type Description Default x array of shape (DX,) The input to compute the predictive distribution for. required Returns: Name Type Description pdf callable A callable that, when given a possible output value y (an array of shape (Dy, ) ) returns the value of the predictive distribution at that point. Source code in berbl/__init__.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def predict_distribution ( self , x ): \"\"\" The predictive distribution for the given input (i.e. the distribution over all outputs for that input). Parameters ---------- x : array of shape (DX,) The input to compute the predictive distribution for. Returns ------- pdf : callable A callable that, when given a possible output value ``y`` (an array of shape ``(Dy, )``) returns the value of the predictive distribution at that point. \"\"\" # TODO Properly validate input here check_is_fitted ( self ) return self . search_ . predict_distribution ( x )","title":"predict_distribution()"},{"location":"reference/berbl/#berbl.BERBL.predict_mean_var","text":"Get the mean and variance of BERBL's predictive density for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d 1 Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. \u21a9 Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Type Description y , y_var Means and variances. Source code in berbl/__init__.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def predict_mean_var ( self , X ): \"\"\" Get the mean and variance of BERBL's predictive density for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d[^1] [^1]: Jan Drugowitsch. 2008. Design and Analysis of Learning Classifier Systems - A Probabilistic Approach. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- y, y_var : tuple of two arrays of shape (N, Dy) Means and variances. \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predict_mean_var ( X )","title":"predict_mean_var()"},{"location":"reference/berbl/#berbl.BERBL.predicts","text":"Get each submodel's predictions, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (n, DX) Inputs to make predictions for. required Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/__init__.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def predicts ( self , X ): \"\"\" Get each submodel's predictions, one by one, without mixing them. Parameters ---------- X : array of shape (n, DX) Inputs to make predictions for. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" # See SLEP010. X = self . _validate_data ( X , reset = False ) check_is_fitted ( self ) return self . search_ . predicts ( X )","title":"predicts()"},{"location":"reference/berbl/mixing/","text":"Mixing ( rules , phi , random_state , A_BETA = 10 ** - 2 , B_BETA = 10 ** - 4 , DELTA_S_L_M_Q = 10 ** - 2 , MAX_ITER_MIXING = 40 , EXP_MIN = np . log ( np . finfo ( None ) . tiny ), LN_MAX = np . log ( np . finfo ( None ) . max ), ** kwargs ) Model for the mixing weights of a set of linear regression rules. Parameters: Name Type Description Default rules list of rule object List of rules (which are held fixed during mixing training). required phi callable Mixing feature function taking input matrices of shape (N, DX) and returning mixing feature matrices of shape (n, V). If None use the LCS default of phi(x) = 1 . required random_state RandomState object required A_BETA float Scale parameter of mixing weight vector variance prior. 10 ** -2 B_BETA float Shape parameter of mixing weight vector variance prior. 10 ** -4 DELTA_S_L_M_Q float Stopping criterion for variational update loop. 10 ** -2 MAX_ITER_MIXING int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). 40 EXP_MIN float Lowest real number x on system such that exp(x) > 0 . The default is the logarithm of the smallest positive number of the default dtype (as of 2020-10-06, this dtype is float64). np.log(np.finfo(None).tiny) LN_MAX float ln(x) , where x is the highest real number on the system. The default is the logarithm of the highest number of the default dtype (as of 2020-10-06, this dtype is float64). np.log(np.finfo(None).max) **kwargs kwargs This is here so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/mixing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , rules , phi , random_state , A_BETA = 10 **- 2 , B_BETA = 10 **- 4 , DELTA_S_L_M_Q = 10 **- 2 , MAX_ITER_MIXING = 40 , EXP_MIN = np . log ( np . finfo ( None ) . tiny ), LN_MAX = np . log ( np . finfo ( None ) . max ), ** kwargs ): \"\"\" Parameters ---------- rules : list of rule object List of rules (which are held fixed during mixing training). phi : callable Mixing feature function taking input matrices of shape (N, DX) and returning mixing feature matrices of shape (n, V). If ``None`` use the LCS default of ``phi(x) = 1``. random_state : RandomState object A_BETA : float Scale parameter of mixing weight vector variance prior. B_BETA : float Shape parameter of mixing weight vector variance prior. DELTA_S_L_M_Q : float Stopping criterion for variational update loop. MAX_ITER_MIXING : int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). EXP_MIN : float Lowest real number ``x`` on system such that ``exp(x) > 0``. The default is the logarithm of the smallest positive number of the default dtype (as of 2020-10-06, this dtype is float64). LN_MAX : float ``ln(x)``, where ``x`` is the highest real number on the system. The default is the logarithm of the highest number of the default dtype (as of 2020-10-06, this dtype is float64). **kwargs : kwargs This is here so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . rules = rules self . phi = phi self . A_BETA = A_BETA self . B_BETA = B_BETA self . DELTA_S_L_M_Q = DELTA_S_L_M_Q self . MAX_ITER_MIXING = MAX_ITER_MIXING self . EXP_MIN = EXP_MIN self . LN_MAX = LN_MAX self . random_state = random_state self . K = len ( self . rules ) _mixing ( M , Phi , V ) [PDF p. 239] Is zero wherever a rule does not match. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required Returns: Name Type Description G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Source code in berbl/mixing.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def _mixing ( self , M : np . ndarray , Phi : np . ndarray , V : np . ndarray ): \"\"\" [PDF p. 239] Is zero wherever a rule does not match. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. Returns ------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. \"\"\" # TODO Use literal.mixing here (remember to provide self.EXP_MIN and # self.LN_MAX, though!) # If Phi is standard, this simply broadcasts V to a matrix [V, V, V, \u2026] # of shape (N, DV). G = Phi @ V # This quasi never happens (at least for the run I checked it did not). # That run also oscillated so this is probably not the source. G = np . clip ( G , self . EXP_MIN , self . LN_MAX - np . log ( self . K )) G = np . exp ( G ) * M # The sum can be 0 meaning we do 0/0 (== NaN) but we ignore it because # it is fixed one line later (this is how Drugowitsch does it). # Drugowitsch does, however, also say that: \u201cUsually, this should never # happen as only model structures are accepted where [(np.sum(G, 1) > # 0).all()]. Nonetheless, this check was added to ensure that even these # cases are handled gracefully.\u201d with np . errstate ( invalid = \"ignore\" ): G = G / np . sum ( G , axis = 1 )[:, np . newaxis ] G = np . nan_to_num ( G , nan = 1 / self . K ) return G _opt_bouchard ( M , Phi , V , alpha , lxi ) Update for the parameters of Bouchard's lower bound. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required alpha array of shape (N, 1) Current value of alpha variational parameters of Bouchard's bound. required lxi array of shape (N, K) Current value of lxi variational parameters of Bouchard's bound. required Returns: Type Description lxi , alpha New values for the variational parameters alpha and lxi . Source code in berbl/mixing.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def _opt_bouchard ( self , M : np . ndarray , Phi : np . ndarray , V , alpha , lxi ): \"\"\" Update for the parameters of Bouchard's lower bound. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. alpha : array of shape (N, 1) Current value of ``alpha`` variational parameters of Bouchard's bound. lxi : array of shape (N, K) Current value of ``lxi`` variational parameters of Bouchard's bound. Returns ------- lxi, alpha : tuple of arrays of shapes (N, 1) and (N, K) New values for the variational parameters ``alpha`` and ``lxi``. \"\"\" N , _ = Phi . shape h = np . log ( M ) + Phi @ V alpha = ( 1 / 2 * ( self . K / 2 - 1 ) + np . sum ( h * lxi , axis = 1 )) / np . sum ( lxi , axis = 1 ) alpha = alpha . reshape (( N , 1 )) xi = np . abs ( alpha - h ) # If ``alpha == h``, then the following contains a division by zero and # then a multiplication by NaN which results in a divide and an invalid # value warning to be thrown. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): lxi = 1 / ( 2 * xi ) * ( 1 / ( 1 + np . exp ( - xi )) - 1 / 2 ) # Where `alpha == h` we get NaN's in the previous formula due to xi = 0 # there. We simply solve that by setting the corresponding entries to # the limit for `x -> 0` of `lambda(x)` which is `0.125`. lxi [ np . where ( np . logical_and ( xi == 0 , np . isnan ( lxi )))] = 0.125 # NOTE Doing this in-place instead of returning values doesn't seem to # result in a significant speedup. We thus opted for the more # descriptive alternative. return alpha , lxi _responsibilities ( X , y , G ) [PDF p. 240] Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required Returns: Name Type Description R array of shape (N, K) Responsibility matrix. Source code in berbl/mixing.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def _responsibilities ( self , X : np . ndarray , y : np . ndarray , G : np . ndarray ): \"\"\" [PDF p. 240] Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Returns ------- R : array of shape (N, K) Responsibility matrix. \"\"\" # NOTE: The code duplication solution used is faster for larger # len(self.rules) than the technically cleaner # W, Lambda_1, a_tau, b_tau = zip( # *[(cl.W_, cl.Lambda_1_, cl.a_tau_, cl.b_tau_) for cl in self.rules] # ) W = [ cl . W_ for cl in self . rules ] Lambda_1 = [ cl . Lambda_1_ for cl in self . rules ] a_tau = [ cl . a_tau_ for cl in self . rules ] b_tau = [ cl . b_tau_ for cl in self . rules ] return responsibilities ( X = X , Y = y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) _train_b_beta ( V , Lambda_V_1 ) [PDF p. 244] TrainMixPriors but only the part concerned with b_beta since a_beta is constant. Parameters: Name Type Description Default V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. required Returns: Name Type Description b_beta array of shape (K,) mixing weight vector prior parameter Source code in berbl/mixing.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def _train_b_beta ( self , V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] TrainMixPriors but only the part concerned with ``b_beta`` since ``a_beta`` is constant. Parameters ---------- V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. Returns ------- b_beta : array of shape (K,) mixing weight vector prior parameter \"\"\" DV , _ = V . shape b_beta = np . repeat ( self . B_BETA , ( self . K , )) Lambda_V_1_diag = np . array ( list ( map ( np . diag , Lambda_V_1 ))) # TODO Performance: LCSBookCode vectorized this: # b[:,1] = b_b + 0.5 * (sum(V * V, 0) + self.cov_Tr) for k in range ( self . K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV # Not that efficient, I think (but very close to [PDF p. 244]). # Lambda_V_1_kk = Lambda_V_1[l:u:1, l:u:1] # b_beta[k] = B_BETA + 0.5 * (np.trace(Lambda_V_1_kk) + v_k.T @ v_k) # More efficient. try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , ( f \"v_k = { v_k } , \" f \"K = { self . K } , \" f \"V = { V } , \" f \"Lambda_V_1 = { Lambda_V_1 } \" ), report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return b_beta _train_mix_weights ( M , X , y , Phi , R , V , a_beta , b_beta , lxi , alpha ) Training routine for mixing weights based on Bouchard's upper bound. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required lxi array of shape (N, K) Parameter of Bouchard's bound. required alpha array of shape (N, 1) Parameter of Bouchard's bound. required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrices. Source code in berbl/mixing.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def _train_mix_weights ( self , M , X , y , Phi , R , V , a_beta , b_beta , lxi , alpha ): \"\"\" Training routine for mixing weights based on Bouchard's upper bound. Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). lxi : array of shape (N, K) Parameter of Bouchard's bound. alpha : array of shape (N, 1) Parameter of Bouchard's bound. Returns ------- V, Lambda_V_1 : tuple of array of shapes (DV, K) and list (length K) of arrays of shape (DV, DV) Updated mixing weight matrix and mixing weight covariance matrices. \"\"\" N , _ = X . shape DV , _ = V . shape E_beta_beta = a_beta / b_beta Lambda_V_1 = [ np . zeros (( DV , DV ))] * self . K Rlxi = R * lxi for k in range ( self . K ): Lambda_V_1 [ k ] = 2 * ( Rlxi [:, [ k ]] . T * Phi . T ) @ Phi + E_beta_beta [ k ] * np . identity ( Lambda_V_1 [ k ] . shape [ 0 ]) t = R [:, [ k ]] * ( 1 / 2 - 2 * np . log ( M [:, [ k ]]) * lxi [:, [ k ]] + alpha * lxi [:, [ k ]]) V [:, [ k ]] = np . linalg . pinv ( Lambda_V_1 [ k ]) @ Phi . T @ t # NOTE Doing this in-place instead of returning values doesn't seem to # result in a significant speedup. We thus opted for the more # descriptive alternative. return V , Lambda_V_1 _var_bound ( G , R , V , Lambda_V_1 , a_beta , b_beta ) [PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/mixing.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def _var_bound ( self , G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" DV , _ = V . shape L_M1q = self . K * ( - ss . gammaln ( self . A_BETA ) + self . A_BETA * np . log ( self . B_BETA )) # TODO Performance: LCSBookCode vectorized this # TODO Performance: ss.gammaln(a_beta[k]) is constant throughout the # loop in the calling function L_M3q = self . K * DV for k in range ( self . K ): L_M1q += ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) # TODO Vectorize or at least get rid of for loop # TODO Maybe cache determinant if Lambda_V_1 [ k ] . shape == ( 1 , ): L_M3q += Lambda_V_1 [ k ] else : L_M3q += np . linalg . slogdet ( Lambda_V_1 [ k ])[ 1 ] L_M3q /= 2 # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. # # ``responsibilities`` performs a ``nan_to_num(\u2026, nan=0, \u2026)``, so we # might divide by 0 here. The intended behaviour is to silently get a # NaN that can then be replaced by 0 again (this is how Drugowitsch does # it [PDF p. 213]). Drugowitsch expects dividing ``x`` by 0 to result # in NaN, however, in Python this is only true for ``x == 0``; for any # other ``x`` this instead results in ``inf`` (with sign depending on # the sign of x). The two cases also throw different errors (\u2018invalid # value encountered\u2019 for ``x == 0`` and \u2018divide by zero\u2019 otherwise). # # NOTE I don't think the neginf is strictly required but let's be safe. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): L_M2q = np . sum ( R * np . nan_to_num ( np . log ( G / R ), nan = 0 , posinf = 0 , neginf = 0 )) # This fixes(?) some numerical problems. if L_M2q > 0 and np . isclose ( L_M2q , 0 ): L_M2q = 0 assert L_M2q <= 0 , f \"Kullback-Leibler divergence less than zero: { L_M2q } \" return L_M1q + L_M2q + L_M3q fit ( X , y ) Fits mixing weights for this mixing weight model's set of rules to the provided data. Source code in berbl/mixing.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def fit ( self , X , y ): \"\"\" Fits mixing weights for this mixing weight model's set of rules to the provided data. \"\"\" Phi = check_phi ( self . phi , X ) M = np . hstack ([ cl . m_ for cl in self . rules ]) _ , self . DX_ = X . shape _ , self . Dy_ = y . shape N , self . DV_ = Phi . shape self . V_ = self . random_state . normal ( loc = 0 , scale = self . A_BETA / self . B_BETA , size = ( self . DV_ , self . K )) # self.a_beta_ is constant (but for the first run of the loop). self . a_beta_ = np . repeat ( self . A_BETA + self . DV_ / 2 , self . K ) self . b_beta_ = np . repeat ( self . B_BETA , self . K ) # Initialize parameters for the Bouchard approximation. self . alpha_ = self . random_state . normal ( loc = 0 , scale = self . A_BETA / self . B_BETA , size = ( N , 1 )) # lxi stands for \u03bb(\u03be) which is used in Bouchard's approximation. Its # supremum value is one over eight. self . lxi_ = self . random_state . random ( size = ( N , self . K )) * 0.125 self . alpha_ , self . lxi_ = self . _opt_bouchard ( M = M , Phi = Phi , V = self . V_ , alpha = self . alpha_ , lxi = self . lxi_ ) self . G_ = self . _mixing ( M , Phi , self . V_ ) self . R_ = self . _responsibilities ( X = X , y = y , G = self . G_ ) self . L_M_q_ = - np . inf delta_L_M_q = self . DELTA_S_L_M_Q + 1 i = 0 while delta_L_M_q > self . DELTA_S_L_M_Q and i < self . MAX_ITER_MIXING : i += 1 self . V_ , self . Lambda_V_1_ = self . _train_mix_weights ( M = M , X = X , y = y , Phi = Phi , R = self . R_ , V = self . V_ , # The first run of the loop should use the hyperparameter. a_beta = ( self . a_beta_ if i > 1 else np . repeat ( self . A_BETA , self . K )), b_beta = self . b_beta_ , lxi = self . lxi_ , alpha = self . alpha_ ) self . alpha_ , self . lxi_ = self . _opt_bouchard ( M = M , Phi = Phi , V = self . V_ , alpha = self . alpha_ , lxi = self . lxi_ ) self . b_beta_ = self . _train_b_beta ( V = self . V_ , Lambda_V_1 = self . Lambda_V_1_ ) self . G_ = self . _mixing ( M , Phi , self . V_ ) self . R_ = self . _responsibilities ( X = X , y = y , G = self . G_ ) L_M_q_prev = self . L_M_q_ self . L_M_q_ = self . _var_bound ( G = self . G_ , R = self . R_ , V = self . V_ , Lambda_V_1 = self . Lambda_V_1_ , a_beta = self . a_beta_ , b_beta = self . b_beta_ ) # LCSBookCode states: \u201cas we are using an approximation, the # variational bound might decrease, so we're not checking and need # to take the abs()\u201d. I guess with approximation he means the use of # the Laplace approximation (which may violate the lower bound # nature of L_M_q). delta_L_M_q = np . abs ( self . L_M_q_ - L_M_q_prev ) # TODO Check whether the abs is necessary for the Bouchard bound. # if self.L_M_q < L_M_q_prev: # print(f\"self.L_M_q < L_M_q_prev: {self.L_M_q} < {L_M_q_prev}\") assert np . all ( ~ np . isnan ( self . lxi_ )) return self mixing ( X ) Calculates the mixing weights for each of the given inputs. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (N, K) Mixing matrix containing the rules' mixing weights for each input. Source code in berbl/mixing.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def mixing ( self , X ): \"\"\" Calculates the mixing weights for each of the given inputs. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (N, K) Mixing matrix containing the rules' mixing weights for each input. \"\"\" Phi = check_phi ( self . phi , X ) # TODO When predicting (which uses this mixing method), I currently # calculate M twice, once when matching for each rule and once in # mixing (see same comment in Mixture). Add as an optional parameter to # Mixing.predict/fit etc. M = matching_matrix ([ cl . match for cl in self . rules ], X ) return self . _mixing ( M , Phi , self . V_ )","title":"mixing"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing","text":"Model for the mixing weights of a set of linear regression rules. Parameters: Name Type Description Default rules list of rule object List of rules (which are held fixed during mixing training). required phi callable Mixing feature function taking input matrices of shape (N, DX) and returning mixing feature matrices of shape (n, V). If None use the LCS default of phi(x) = 1 . required random_state RandomState object required A_BETA float Scale parameter of mixing weight vector variance prior. 10 ** -2 B_BETA float Shape parameter of mixing weight vector variance prior. 10 ** -4 DELTA_S_L_M_Q float Stopping criterion for variational update loop. 10 ** -2 MAX_ITER_MIXING int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). 40 EXP_MIN float Lowest real number x on system such that exp(x) > 0 . The default is the logarithm of the smallest positive number of the default dtype (as of 2020-10-06, this dtype is float64). np.log(np.finfo(None).tiny) LN_MAX float ln(x) , where x is the highest real number on the system. The default is the logarithm of the highest number of the default dtype (as of 2020-10-06, this dtype is float64). np.log(np.finfo(None).max) **kwargs kwargs This is here so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/mixing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , rules , phi , random_state , A_BETA = 10 **- 2 , B_BETA = 10 **- 4 , DELTA_S_L_M_Q = 10 **- 2 , MAX_ITER_MIXING = 40 , EXP_MIN = np . log ( np . finfo ( None ) . tiny ), LN_MAX = np . log ( np . finfo ( None ) . max ), ** kwargs ): \"\"\" Parameters ---------- rules : list of rule object List of rules (which are held fixed during mixing training). phi : callable Mixing feature function taking input matrices of shape (N, DX) and returning mixing feature matrices of shape (n, V). If ``None`` use the LCS default of ``phi(x) = 1``. random_state : RandomState object A_BETA : float Scale parameter of mixing weight vector variance prior. B_BETA : float Shape parameter of mixing weight vector variance prior. DELTA_S_L_M_Q : float Stopping criterion for variational update loop. MAX_ITER_MIXING : int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). EXP_MIN : float Lowest real number ``x`` on system such that ``exp(x) > 0``. The default is the logarithm of the smallest positive number of the default dtype (as of 2020-10-06, this dtype is float64). LN_MAX : float ``ln(x)``, where ``x`` is the highest real number on the system. The default is the logarithm of the highest number of the default dtype (as of 2020-10-06, this dtype is float64). **kwargs : kwargs This is here so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . rules = rules self . phi = phi self . A_BETA = A_BETA self . B_BETA = B_BETA self . DELTA_S_L_M_Q = DELTA_S_L_M_Q self . MAX_ITER_MIXING = MAX_ITER_MIXING self . EXP_MIN = EXP_MIN self . LN_MAX = LN_MAX self . random_state = random_state self . K = len ( self . rules )","title":"Mixing"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._mixing","text":"[PDF p. 239] Is zero wherever a rule does not match. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required Returns: Name Type Description G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Source code in berbl/mixing.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def _mixing ( self , M : np . ndarray , Phi : np . ndarray , V : np . ndarray ): \"\"\" [PDF p. 239] Is zero wherever a rule does not match. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. Returns ------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. \"\"\" # TODO Use literal.mixing here (remember to provide self.EXP_MIN and # self.LN_MAX, though!) # If Phi is standard, this simply broadcasts V to a matrix [V, V, V, \u2026] # of shape (N, DV). G = Phi @ V # This quasi never happens (at least for the run I checked it did not). # That run also oscillated so this is probably not the source. G = np . clip ( G , self . EXP_MIN , self . LN_MAX - np . log ( self . K )) G = np . exp ( G ) * M # The sum can be 0 meaning we do 0/0 (== NaN) but we ignore it because # it is fixed one line later (this is how Drugowitsch does it). # Drugowitsch does, however, also say that: \u201cUsually, this should never # happen as only model structures are accepted where [(np.sum(G, 1) > # 0).all()]. Nonetheless, this check was added to ensure that even these # cases are handled gracefully.\u201d with np . errstate ( invalid = \"ignore\" ): G = G / np . sum ( G , axis = 1 )[:, np . newaxis ] G = np . nan_to_num ( G , nan = 1 / self . K ) return G","title":"_mixing()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._opt_bouchard","text":"Update for the parameters of Bouchard's lower bound. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required alpha array of shape (N, 1) Current value of alpha variational parameters of Bouchard's bound. required lxi array of shape (N, K) Current value of lxi variational parameters of Bouchard's bound. required Returns: Type Description lxi , alpha New values for the variational parameters alpha and lxi . Source code in berbl/mixing.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def _opt_bouchard ( self , M : np . ndarray , Phi : np . ndarray , V , alpha , lxi ): \"\"\" Update for the parameters of Bouchard's lower bound. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. alpha : array of shape (N, 1) Current value of ``alpha`` variational parameters of Bouchard's bound. lxi : array of shape (N, K) Current value of ``lxi`` variational parameters of Bouchard's bound. Returns ------- lxi, alpha : tuple of arrays of shapes (N, 1) and (N, K) New values for the variational parameters ``alpha`` and ``lxi``. \"\"\" N , _ = Phi . shape h = np . log ( M ) + Phi @ V alpha = ( 1 / 2 * ( self . K / 2 - 1 ) + np . sum ( h * lxi , axis = 1 )) / np . sum ( lxi , axis = 1 ) alpha = alpha . reshape (( N , 1 )) xi = np . abs ( alpha - h ) # If ``alpha == h``, then the following contains a division by zero and # then a multiplication by NaN which results in a divide and an invalid # value warning to be thrown. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): lxi = 1 / ( 2 * xi ) * ( 1 / ( 1 + np . exp ( - xi )) - 1 / 2 ) # Where `alpha == h` we get NaN's in the previous formula due to xi = 0 # there. We simply solve that by setting the corresponding entries to # the limit for `x -> 0` of `lambda(x)` which is `0.125`. lxi [ np . where ( np . logical_and ( xi == 0 , np . isnan ( lxi )))] = 0.125 # NOTE Doing this in-place instead of returning values doesn't seem to # result in a significant speedup. We thus opted for the more # descriptive alternative. return alpha , lxi","title":"_opt_bouchard()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._responsibilities","text":"[PDF p. 240] Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required Returns: Name Type Description R array of shape (N, K) Responsibility matrix. Source code in berbl/mixing.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def _responsibilities ( self , X : np . ndarray , y : np . ndarray , G : np . ndarray ): \"\"\" [PDF p. 240] Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Returns ------- R : array of shape (N, K) Responsibility matrix. \"\"\" # NOTE: The code duplication solution used is faster for larger # len(self.rules) than the technically cleaner # W, Lambda_1, a_tau, b_tau = zip( # *[(cl.W_, cl.Lambda_1_, cl.a_tau_, cl.b_tau_) for cl in self.rules] # ) W = [ cl . W_ for cl in self . rules ] Lambda_1 = [ cl . Lambda_1_ for cl in self . rules ] a_tau = [ cl . a_tau_ for cl in self . rules ] b_tau = [ cl . b_tau_ for cl in self . rules ] return responsibilities ( X = X , Y = y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau )","title":"_responsibilities()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._train_b_beta","text":"[PDF p. 244] TrainMixPriors but only the part concerned with b_beta since a_beta is constant. Parameters: Name Type Description Default V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. required Returns: Name Type Description b_beta array of shape (K,) mixing weight vector prior parameter Source code in berbl/mixing.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def _train_b_beta ( self , V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] TrainMixPriors but only the part concerned with ``b_beta`` since ``a_beta`` is constant. Parameters ---------- V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. Returns ------- b_beta : array of shape (K,) mixing weight vector prior parameter \"\"\" DV , _ = V . shape b_beta = np . repeat ( self . B_BETA , ( self . K , )) Lambda_V_1_diag = np . array ( list ( map ( np . diag , Lambda_V_1 ))) # TODO Performance: LCSBookCode vectorized this: # b[:,1] = b_b + 0.5 * (sum(V * V, 0) + self.cov_Tr) for k in range ( self . K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV # Not that efficient, I think (but very close to [PDF p. 244]). # Lambda_V_1_kk = Lambda_V_1[l:u:1, l:u:1] # b_beta[k] = B_BETA + 0.5 * (np.trace(Lambda_V_1_kk) + v_k.T @ v_k) # More efficient. try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , ( f \"v_k = { v_k } , \" f \"K = { self . K } , \" f \"V = { V } , \" f \"Lambda_V_1 = { Lambda_V_1 } \" ), report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return b_beta","title":"_train_b_beta()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._train_mix_weights","text":"Training routine for mixing weights based on Bouchard's upper bound. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required lxi array of shape (N, K) Parameter of Bouchard's bound. required alpha array of shape (N, 1) Parameter of Bouchard's bound. required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrices. Source code in berbl/mixing.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def _train_mix_weights ( self , M , X , y , Phi , R , V , a_beta , b_beta , lxi , alpha ): \"\"\" Training routine for mixing weights based on Bouchard's upper bound. Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). lxi : array of shape (N, K) Parameter of Bouchard's bound. alpha : array of shape (N, 1) Parameter of Bouchard's bound. Returns ------- V, Lambda_V_1 : tuple of array of shapes (DV, K) and list (length K) of arrays of shape (DV, DV) Updated mixing weight matrix and mixing weight covariance matrices. \"\"\" N , _ = X . shape DV , _ = V . shape E_beta_beta = a_beta / b_beta Lambda_V_1 = [ np . zeros (( DV , DV ))] * self . K Rlxi = R * lxi for k in range ( self . K ): Lambda_V_1 [ k ] = 2 * ( Rlxi [:, [ k ]] . T * Phi . T ) @ Phi + E_beta_beta [ k ] * np . identity ( Lambda_V_1 [ k ] . shape [ 0 ]) t = R [:, [ k ]] * ( 1 / 2 - 2 * np . log ( M [:, [ k ]]) * lxi [:, [ k ]] + alpha * lxi [:, [ k ]]) V [:, [ k ]] = np . linalg . pinv ( Lambda_V_1 [ k ]) @ Phi . T @ t # NOTE Doing this in-place instead of returning values doesn't seem to # result in a significant speedup. We thus opted for the more # descriptive alternative. return V , Lambda_V_1","title":"_train_mix_weights()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing._var_bound","text":"[PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/mixing.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def _var_bound ( self , G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : list (length K) of arrays of shape (DV, DV) List of mixing weight covariance matrices. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" DV , _ = V . shape L_M1q = self . K * ( - ss . gammaln ( self . A_BETA ) + self . A_BETA * np . log ( self . B_BETA )) # TODO Performance: LCSBookCode vectorized this # TODO Performance: ss.gammaln(a_beta[k]) is constant throughout the # loop in the calling function L_M3q = self . K * DV for k in range ( self . K ): L_M1q += ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) # TODO Vectorize or at least get rid of for loop # TODO Maybe cache determinant if Lambda_V_1 [ k ] . shape == ( 1 , ): L_M3q += Lambda_V_1 [ k ] else : L_M3q += np . linalg . slogdet ( Lambda_V_1 [ k ])[ 1 ] L_M3q /= 2 # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. # # ``responsibilities`` performs a ``nan_to_num(\u2026, nan=0, \u2026)``, so we # might divide by 0 here. The intended behaviour is to silently get a # NaN that can then be replaced by 0 again (this is how Drugowitsch does # it [PDF p. 213]). Drugowitsch expects dividing ``x`` by 0 to result # in NaN, however, in Python this is only true for ``x == 0``; for any # other ``x`` this instead results in ``inf`` (with sign depending on # the sign of x). The two cases also throw different errors (\u2018invalid # value encountered\u2019 for ``x == 0`` and \u2018divide by zero\u2019 otherwise). # # NOTE I don't think the neginf is strictly required but let's be safe. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): L_M2q = np . sum ( R * np . nan_to_num ( np . log ( G / R ), nan = 0 , posinf = 0 , neginf = 0 )) # This fixes(?) some numerical problems. if L_M2q > 0 and np . isclose ( L_M2q , 0 ): L_M2q = 0 assert L_M2q <= 0 , f \"Kullback-Leibler divergence less than zero: { L_M2q } \" return L_M1q + L_M2q + L_M3q","title":"_var_bound()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing.fit","text":"Fits mixing weights for this mixing weight model's set of rules to the provided data. Source code in berbl/mixing.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def fit ( self , X , y ): \"\"\" Fits mixing weights for this mixing weight model's set of rules to the provided data. \"\"\" Phi = check_phi ( self . phi , X ) M = np . hstack ([ cl . m_ for cl in self . rules ]) _ , self . DX_ = X . shape _ , self . Dy_ = y . shape N , self . DV_ = Phi . shape self . V_ = self . random_state . normal ( loc = 0 , scale = self . A_BETA / self . B_BETA , size = ( self . DV_ , self . K )) # self.a_beta_ is constant (but for the first run of the loop). self . a_beta_ = np . repeat ( self . A_BETA + self . DV_ / 2 , self . K ) self . b_beta_ = np . repeat ( self . B_BETA , self . K ) # Initialize parameters for the Bouchard approximation. self . alpha_ = self . random_state . normal ( loc = 0 , scale = self . A_BETA / self . B_BETA , size = ( N , 1 )) # lxi stands for \u03bb(\u03be) which is used in Bouchard's approximation. Its # supremum value is one over eight. self . lxi_ = self . random_state . random ( size = ( N , self . K )) * 0.125 self . alpha_ , self . lxi_ = self . _opt_bouchard ( M = M , Phi = Phi , V = self . V_ , alpha = self . alpha_ , lxi = self . lxi_ ) self . G_ = self . _mixing ( M , Phi , self . V_ ) self . R_ = self . _responsibilities ( X = X , y = y , G = self . G_ ) self . L_M_q_ = - np . inf delta_L_M_q = self . DELTA_S_L_M_Q + 1 i = 0 while delta_L_M_q > self . DELTA_S_L_M_Q and i < self . MAX_ITER_MIXING : i += 1 self . V_ , self . Lambda_V_1_ = self . _train_mix_weights ( M = M , X = X , y = y , Phi = Phi , R = self . R_ , V = self . V_ , # The first run of the loop should use the hyperparameter. a_beta = ( self . a_beta_ if i > 1 else np . repeat ( self . A_BETA , self . K )), b_beta = self . b_beta_ , lxi = self . lxi_ , alpha = self . alpha_ ) self . alpha_ , self . lxi_ = self . _opt_bouchard ( M = M , Phi = Phi , V = self . V_ , alpha = self . alpha_ , lxi = self . lxi_ ) self . b_beta_ = self . _train_b_beta ( V = self . V_ , Lambda_V_1 = self . Lambda_V_1_ ) self . G_ = self . _mixing ( M , Phi , self . V_ ) self . R_ = self . _responsibilities ( X = X , y = y , G = self . G_ ) L_M_q_prev = self . L_M_q_ self . L_M_q_ = self . _var_bound ( G = self . G_ , R = self . R_ , V = self . V_ , Lambda_V_1 = self . Lambda_V_1_ , a_beta = self . a_beta_ , b_beta = self . b_beta_ ) # LCSBookCode states: \u201cas we are using an approximation, the # variational bound might decrease, so we're not checking and need # to take the abs()\u201d. I guess with approximation he means the use of # the Laplace approximation (which may violate the lower bound # nature of L_M_q). delta_L_M_q = np . abs ( self . L_M_q_ - L_M_q_prev ) # TODO Check whether the abs is necessary for the Bouchard bound. # if self.L_M_q < L_M_q_prev: # print(f\"self.L_M_q < L_M_q_prev: {self.L_M_q} < {L_M_q_prev}\") assert np . all ( ~ np . isnan ( self . lxi_ )) return self","title":"fit()"},{"location":"reference/berbl/mixing/#berbl.mixing.Mixing.mixing","text":"Calculates the mixing weights for each of the given inputs. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (N, K) Mixing matrix containing the rules' mixing weights for each input. Source code in berbl/mixing.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def mixing ( self , X ): \"\"\" Calculates the mixing weights for each of the given inputs. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (N, K) Mixing matrix containing the rules' mixing weights for each input. \"\"\" Phi = check_phi ( self . phi , X ) # TODO When predicting (which uses this mixing method), I currently # calculate M twice, once when matching for each rule and once in # mixing (see same comment in Mixture). Add as an optional parameter to # Mixing.predict/fit etc. M = matching_matrix ([ cl . match for cl in self . rules ], X ) return self . _mixing ( M , Phi , self . V_ )","title":"mixing()"},{"location":"reference/berbl/mixing_laplace/","text":"MixingLaplace ( DELTA_S_KLRG = 10 ** - 8 , ** kwargs ) Bases: Mixing Model for the mixing weights of a set of linear regression rules. Fitted using Drugowitsch's Laplace approximation. Structurally, the main difference to Mixing is that Lambda_V_1 is a matrix of shape (K * DV, K * DV) (i.e. the mixing problem is solved for all submodels at once), whereas Mixing has K mixing matrices (one for each submodel). Parameters: Name Type Description Default DELTA_S_KLRG float Stopping criterion for the iterative Laplace approximation in the mixing weight update. 10 ** -8 **kwargs kwargs This is here for two reasons: To be able to provide the parent with all the parameters it uses (we only add DELTA_S_KLRG ) and so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/mixing_laplace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , DELTA_S_KLRG = 10 **- 8 , ** kwargs ): \"\"\" Parameters ---------- DELTA_S_KLRG : float Stopping criterion for the iterative Laplace approximation in the mixing weight update. **kwargs : kwargs This is here for two reasons: To be able to provide the parent with all the parameters it uses (we only add ``DELTA_S_KLRG``) and so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . DELTA_S_KLRG = DELTA_S_KLRG super () . __init__ ( ** kwargs ) _train_b_beta ( V , Lambda_V_1 ) [PDF p. 244] TrainMixPriors but only the part concerned with b_beta since a_beta is constant. Note that we override this because Lambda_V_1 has a different form here than in mixing.Mixing (where it is a list of K matrices with shapes (DV, DV) ). Parameters: Name Type Description Default V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required Returns: Name Type Description b_beta array of shape (K,) mixing weight vector prior parameter Source code in berbl/mixing_laplace.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def _train_b_beta ( self , V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] TrainMixPriors but only the part concerned with ``b_beta`` since ``a_beta`` is constant. Note that we override this because ``Lambda_V_1`` has a different form here than in ``mixing.Mixing`` (where it is a list of ``K`` matrices with shapes ``(DV, DV)``). Parameters ---------- V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. Returns ------- b_beta : array of shape (K,) mixing weight vector prior parameter \"\"\" DV , K = V . shape b_beta = np . repeat ( self . B_BETA , ( self . K , )) Lambda_V_1_diag = np . diag ( Lambda_V_1 ) # TODO Performance: LCSBookCode vectorized this: # b[:,1] = b_b + 0.5 * (sum(V * V, 0) + self.cov_Tr) for k in range ( self . K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV # print(f\"sum {k} Lambda_V_1_diag\", np.sum(Lambda_V_1_diag[l:u:1])) # Not that efficient, I think (but very close to [PDF p. 244]). # Lambda_V_1_kk = Lambda_V_1[l:u:1, l:u:1] # b_beta[k] = B_BETA + 0.5 * (np.trace(Lambda_V_1_kk) + v_k.T @ v_k) # More efficient. try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , ( f \"v_k = { v_k } , \" f \"K = { self . K } , \" f \"V = { V } , \" f \"Lambda_V_1 = { Lambda_V_1 } \" ), report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return b_beta _train_mix_weights ( M , X , y , Phi , G , R , V , a_beta , b_beta ) Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book). Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrix. Source code in berbl/mixing_laplace.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def _train_mix_weights ( self , M , X , y , Phi , G , R , V , a_beta , b_beta ): \"\"\" Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book). Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- V, Lambda_V_1 : tuple of arrays of shapes (DV, K) and (K * DV, K * DV) Updated mixing weight matrix and mixing weight covariance matrix. \"\"\" _var_bound ( G , R , V , Lambda_V_1 , a_beta , b_beta ) [PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/mixing_laplace.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def _var_bound ( self , G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" # NOTE We don't use the version from literal here because we # can cache/precompute several values that are computed each time # literal.train_mix_weights is called. DV , K = V . shape L_M1q = K * ( - ss . gammaln ( self . A_BETA ) + self . A_BETA * np . log ( self . B_BETA )) # TODO Performance: LCSBookCode vectorized this # TODO Performance: ss.gammaln(a_beta[k]) is constant throughout the # loop in the calling function for k in range ( self . K ): L_M1q += ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. L_M2q = _kl ( R , G ) # TODO Performance: slogdet can be cached, is computed more than once # L_M3q may be -inf after the following line but that is probably OK since # the ``train_mixing`` loop then aborts (also see comment in # ``train_mixing``). L_M3q = 0.5 * np . linalg . slogdet ( Lambda_V_1 )[ 1 ] + K * DV / 2 if np . any ( ~ np . isfinite ([ L_M1q , L_M2q , L_M3q ])): known_issue ( \"Infinite var_mix_bound\" , ( f \"Lambda_V_1 = { Lambda_V_1 } , \" f \"L_M1q = { L_M1q } , \" f \"L_M2q = { L_M2q } , \" f \"L_M3q = { L_M3q } \" )) return L_M1q + L_M2q + L_M3q","title":"mixing_laplace"},{"location":"reference/berbl/mixing_laplace/#berbl.mixing_laplace.MixingLaplace","text":"Bases: Mixing Model for the mixing weights of a set of linear regression rules. Fitted using Drugowitsch's Laplace approximation. Structurally, the main difference to Mixing is that Lambda_V_1 is a matrix of shape (K * DV, K * DV) (i.e. the mixing problem is solved for all submodels at once), whereas Mixing has K mixing matrices (one for each submodel). Parameters: Name Type Description Default DELTA_S_KLRG float Stopping criterion for the iterative Laplace approximation in the mixing weight update. 10 ** -8 **kwargs kwargs This is here for two reasons: To be able to provide the parent with all the parameters it uses (we only add DELTA_S_KLRG ) and so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/mixing_laplace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , DELTA_S_KLRG = 10 **- 8 , ** kwargs ): \"\"\" Parameters ---------- DELTA_S_KLRG : float Stopping criterion for the iterative Laplace approximation in the mixing weight update. **kwargs : kwargs This is here for two reasons: To be able to provide the parent with all the parameters it uses (we only add ``DELTA_S_KLRG``) and so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . DELTA_S_KLRG = DELTA_S_KLRG super () . __init__ ( ** kwargs )","title":"MixingLaplace"},{"location":"reference/berbl/mixing_laplace/#berbl.mixing_laplace.MixingLaplace._train_b_beta","text":"[PDF p. 244] TrainMixPriors but only the part concerned with b_beta since a_beta is constant. Note that we override this because Lambda_V_1 has a different form here than in mixing.Mixing (where it is a list of K matrices with shapes (DV, DV) ). Parameters: Name Type Description Default V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required Returns: Name Type Description b_beta array of shape (K,) mixing weight vector prior parameter Source code in berbl/mixing_laplace.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def _train_b_beta ( self , V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] TrainMixPriors but only the part concerned with ``b_beta`` since ``a_beta`` is constant. Note that we override this because ``Lambda_V_1`` has a different form here than in ``mixing.Mixing`` (where it is a list of ``K`` matrices with shapes ``(DV, DV)``). Parameters ---------- V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. Returns ------- b_beta : array of shape (K,) mixing weight vector prior parameter \"\"\" DV , K = V . shape b_beta = np . repeat ( self . B_BETA , ( self . K , )) Lambda_V_1_diag = np . diag ( Lambda_V_1 ) # TODO Performance: LCSBookCode vectorized this: # b[:,1] = b_b + 0.5 * (sum(V * V, 0) + self.cov_Tr) for k in range ( self . K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV # print(f\"sum {k} Lambda_V_1_diag\", np.sum(Lambda_V_1_diag[l:u:1])) # Not that efficient, I think (but very close to [PDF p. 244]). # Lambda_V_1_kk = Lambda_V_1[l:u:1, l:u:1] # b_beta[k] = B_BETA + 0.5 * (np.trace(Lambda_V_1_kk) + v_k.T @ v_k) # More efficient. try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , ( f \"v_k = { v_k } , \" f \"K = { self . K } , \" f \"V = { V } , \" f \"Lambda_V_1 = { Lambda_V_1 } \" ), report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return b_beta","title":"_train_b_beta()"},{"location":"reference/berbl/mixing_laplace/#berbl.mixing_laplace.MixingLaplace._train_mix_weights","text":"Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book). Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrix. Source code in berbl/mixing_laplace.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def _train_mix_weights ( self , M , X , y , Phi , G , R , V , a_beta , b_beta ): \"\"\" Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book). Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- V, Lambda_V_1 : tuple of arrays of shapes (DV, K) and (K * DV, K * DV) Updated mixing weight matrix and mixing weight covariance matrix. \"\"\"","title":"_train_mix_weights()"},{"location":"reference/berbl/mixing_laplace/#berbl.mixing_laplace.MixingLaplace._var_bound","text":"[PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/mixing_laplace.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def _var_bound ( self , G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" # NOTE We don't use the version from literal here because we # can cache/precompute several values that are computed each time # literal.train_mix_weights is called. DV , K = V . shape L_M1q = K * ( - ss . gammaln ( self . A_BETA ) + self . A_BETA * np . log ( self . B_BETA )) # TODO Performance: LCSBookCode vectorized this # TODO Performance: ss.gammaln(a_beta[k]) is constant throughout the # loop in the calling function for k in range ( self . K ): L_M1q += ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. L_M2q = _kl ( R , G ) # TODO Performance: slogdet can be cached, is computed more than once # L_M3q may be -inf after the following line but that is probably OK since # the ``train_mixing`` loop then aborts (also see comment in # ``train_mixing``). L_M3q = 0.5 * np . linalg . slogdet ( Lambda_V_1 )[ 1 ] + K * DV / 2 if np . any ( ~ np . isfinite ([ L_M1q , L_M2q , L_M3q ])): known_issue ( \"Infinite var_mix_bound\" , ( f \"Lambda_V_1 = { Lambda_V_1 } , \" f \"L_M1q = { L_M1q } , \" f \"L_M2q = { L_M2q } , \" f \"L_M3q = { L_M3q } \" )) return L_M1q + L_M2q + L_M3q","title":"_var_bound()"},{"location":"reference/berbl/mixture/","text":"Mixture ( matchs , random_state , add_bias = True , phi = None , fit_mixing = 'bouchard' , ** kwargs ) A model based on mixing linear regression rules using the given model structure. Parameters: Name Type Description Default matchs List A list of matching functions (i.e. objects implementing a match attribute) defining the structure of this mixture. required random_state RandomState object required add_bias bool Whether to add an all-ones bias column to the input data. True phi callable Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . None fit_mixing str Either of \"bouchard\" or \"laplace\". 'bouchard' **kwargs This is passed through unchanged to both Mixing and Rule . {} Source code in berbl/mixture.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , matchs : List , random_state , add_bias = True , phi = None , fit_mixing = \"bouchard\" , ** kwargs ): \"\"\" A model based on mixing linear regression rules using the given model structure. Parameters ---------- matchs A list of matching functions (i.e. objects implementing a ``match`` attribute) defining the structure of this mixture. random_state : RandomState object add_bias : bool Whether to add an all-ones bias column to the input data. phi : callable Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. fit_mixing : str Either of \"bouchard\" or \"laplace\". **kwargs This is passed through unchanged to both ``Mixing`` and ``Rule``. \"\"\" self . matchs = matchs self . add_bias = add_bias self . phi = phi self . fit_mixing = fit_mixing self . random_state = random_state # TODO Should probably validate these kwargs because otherwise we don't # notice when something is used the wrong way. self . __kwargs = kwargs _predict_vars ( X ) No bias is added. Source code in berbl/mixture.py 220 221 222 223 224 225 226 227 228 229 230 def _predict_vars ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y_vars = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): y_vars [ k ] = self . rules_ [ k ] . predict_var ( X ) return y_vars _predicts ( X ) No bias is added. Source code in berbl/mixture.py 189 190 191 192 193 194 195 196 197 198 def _predicts ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): y [ k ] = self . rules_ [ k ] . predict ( X ) return y fit ( X , y ) Fits this model to the provided data. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Source code in berbl/mixture.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fits this model to the provided data. Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. \"\"\" if self . add_bias : X = add_bias ( X ) self . K_ = len ( self . matchs ) _ , self . DX_ = X . shape y = y . reshape (( len ( X ), - 1 )) _ , self . Dy_ = y . shape # Train submodels. # # \u201cWhen fit is called, any previous call to fit should be ignored.\u201d self . rules_ = list ( map ( lambda m : Rule ( m , ** self . __kwargs ), self . matchs )) # TODO Cache trained rules at the GA level. for k in range ( self . K_ ): self . rules_ [ k ] . fit ( X , y ) # Train mixing model. # # \u201cWhen fit is called, any previous call to fit should be ignored.\u201d if self . fit_mixing == \"bouchard\" : self . mixing_ = Mixing ( rules = self . rules_ , phi = self . phi , random_state = self . random_state , ** self . __kwargs ) elif self . fit_mixing == \"laplace\" : self . mixing_ = MixingLaplace ( rules = self . rules_ , phi = self . phi , random_state = self . random_state , ** self . __kwargs ) else : raise NotImplementedError ( \"Only 'bouchard' and 'laplace' supported for fit_mixing\" ) self . mixing_ . fit ( X , y ) # We need to recalculate the rules' variational bounds here because we # now have access to the final value of R (which we substituted by M # during submodel training for ensuring independence). self . L_C_q_ = np . repeat ( - np . inf , len ( self . rules_ )) for k in range ( self . K_ ): self . L_C_q_ [ k ] = self . rules_ [ k ] . var_bound ( X , y , r = self . mixing_ . R_ [:, [ k ]]) self . L_M_q_ = self . mixing_ . L_M_q_ self . L_q_ = np . sum ( self . L_C_q_ ) + self . L_M_q_ # TODO Replace this with volume-dependent formula (e.g. the one from the # book s.t. p(K) = \\exp(-V) V^K/K!). self . ln_p_M_ = - np . log ( float ( np . math . factorial ( self . K_ ))) # (7.3), i.e. p_M \\propto 1/K self . p_M_D_ = self . L_q_ + self . ln_p_M_ return self predict_distribution ( X ) Returns: Type Description callable A function expecting a y and returning the values of the predictive distributions at positions X . Source code in berbl/mixture.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def predict_distribution ( self , X ): \"\"\" Returns ------- callable A function expecting a ``y`` and returning the values of the predictive distributions at positions ``X``. \"\"\" if self . add_bias : X = add_bias ( X ) G = self . mixing_ . mixing ( X ) . T # (K, N) W = self . _predicts ( X ) # (K, N, Dy) var = self . _predict_vars ( X ) # (K, N, Dy) def pdf ( y ): # TODO Vectorize if possible res = 0 for k in range ( self . K_ ): prd = 1 for j in range ( self . Dy_ ): prd *= t ( mu = W [ k ][:, j ], prec = 2 / var [ k ][:, j ], df = 2 * self . rules_ [ k ] . a_tau_ )( y ) res += G [ k ] * prd return res return pdf predict_mean_var ( X ) [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description y , y_var Source code in berbl/mixture.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def predict_mean_var ( self , X ): \"\"\" [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- y, y_var : tuple of two arrays of shape (N, Dy) \"\"\" N , _ = X . shape Dy , DX = self . rules_ [ 0 ] . W_ . shape Phi = check_phi ( self . phi , X ) # After having called ``predicts``, add the bias term (``predicts`` also # adds the bias term internally). if self . add_bias : X = add_bias ( X ) # Collect the independent predictions and variances of each submodel. We # use the implementations of those that do neither perform input # checking nor bias adding to save some time. ys = self . _predicts ( X ) # shape (K, N, Dy) y_vars = self . _predict_vars ( X ) G_ = self . mixing_ . mixing ( X ) . T # K \u00d7 N # For each submodel's prediction, we weigh every dimension of the output # vector by the same amount, thus we simply repeat the G values over Dy. G = G_ [:, :, np . newaxis ] . repeat ( Dy , axis = 2 ) # K \u00d7 N \u00d7 Dy y = np . sum ( G * ys , axis = 0 ) y_var = np . sum ( G * ( y_vars + ys ** 2 ), axis = 0 ) - y ** 2 return y , y_var predict_vars ( X ) Returns this model's submodel's prediction variances, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (K, N, Dy) Prediction variances of each submodel. Source code in berbl/mixture.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def predict_vars ( self , X ): \"\"\" Returns this model's submodel's prediction variances, one by one, without mixing them. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (K, N, Dy) Prediction variances of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predict_vars ( X ) predicts ( X ) Returns this model's submodel's predictions, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/mixture.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def predicts ( self , X ): \"\"\" Returns this model's submodel's predictions, one by one, without mixing them. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predicts ( X )","title":"mixture"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture","text":"A model based on mixing linear regression rules using the given model structure. Parameters: Name Type Description Default matchs List A list of matching functions (i.e. objects implementing a match attribute) defining the structure of this mixture. required random_state RandomState object required add_bias bool Whether to add an all-ones bias column to the input data. True phi callable Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . None fit_mixing str Either of \"bouchard\" or \"laplace\". 'bouchard' **kwargs This is passed through unchanged to both Mixing and Rule . {} Source code in berbl/mixture.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , matchs : List , random_state , add_bias = True , phi = None , fit_mixing = \"bouchard\" , ** kwargs ): \"\"\" A model based on mixing linear regression rules using the given model structure. Parameters ---------- matchs A list of matching functions (i.e. objects implementing a ``match`` attribute) defining the structure of this mixture. random_state : RandomState object add_bias : bool Whether to add an all-ones bias column to the input data. phi : callable Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. fit_mixing : str Either of \"bouchard\" or \"laplace\". **kwargs This is passed through unchanged to both ``Mixing`` and ``Rule``. \"\"\" self . matchs = matchs self . add_bias = add_bias self . phi = phi self . fit_mixing = fit_mixing self . random_state = random_state # TODO Should probably validate these kwargs because otherwise we don't # notice when something is used the wrong way. self . __kwargs = kwargs","title":"Mixture"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture._predict_vars","text":"No bias is added. Source code in berbl/mixture.py 220 221 222 223 224 225 226 227 228 229 230 def _predict_vars ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y_vars = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): y_vars [ k ] = self . rules_ [ k ] . predict_var ( X ) return y_vars","title":"_predict_vars()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture._predicts","text":"No bias is added. Source code in berbl/mixture.py 189 190 191 192 193 194 195 196 197 198 def _predicts ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): y [ k ] = self . rules_ [ k ] . predict ( X ) return y","title":"_predicts()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture.fit","text":"Fits this model to the provided data. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Source code in berbl/mixture.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fits this model to the provided data. Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. \"\"\" if self . add_bias : X = add_bias ( X ) self . K_ = len ( self . matchs ) _ , self . DX_ = X . shape y = y . reshape (( len ( X ), - 1 )) _ , self . Dy_ = y . shape # Train submodels. # # \u201cWhen fit is called, any previous call to fit should be ignored.\u201d self . rules_ = list ( map ( lambda m : Rule ( m , ** self . __kwargs ), self . matchs )) # TODO Cache trained rules at the GA level. for k in range ( self . K_ ): self . rules_ [ k ] . fit ( X , y ) # Train mixing model. # # \u201cWhen fit is called, any previous call to fit should be ignored.\u201d if self . fit_mixing == \"bouchard\" : self . mixing_ = Mixing ( rules = self . rules_ , phi = self . phi , random_state = self . random_state , ** self . __kwargs ) elif self . fit_mixing == \"laplace\" : self . mixing_ = MixingLaplace ( rules = self . rules_ , phi = self . phi , random_state = self . random_state , ** self . __kwargs ) else : raise NotImplementedError ( \"Only 'bouchard' and 'laplace' supported for fit_mixing\" ) self . mixing_ . fit ( X , y ) # We need to recalculate the rules' variational bounds here because we # now have access to the final value of R (which we substituted by M # during submodel training for ensuring independence). self . L_C_q_ = np . repeat ( - np . inf , len ( self . rules_ )) for k in range ( self . K_ ): self . L_C_q_ [ k ] = self . rules_ [ k ] . var_bound ( X , y , r = self . mixing_ . R_ [:, [ k ]]) self . L_M_q_ = self . mixing_ . L_M_q_ self . L_q_ = np . sum ( self . L_C_q_ ) + self . L_M_q_ # TODO Replace this with volume-dependent formula (e.g. the one from the # book s.t. p(K) = \\exp(-V) V^K/K!). self . ln_p_M_ = - np . log ( float ( np . math . factorial ( self . K_ ))) # (7.3), i.e. p_M \\propto 1/K self . p_M_D_ = self . L_q_ + self . ln_p_M_ return self","title":"fit()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture.predict_distribution","text":"Returns: Type Description callable A function expecting a y and returning the values of the predictive distributions at positions X . Source code in berbl/mixture.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def predict_distribution ( self , X ): \"\"\" Returns ------- callable A function expecting a ``y`` and returning the values of the predictive distributions at positions ``X``. \"\"\" if self . add_bias : X = add_bias ( X ) G = self . mixing_ . mixing ( X ) . T # (K, N) W = self . _predicts ( X ) # (K, N, Dy) var = self . _predict_vars ( X ) # (K, N, Dy) def pdf ( y ): # TODO Vectorize if possible res = 0 for k in range ( self . K_ ): prd = 1 for j in range ( self . Dy_ ): prd *= t ( mu = W [ k ][:, j ], prec = 2 / var [ k ][:, j ], df = 2 * self . rules_ [ k ] . a_tau_ )( y ) res += G [ k ] * prd return res return pdf","title":"predict_distribution()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture.predict_mean_var","text":"[PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description y , y_var Source code in berbl/mixture.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def predict_mean_var ( self , X ): \"\"\" [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- y, y_var : tuple of two arrays of shape (N, Dy) \"\"\" N , _ = X . shape Dy , DX = self . rules_ [ 0 ] . W_ . shape Phi = check_phi ( self . phi , X ) # After having called ``predicts``, add the bias term (``predicts`` also # adds the bias term internally). if self . add_bias : X = add_bias ( X ) # Collect the independent predictions and variances of each submodel. We # use the implementations of those that do neither perform input # checking nor bias adding to save some time. ys = self . _predicts ( X ) # shape (K, N, Dy) y_vars = self . _predict_vars ( X ) G_ = self . mixing_ . mixing ( X ) . T # K \u00d7 N # For each submodel's prediction, we weigh every dimension of the output # vector by the same amount, thus we simply repeat the G values over Dy. G = G_ [:, :, np . newaxis ] . repeat ( Dy , axis = 2 ) # K \u00d7 N \u00d7 Dy y = np . sum ( G * ys , axis = 0 ) y_var = np . sum ( G * ( y_vars + ys ** 2 ), axis = 0 ) - y ** 2 return y , y_var","title":"predict_mean_var()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture.predict_vars","text":"Returns this model's submodel's prediction variances, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (K, N, Dy) Prediction variances of each submodel. Source code in berbl/mixture.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def predict_vars ( self , X ): \"\"\" Returns this model's submodel's prediction variances, one by one, without mixing them. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (K, N, Dy) Prediction variances of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predict_vars ( X )","title":"predict_vars()"},{"location":"reference/berbl/mixture/#berbl.mixture.Mixture.predicts","text":"Returns this model's submodel's predictions, one by one, without mixing them. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/mixture.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def predicts ( self , X ): \"\"\" Returns this model's submodel's predictions, one by one, without mixing them. Parameters ---------- X : array of shape (N, DX) Input matrix. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predicts ( X )","title":"predicts()"},{"location":"reference/berbl/rule/","text":"Rule ( match , A_ALPHA = 10 ** - 2 , B_ALPHA = 10 ** - 4 , A_TAU = 10 ** - 2 , B_TAU = 10 ** - 4 , DELTA_S_L_K_Q = 10 ** - 4 , MAX_ITER_RULE = 20 , ** kwargs ) A rule based on a provided match function. The submodel of the rule is a local linear regression model. In LCS speak, this rule is a \u201clinear regression classifier\u201d. Parameters: Name Type Description Default match object match.match is this rule's match function. According to Drugowitsch's framework (or mixture of experts), each rule should get assigned a responsibility for each data point. However, in order to be able to train the submodels independently, that responsibility (which depends on the matching function but also on the other rules' responsibilities) is replaced with the matching function. required A_ALPHA float Scale parameter of weight vector variance prior. 10 ** -2 B_ALPHA float Shape parameter of weight vector variance prior. 10 ** -4 A_TAU float Scale parameter of noise variance prior. 10 ** -2 B_TAU float Shape parameter of noise variance prior. 10 ** -4 DELTA_S_L_K_Q float Stopping criterion for variational update loop. 10 ** -4 MAX_ITER_RULE int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). 20 **kwargs kwargs This is here so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/rule.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , match , A_ALPHA = 10 **- 2 , B_ALPHA = 10 **- 4 , A_TAU = 10 **- 2 , B_TAU = 10 **- 4 , DELTA_S_L_K_Q = 10 **- 4 , MAX_ITER_RULE = 20 , ** kwargs ): \"\"\" Parameters ---------- match : object ``match.match`` is this rule's match function. According to Drugowitsch's framework (or mixture of experts), each rule should get assigned a responsibility for each data point. However, in order to be able to train the submodels independently, that responsibility (which depends on the matching function but also on the other rules' responsibilities) is replaced with the matching function. A_ALPHA : float Scale parameter of weight vector variance prior. B_ALPHA : float Shape parameter of weight vector variance prior. A_TAU : float Scale parameter of noise variance prior. B_TAU : float Shape parameter of noise variance prior. DELTA_S_L_K_Q : float Stopping criterion for variational update loop. MAX_ITER_RULE : int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). **kwargs : kwargs This is here so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . match = match self . A_ALPHA = A_ALPHA self . B_ALPHA = B_ALPHA self . A_TAU = A_TAU self . B_TAU = B_TAU self . DELTA_S_L_K_Q = DELTA_S_L_K_Q self . MAX_ITER_RULE = MAX_ITER_RULE fit ( X , y ) Fits this rule's (sub)model to the part of the provided data that the rule matches. See TrainClassifier [PDF p. 238]. Source code in berbl/rule.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fits this rule's (sub)model to the part of the provided data that the rule matches. See TrainClassifier [PDF p. 238]. \"\"\" self . m_ = self . match . match ( X ) N , self . DX_ = X . shape N , self . Dy_ = y . shape X_ = X * np . sqrt ( self . m_ ) y_ = y * np . sqrt ( self . m_ ) E_alpha_alpha = self . A_ALPHA / self . B_ALPHA self . a_alpha_ = self . A_ALPHA + self . DX_ * self . Dy_ / 2 # self.a_tau_ is constant. self . a_tau_ = self . A_TAU + 0.5 * np . sum ( self . m_ ) self . b_tau_ = self . B_TAU # TODO Why not precompute L_q_ using self.var_bound? self . L_q_ = - np . inf delta_L_q = self . DELTA_S_L_K_Q + 1 iter = 0 while delta_L_q > self . DELTA_S_L_K_Q and iter < self . MAX_ITER_RULE : iter += 1 self . Lambda_ = np . diag ([ E_alpha_alpha ] * self . DX_ ) + X_ . T @ X_ # While, in theory, Lambda is always invertible here and we thus # should be able to use inv (as it is described in the algorithm we # implement), we (seldomly) get a singular matrix, probably due to # numerical issues. Thus we simply use pinv which yields the same # result as inv anyways if the matrix is in fact non-singular. Also, # in his own code, Drugowitsch always uses pseudo inverse here. self . Lambda_1_ = np . linalg . pinv ( self . Lambda_ ) self . W_ = y_ . T @ X_ @ self . Lambda_1_ self . b_tau_ = self . B_TAU + 1 / ( 2 * self . Dy_ ) * ( np . sum ( y_ * y_ ) - np . sum ( self . W_ * ( self . W_ @ self . Lambda_ ))) E_tau_tau = self . a_tau_ / self . b_tau_ # Dy factor in front of trace due to sum over Dy elements (7.100). self . b_alpha_ = self . B_ALPHA + 0.5 * ( E_tau_tau * np . sum ( self . W_ * self . W_ ) + self . Dy_ * np . trace ( self . Lambda_1_ )) E_alpha_alpha = self . a_alpha_ / self . b_alpha_ L_q_prev = self . L_q_ self . L_q_ = self . var_bound ( X = X , y = y , # Substitute r by m in order to train submodels independently # (see [PDF p. 219]). Note, however, that after having trained # the mixing model we finally evaluate the submodels using # ``r=R[:,[k]]`` as intended. r = self . m_ ) delta_L_q = self . L_q_ - L_q_prev return self predict ( X ) The rule's submodel's mean at the given positions; may serve as a prediction. Parameters: Name Type Description Default X array of shape (N, DX) required Returns: Name Type Description mean array of shape (N, Dy) Source code in berbl/rule.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def predict ( self , X ): \"\"\" The rule's submodel's mean at the given positions; may serve as a prediction. Parameters ---------- X : array of shape (N, DX) Returns ------- mean : array of shape (N, Dy) \"\"\" return X @ self . W_ . T predict_var ( X ) The rule's submodel's variance at the given positions; may serve as some kind of confidence estimate for the prediction. The model currently assumes the same variance in all dimensions; thus the same value is repeated for each dimension. Parameters: Name Type Description Default X array of shape (N, DX) required Returns: Name Type Description variance array of shape (N, Dy) Source code in berbl/rule.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def predict_var ( self , X ): \"\"\" The rule's submodel's variance at the given positions; may serve as some kind of confidence estimate for the prediction. The model currently assumes the same variance in all dimensions; thus the same value is repeated for each dimension. Parameters ---------- X : array of shape (N, DX) Returns ------- variance : array of shape (N, Dy) \"\"\" # The sum corresponds to x @ self.Lambda_1 @ x for each x in X (i.e. # np.diag(X @ self.Lambda_1_ @ X.T)). var = 2 * self . b_tau_ / ( self . a_tau_ - 1 ) * ( 1 + np . sum ( ( X @ self . Lambda_1_ ) * X , axis = 1 )) # The same value is repeated for each dimension since the model # currently assumes the same variance in all dimensions. return var [:, np . newaxis ] . repeat ( self . Dy_ , axis = 1 ) var_bound ( X , y , r ) The components of the variational bound specific to this rule. See VarClBound [PDF p. 247]. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required r array of shape (N, 1) Responsibilities (during training replaced with matching array of this rule in order to enable independent submodel training). required Source code in berbl/rule.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def var_bound ( self , X : np . ndarray , y : np . ndarray , r : np . ndarray ): \"\"\" The components of the variational bound specific to this rule. See VarClBound [PDF p. 247]. Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. r : array of shape (N, 1) Responsibilities (during training replaced with matching array of this rule in order to enable independent submodel training). \"\"\" E_tau_tau = self . a_tau_ / self . b_tau_ L_1_q = self . Dy_ / 2 * ( ss . digamma ( self . a_tau_ ) - np . log ( self . b_tau_ ) - np . log ( 2 * np . pi )) * np . sum ( r ) # We reshape r to a NumPy row vector since NumPy seems to understand # what we want to do when we multiply two row vectors (i.e. a^T a). L_2_q = ( - 0.5 * r ) . reshape ( ( - 1 )) @ ( E_tau_tau * np . sum (( y - X @ self . W_ . T ) ** 2 , axis = 1 ) + self . Dy_ * np . sum ( X * ( X @ self . Lambda_1_ ), axis = 1 )) L_3_q = - ss . gammaln ( self . A_ALPHA ) + self . A_ALPHA * np . log ( self . B_ALPHA ) + ss . gammaln ( self . a_alpha_ ) - self . a_alpha_ * np . log ( self . b_alpha_ ) + self . DX_ * self . Dy_ / 2 + self . Dy_ / 2 * np . log ( np . linalg . det ( self . Lambda_1_ )) L_4_q = self . Dy_ * ( - ss . gammaln ( self . A_TAU ) + self . A_TAU * np . log ( self . B_TAU ) + ( self . A_TAU - self . a_tau_ ) * ss . digamma ( self . a_tau_ ) - self . A_TAU * np . log ( self . b_tau_ ) - self . B_TAU * E_tau_tau + ss . gammaln ( self . a_tau_ ) + self . a_tau_ ) return L_1_q + L_2_q + L_3_q + L_4_q","title":"rule"},{"location":"reference/berbl/rule/#berbl.rule.Rule","text":"A rule based on a provided match function. The submodel of the rule is a local linear regression model. In LCS speak, this rule is a \u201clinear regression classifier\u201d. Parameters: Name Type Description Default match object match.match is this rule's match function. According to Drugowitsch's framework (or mixture of experts), each rule should get assigned a responsibility for each data point. However, in order to be able to train the submodels independently, that responsibility (which depends on the matching function but also on the other rules' responsibilities) is replaced with the matching function. required A_ALPHA float Scale parameter of weight vector variance prior. 10 ** -2 B_ALPHA float Shape parameter of weight vector variance prior. 10 ** -4 A_TAU float Scale parameter of noise variance prior. 10 ** -2 B_TAU float Shape parameter of noise variance prior. 10 ** -4 DELTA_S_L_K_Q float Stopping criterion for variational update loop. 10 ** -4 MAX_ITER_RULE int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). 20 **kwargs kwargs This is here so that we don't need to repeat all the hyperparameters in Mixture etc. Mixture simply passes through all **kwargs to both Mixing and Rule . This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. {} Source code in berbl/rule.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , match , A_ALPHA = 10 **- 2 , B_ALPHA = 10 **- 4 , A_TAU = 10 **- 2 , B_TAU = 10 **- 4 , DELTA_S_L_K_Q = 10 **- 4 , MAX_ITER_RULE = 20 , ** kwargs ): \"\"\" Parameters ---------- match : object ``match.match`` is this rule's match function. According to Drugowitsch's framework (or mixture of experts), each rule should get assigned a responsibility for each data point. However, in order to be able to train the submodels independently, that responsibility (which depends on the matching function but also on the other rules' responsibilities) is replaced with the matching function. A_ALPHA : float Scale parameter of weight vector variance prior. B_ALPHA : float Shape parameter of weight vector variance prior. A_TAU : float Scale parameter of noise variance prior. B_TAU : float Shape parameter of noise variance prior. DELTA_S_L_K_Q : float Stopping criterion for variational update loop. MAX_ITER_RULE : int Only perform up to this many iterations of variational updates (abort then, even if stopping criterion is not yet met). **kwargs : kwargs This is here so that we don't need to repeat all the hyperparameters in ``Mixture`` etc. ``Mixture`` simply passes through all ``**kwargs`` to both ``Mixing`` and ``Rule``. This means that during implementation, we need to be aware that if there are parameters in those two classes with the same name, they always receive the same value. \"\"\" self . match = match self . A_ALPHA = A_ALPHA self . B_ALPHA = B_ALPHA self . A_TAU = A_TAU self . B_TAU = B_TAU self . DELTA_S_L_K_Q = DELTA_S_L_K_Q self . MAX_ITER_RULE = MAX_ITER_RULE","title":"Rule"},{"location":"reference/berbl/rule/#berbl.rule.Rule.fit","text":"Fits this rule's (sub)model to the part of the provided data that the rule matches. See TrainClassifier [PDF p. 238]. Source code in berbl/rule.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fits this rule's (sub)model to the part of the provided data that the rule matches. See TrainClassifier [PDF p. 238]. \"\"\" self . m_ = self . match . match ( X ) N , self . DX_ = X . shape N , self . Dy_ = y . shape X_ = X * np . sqrt ( self . m_ ) y_ = y * np . sqrt ( self . m_ ) E_alpha_alpha = self . A_ALPHA / self . B_ALPHA self . a_alpha_ = self . A_ALPHA + self . DX_ * self . Dy_ / 2 # self.a_tau_ is constant. self . a_tau_ = self . A_TAU + 0.5 * np . sum ( self . m_ ) self . b_tau_ = self . B_TAU # TODO Why not precompute L_q_ using self.var_bound? self . L_q_ = - np . inf delta_L_q = self . DELTA_S_L_K_Q + 1 iter = 0 while delta_L_q > self . DELTA_S_L_K_Q and iter < self . MAX_ITER_RULE : iter += 1 self . Lambda_ = np . diag ([ E_alpha_alpha ] * self . DX_ ) + X_ . T @ X_ # While, in theory, Lambda is always invertible here and we thus # should be able to use inv (as it is described in the algorithm we # implement), we (seldomly) get a singular matrix, probably due to # numerical issues. Thus we simply use pinv which yields the same # result as inv anyways if the matrix is in fact non-singular. Also, # in his own code, Drugowitsch always uses pseudo inverse here. self . Lambda_1_ = np . linalg . pinv ( self . Lambda_ ) self . W_ = y_ . T @ X_ @ self . Lambda_1_ self . b_tau_ = self . B_TAU + 1 / ( 2 * self . Dy_ ) * ( np . sum ( y_ * y_ ) - np . sum ( self . W_ * ( self . W_ @ self . Lambda_ ))) E_tau_tau = self . a_tau_ / self . b_tau_ # Dy factor in front of trace due to sum over Dy elements (7.100). self . b_alpha_ = self . B_ALPHA + 0.5 * ( E_tau_tau * np . sum ( self . W_ * self . W_ ) + self . Dy_ * np . trace ( self . Lambda_1_ )) E_alpha_alpha = self . a_alpha_ / self . b_alpha_ L_q_prev = self . L_q_ self . L_q_ = self . var_bound ( X = X , y = y , # Substitute r by m in order to train submodels independently # (see [PDF p. 219]). Note, however, that after having trained # the mixing model we finally evaluate the submodels using # ``r=R[:,[k]]`` as intended. r = self . m_ ) delta_L_q = self . L_q_ - L_q_prev return self","title":"fit()"},{"location":"reference/berbl/rule/#berbl.rule.Rule.predict","text":"The rule's submodel's mean at the given positions; may serve as a prediction. Parameters: Name Type Description Default X array of shape (N, DX) required Returns: Name Type Description mean array of shape (N, Dy) Source code in berbl/rule.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def predict ( self , X ): \"\"\" The rule's submodel's mean at the given positions; may serve as a prediction. Parameters ---------- X : array of shape (N, DX) Returns ------- mean : array of shape (N, Dy) \"\"\" return X @ self . W_ . T","title":"predict()"},{"location":"reference/berbl/rule/#berbl.rule.Rule.predict_var","text":"The rule's submodel's variance at the given positions; may serve as some kind of confidence estimate for the prediction. The model currently assumes the same variance in all dimensions; thus the same value is repeated for each dimension. Parameters: Name Type Description Default X array of shape (N, DX) required Returns: Name Type Description variance array of shape (N, Dy) Source code in berbl/rule.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def predict_var ( self , X ): \"\"\" The rule's submodel's variance at the given positions; may serve as some kind of confidence estimate for the prediction. The model currently assumes the same variance in all dimensions; thus the same value is repeated for each dimension. Parameters ---------- X : array of shape (N, DX) Returns ------- variance : array of shape (N, Dy) \"\"\" # The sum corresponds to x @ self.Lambda_1 @ x for each x in X (i.e. # np.diag(X @ self.Lambda_1_ @ X.T)). var = 2 * self . b_tau_ / ( self . a_tau_ - 1 ) * ( 1 + np . sum ( ( X @ self . Lambda_1_ ) * X , axis = 1 )) # The same value is repeated for each dimension since the model # currently assumes the same variance in all dimensions. return var [:, np . newaxis ] . repeat ( self . Dy_ , axis = 1 )","title":"predict_var()"},{"location":"reference/berbl/rule/#berbl.rule.Rule.var_bound","text":"The components of the variational bound specific to this rule. See VarClBound [PDF p. 247]. Parameters: Name Type Description Default X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required r array of shape (N, 1) Responsibilities (during training replaced with matching array of this rule in order to enable independent submodel training). required Source code in berbl/rule.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def var_bound ( self , X : np . ndarray , y : np . ndarray , r : np . ndarray ): \"\"\" The components of the variational bound specific to this rule. See VarClBound [PDF p. 247]. Parameters ---------- X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. r : array of shape (N, 1) Responsibilities (during training replaced with matching array of this rule in order to enable independent submodel training). \"\"\" E_tau_tau = self . a_tau_ / self . b_tau_ L_1_q = self . Dy_ / 2 * ( ss . digamma ( self . a_tau_ ) - np . log ( self . b_tau_ ) - np . log ( 2 * np . pi )) * np . sum ( r ) # We reshape r to a NumPy row vector since NumPy seems to understand # what we want to do when we multiply two row vectors (i.e. a^T a). L_2_q = ( - 0.5 * r ) . reshape ( ( - 1 )) @ ( E_tau_tau * np . sum (( y - X @ self . W_ . T ) ** 2 , axis = 1 ) + self . Dy_ * np . sum ( X * ( X @ self . Lambda_1_ ), axis = 1 )) L_3_q = - ss . gammaln ( self . A_ALPHA ) + self . A_ALPHA * np . log ( self . B_ALPHA ) + ss . gammaln ( self . a_alpha_ ) - self . a_alpha_ * np . log ( self . b_alpha_ ) + self . DX_ * self . Dy_ / 2 + self . Dy_ / 2 * np . log ( np . linalg . det ( self . Lambda_1_ )) L_4_q = self . Dy_ * ( - ss . gammaln ( self . A_TAU ) + self . A_TAU * np . log ( self . B_TAU ) + ( self . A_TAU - self . a_tau_ ) * ss . digamma ( self . a_tau_ ) - self . A_TAU * np . log ( self . b_tau_ ) - self . B_TAU * E_tau_tau + ss . gammaln ( self . a_tau_ ) + self . a_tau_ ) return L_1_q + L_2_q + L_3_q + L_4_q","title":"var_bound()"},{"location":"reference/berbl/utils/","text":"add_bias ( X ) Prefixes each input vector (i.e. row) in the given input matrix with 1 for fitting the intercept. :param X: input data as an (N, DX) matrix :returns: a (N, DX + 1) matrix where each row is the corresponding original matrix's row prefixed with 1 Source code in berbl/utils.py 65 66 67 68 69 70 71 72 73 74 75 76 def add_bias ( X : np . ndarray ): \"\"\" Prefixes each input vector (i.e. row) in the given input matrix with 1 for fitting the intercept. :param X: input data as an ``(N, DX)`` matrix :returns: a ``(N, DX + 1)`` matrix where each row is the corresponding original matrix's row prefixed with 1 \"\"\" N , DX = X . shape return np . hstack ([ np . ones (( N , 1 )), X ]) ball_vol ( r , n ) Volume of an n-ball with the given radius. Parameters: Name Type Description Default r float Radius. required n int Dimensionality. required Source code in berbl/utils.py 159 160 161 162 163 164 165 166 167 168 169 170 def ball_vol ( r : float , n : int ): \"\"\" Volume of an n-ball with the given radius. Parameters ---------- r : float Radius. n : int Dimensionality. \"\"\" return np . pi ** ( n / 2 ) / sp . gamma ( n / 2 + 1 ) * r ** n check_phi ( phi , X ) Given a mixing feature mapping phi , compute the mixing feature matrix Phi . If phi is None , use the default LCS mixing feature mapping, i.e. a mixing feature vector of phi(x) = 1 for each data point x . Parameters: Name Type Description Default phi callable receiving Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . required X array of shape (N, DX) Input matrix. required Returns: Name Type Description Phi array of shape (N, DV) Mixing feature matrix. Source code in berbl/utils.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def check_phi ( phi , X : np . ndarray ): \"\"\" Given a mixing feature mapping ``phi``, compute the mixing feature matrix ``Phi``. If ``phi`` is ``None``, use the default LCS mixing feature mapping, i.e. a mixing feature vector of ``phi(x) = 1`` for each data point ``x``. Parameters ---------- phi : callable receiving ``X`` or ``None`` Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. X : array of shape (N, DX) Input matrix. Returns ------- Phi : array of shape (N, DV) Mixing feature matrix. \"\"\" # NOTE This is named like this in order to stay close to sklearn's naming # scheme (e.g. check_random_state etc.). N , _ = X . shape if phi is None : Phi = np . ones (( N , 1 )) else : Phi = phi ( X ) return Phi ellipsoid_vol ( rs , n ) Volume of an ellipsoid with the given radii. Parameters: Name Type Description Default rs array of shape Radius. required n int Dimensionality. required Source code in berbl/utils.py 173 174 175 176 177 178 179 180 181 182 183 184 def ellipsoid_vol ( rs : np . ndarray , n : int ): \"\"\" Volume of an ellipsoid with the given radii. Parameters ---------- rs : array of shape ``(n)`` Radius. n : int Dimensionality. \"\"\" return np . pi ** ( n / 2 ) / sp . gamma ( n / 2 + 1 ) * np . prod ( rs ) get_ranges ( X ) Computes the value range for each dimension. :param X: input data as an (N, DX) matrix :returns: a (DX, 2) matrix where each row consists the minimum and maximum in the respective dimension Source code in berbl/utils.py 53 54 55 56 57 58 59 60 61 62 def get_ranges ( X : np . ndarray ): \"\"\" Computes the value range for each dimension. :param X: input data as an ``(N, DX)`` matrix :returns: a ``(DX, 2)`` matrix where each row consists the minimum and maximum in the respective dimension \"\"\" return np . vstack ([ np . min ( X , axis = 0 ), np . max ( X , axis = 0 )]) . T initRepeat_binom ( container , func , n , p , random_state , kmin = 1 , kmax = 100 ) Alternative to deap.tools.initRepeat that samples individual sizes from a binomial distribution B(n, p). Source code in berbl/utils.py 243 244 245 246 247 248 249 def initRepeat_binom ( container , func , n , p , random_state , kmin = 1 , kmax = 100 ): \"\"\" Alternative to `deap.tools.initRepeat` that samples individual sizes from a binomial distribution B(n, p). \"\"\" size = np . clip ( random_state . binomial ( n , p ), kmin , kmax ) return tools . initRepeat ( container , func , size ) known_issue ( expl , variables , report = False ) Document a known issue. Source code in berbl/utils.py 252 253 254 255 256 257 258 259 260 261 def known_issue ( expl , variables , report = False ): \"\"\" Document a known issue. \"\"\" print ( f \"Warning: { expl } .\" ) if report : print ( \"This should not have occurred, please report it!\" ) else : print ( \"This is a known issue and can probably be ignored.\" ) print ( f \"Relevant variables: { variables } .\" ) logstartstop ( f ) Simple decorator for adding stdout prints when the given callable is called and when it returns. Source code in berbl/utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def logstartstop ( f ): \"\"\" Simple decorator for adding stdout prints when the given callable is called and when it returns. \"\"\" @wraps ( f ) def wrap ( * args , ** kw ): ts = time () print ( f \"Start { f . __name__ } at { asctime ( localtime ( ts )) } \" ) r = f ( * args , ** kw ) te = time () print ( f \"Stop { f . __name__ } after %2.4f s\" % ( te - ts )) return r return wrap matching_matrix ( matchs , X ) :param ind: an individual for which the matching matrix is returned :param X: input matrix (N \u00d7 DX) :returns: matching matrix (N \u00d7 K) Source code in berbl/utils.py 232 233 234 235 236 237 238 239 240 def matching_matrix ( matchs : List , X : np . ndarray ): \"\"\" :param ind: an individual for which the matching matrix is returned :param X: input matrix (N \u00d7 DX) :returns: matching matrix (N \u00d7 K) \"\"\" # TODO Can we maybe vectorize this? return np . hstack ([ m . match ( X ) for m in matchs ]) pr_in_sd ( n = 3 , r = 1 ) Expected percentage of examples falling within multiples of a standard deviation of a multivariate Gaussian distribution. Reference for the used formulae: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118537 . Parameters: Name Type Description Default n positive int Dimensionality of the Gaussian. 3 r float greater than 1 Factor for standard deviation radius. Numerical issues(?) if radius too close to zero. We probably never want require r < 1, though, so this is probably fine. 1 Source code in berbl/utils.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def pr_in_sd ( n = 3 , r = 1 ): \"\"\" Expected percentage of examples falling within multiples of a standard deviation of a multivariate Gaussian distribution. Reference for the used formulae: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118537 . Parameters ---------- n : positive int Dimensionality of the Gaussian. r : float greater than 1 Factor for standard deviation radius. Numerical issues(?) if radius too close to zero. We probably never want require r < 1, though, so this is probably fine. \"\"\" if r < 1 : raise ValueError ( f \"r = { r } < 1 may result in numerical issues\" ) if n == 1 : return pr_in_sd1 ( r = r ) elif n == 2 : return pr_in_sd2 ( r = r ) elif n >= 3 : ci = pr_in_sd ( n = n - 2 , r = r ) - ( r / np . sqrt ( 2 )) ** ( n - 2 ) * np . exp ( - ( r ** 2 ) / 2 ) / sp . gamma ( n / 2 ) if ci < 0 and np . isclose ( ci , 0 ): return 0 else : return ci else : raise ValueError ( \"n must be positive\" ) pr_in_sd1 ( r = 1 ) Expected percentage of examples falling within one standard deviation of a one-dimensional Gaussian distribution. See pr_in_sd . Parameters: Name Type Description Default r float Radius (in multiples of standard deviation). 1 Source code in berbl/utils.py 79 80 81 82 83 84 85 86 87 88 89 90 def pr_in_sd1 ( r = 1 ): \"\"\" Expected percentage of examples falling within one standard deviation of a one-dimensional Gaussian distribution. See ``pr_in_sd``. Parameters ---------- r : float Radius (in multiples of standard deviation). \"\"\" # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erf.html return sp . erf ( r / np . sqrt ( 2 )) pr_in_sd2 ( r = 1 ) Expected percentage of examples falling within one standard deviation of a two-dimensional Gaussian distribution. See pr_in_sd . Parameters: Name Type Description Default r float Radius (in multiples of standard deviation). 1 Source code in berbl/utils.py 93 94 95 96 97 98 99 100 101 102 103 def pr_in_sd2 ( r = 1 ): \"\"\" Expected percentage of examples falling within one standard deviation of a two-dimensional Gaussian distribution. See ``pr_in_sd``. Parameters ---------- r : float Radius (in multiples of standard deviation). \"\"\" return 1 - np . exp ( - ( r ** 2 ) / 2 ) radius_for_ci ( n = 3 , ci = 0.5 ) Calculate how many standard deviations are required to fulfill the given confidence interval for a multivariate Gaussian of the given dimensionality. Source code in berbl/utils.py 144 145 146 147 148 149 150 151 152 def radius_for_ci ( n = 3 , ci = 0.5 ): \"\"\" Calculate how many standard deviations are required to fulfill the given confidence interval for a multivariate Gaussian of the given dimensionality. \"\"\" # Other than in this German Wikipedia article we actually need to use # SciPy's inverse of the *lower* incomplete gamma function: # https://de.wikipedia.org/wiki/Mehrdimensionale_Normalverteilung return np . sqrt ( 2 * sp . gammaincinv ( n / 2 , ci )) randseed ( random_state ) Sometimes we need to generate a new random seed from a RandomState due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy RandomState API, DEAP uses random.random ). Source code in berbl/utils.py 15 16 17 18 19 20 21 22 def randseed ( random_state : np . random . RandomState ): \"\"\" Sometimes we need to generate a new random seed from a ``RandomState`` due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy ``RandomState`` API, DEAP uses ``random.random``). \"\"\" # Highest possible seed is `2**32 - 1` for NumPy legacy generators. return random_state . randint ( 2 ** 32 - 1 ) randseed_legacy () Sometimes we need to generate a new random seed for a RandomState from the standard random library due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy RandomState API, DEAP uses random.random ). Source code in berbl/utils.py 25 26 27 28 29 30 31 32 33 def randseed_legacy (): \"\"\" Sometimes we need to generate a new random seed for a ``RandomState`` from the standard random library due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy ``RandomState`` API, DEAP uses ``random.random``). \"\"\" # Highest possible seed is `2**32 - 1` for NumPy legacy generators. return random . randint ( 0 , 2 ** 32 - 1 ) space_vol ( dim ) The volume of an [-1, 1]^dim space. Source code in berbl/utils.py 191 192 193 194 195 def space_vol ( dim ): \"\"\" The volume of an ``[-1, 1]^dim`` space. \"\"\" return 2. ** dim t ( mu , prec , df ) Alternative form of the Student's t distribution used by Drugowtisch (see e.g. (Bishop, 2006)). Parameters: Name Type Description Default mu float or array Mean of the distribution. required prec float or array \u201cPrecision\u201d of the distribution (although \u201cnot in general equal to the inverse of the variance\u201d, see (Bishop, 2006)). required df float or array Degrees of freedom. required Returns: Type Description callable A probability density function. Source code in berbl/utils.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def t ( mu , prec , df ): \"\"\" Alternative form of the Student's t distribution used by Drugowtisch (see e.g. (Bishop, 2006)). Parameters ---------- mu : float or array Mean of the distribution. prec : float or array \u201cPrecision\u201d of the distribution (although \u201cnot in general equal to the inverse of the variance\u201d, see (Bishop, 2006)). df : float or array Degrees of freedom. Returns ------- callable A probability density function. \"\"\" def pdf ( X ): # Repeat X so that we can perform vectorized calculation. X = X [:, np . newaxis ] . repeat ( len ( mu ), axis = 1 ) return sp . gamma (( df + 1 ) / 2 ) / sp . gamma ( df / 2 ) * np . sqrt ( prec / ( np . pi * df )) * ( 1 + ( prec * ( X - mu ) ** 2 ) / df ) ** ( - ( df + 1 ) / 2 ) return pdf","title":"utils"},{"location":"reference/berbl/utils/#berbl.utils.add_bias","text":"Prefixes each input vector (i.e. row) in the given input matrix with 1 for fitting the intercept. :param X: input data as an (N, DX) matrix :returns: a (N, DX + 1) matrix where each row is the corresponding original matrix's row prefixed with 1 Source code in berbl/utils.py 65 66 67 68 69 70 71 72 73 74 75 76 def add_bias ( X : np . ndarray ): \"\"\" Prefixes each input vector (i.e. row) in the given input matrix with 1 for fitting the intercept. :param X: input data as an ``(N, DX)`` matrix :returns: a ``(N, DX + 1)`` matrix where each row is the corresponding original matrix's row prefixed with 1 \"\"\" N , DX = X . shape return np . hstack ([ np . ones (( N , 1 )), X ])","title":"add_bias()"},{"location":"reference/berbl/utils/#berbl.utils.ball_vol","text":"Volume of an n-ball with the given radius. Parameters: Name Type Description Default r float Radius. required n int Dimensionality. required Source code in berbl/utils.py 159 160 161 162 163 164 165 166 167 168 169 170 def ball_vol ( r : float , n : int ): \"\"\" Volume of an n-ball with the given radius. Parameters ---------- r : float Radius. n : int Dimensionality. \"\"\" return np . pi ** ( n / 2 ) / sp . gamma ( n / 2 + 1 ) * r ** n","title":"ball_vol()"},{"location":"reference/berbl/utils/#berbl.utils.check_phi","text":"Given a mixing feature mapping phi , compute the mixing feature matrix Phi . If phi is None , use the default LCS mixing feature mapping, i.e. a mixing feature vector of phi(x) = 1 for each data point x . Parameters: Name Type Description Default phi callable receiving Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . required X array of shape (N, DX) Input matrix. required Returns: Name Type Description Phi array of shape (N, DV) Mixing feature matrix. Source code in berbl/utils.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def check_phi ( phi , X : np . ndarray ): \"\"\" Given a mixing feature mapping ``phi``, compute the mixing feature matrix ``Phi``. If ``phi`` is ``None``, use the default LCS mixing feature mapping, i.e. a mixing feature vector of ``phi(x) = 1`` for each data point ``x``. Parameters ---------- phi : callable receiving ``X`` or ``None`` Mixing feature extractor (N \u00d7 DX \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. X : array of shape (N, DX) Input matrix. Returns ------- Phi : array of shape (N, DV) Mixing feature matrix. \"\"\" # NOTE This is named like this in order to stay close to sklearn's naming # scheme (e.g. check_random_state etc.). N , _ = X . shape if phi is None : Phi = np . ones (( N , 1 )) else : Phi = phi ( X ) return Phi","title":"check_phi()"},{"location":"reference/berbl/utils/#berbl.utils.ellipsoid_vol","text":"Volume of an ellipsoid with the given radii. Parameters: Name Type Description Default rs array of shape Radius. required n int Dimensionality. required Source code in berbl/utils.py 173 174 175 176 177 178 179 180 181 182 183 184 def ellipsoid_vol ( rs : np . ndarray , n : int ): \"\"\" Volume of an ellipsoid with the given radii. Parameters ---------- rs : array of shape ``(n)`` Radius. n : int Dimensionality. \"\"\" return np . pi ** ( n / 2 ) / sp . gamma ( n / 2 + 1 ) * np . prod ( rs )","title":"ellipsoid_vol()"},{"location":"reference/berbl/utils/#berbl.utils.get_ranges","text":"Computes the value range for each dimension. :param X: input data as an (N, DX) matrix :returns: a (DX, 2) matrix where each row consists the minimum and maximum in the respective dimension Source code in berbl/utils.py 53 54 55 56 57 58 59 60 61 62 def get_ranges ( X : np . ndarray ): \"\"\" Computes the value range for each dimension. :param X: input data as an ``(N, DX)`` matrix :returns: a ``(DX, 2)`` matrix where each row consists the minimum and maximum in the respective dimension \"\"\" return np . vstack ([ np . min ( X , axis = 0 ), np . max ( X , axis = 0 )]) . T","title":"get_ranges()"},{"location":"reference/berbl/utils/#berbl.utils.initRepeat_binom","text":"Alternative to deap.tools.initRepeat that samples individual sizes from a binomial distribution B(n, p). Source code in berbl/utils.py 243 244 245 246 247 248 249 def initRepeat_binom ( container , func , n , p , random_state , kmin = 1 , kmax = 100 ): \"\"\" Alternative to `deap.tools.initRepeat` that samples individual sizes from a binomial distribution B(n, p). \"\"\" size = np . clip ( random_state . binomial ( n , p ), kmin , kmax ) return tools . initRepeat ( container , func , size )","title":"initRepeat_binom()"},{"location":"reference/berbl/utils/#berbl.utils.known_issue","text":"Document a known issue. Source code in berbl/utils.py 252 253 254 255 256 257 258 259 260 261 def known_issue ( expl , variables , report = False ): \"\"\" Document a known issue. \"\"\" print ( f \"Warning: { expl } .\" ) if report : print ( \"This should not have occurred, please report it!\" ) else : print ( \"This is a known issue and can probably be ignored.\" ) print ( f \"Relevant variables: { variables } .\" )","title":"known_issue()"},{"location":"reference/berbl/utils/#berbl.utils.logstartstop","text":"Simple decorator for adding stdout prints when the given callable is called and when it returns. Source code in berbl/utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def logstartstop ( f ): \"\"\" Simple decorator for adding stdout prints when the given callable is called and when it returns. \"\"\" @wraps ( f ) def wrap ( * args , ** kw ): ts = time () print ( f \"Start { f . __name__ } at { asctime ( localtime ( ts )) } \" ) r = f ( * args , ** kw ) te = time () print ( f \"Stop { f . __name__ } after %2.4f s\" % ( te - ts )) return r return wrap","title":"logstartstop()"},{"location":"reference/berbl/utils/#berbl.utils.matching_matrix","text":":param ind: an individual for which the matching matrix is returned :param X: input matrix (N \u00d7 DX) :returns: matching matrix (N \u00d7 K) Source code in berbl/utils.py 232 233 234 235 236 237 238 239 240 def matching_matrix ( matchs : List , X : np . ndarray ): \"\"\" :param ind: an individual for which the matching matrix is returned :param X: input matrix (N \u00d7 DX) :returns: matching matrix (N \u00d7 K) \"\"\" # TODO Can we maybe vectorize this? return np . hstack ([ m . match ( X ) for m in matchs ])","title":"matching_matrix()"},{"location":"reference/berbl/utils/#berbl.utils.pr_in_sd","text":"Expected percentage of examples falling within multiples of a standard deviation of a multivariate Gaussian distribution. Reference for the used formulae: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118537 . Parameters: Name Type Description Default n positive int Dimensionality of the Gaussian. 3 r float greater than 1 Factor for standard deviation radius. Numerical issues(?) if radius too close to zero. We probably never want require r < 1, though, so this is probably fine. 1 Source code in berbl/utils.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def pr_in_sd ( n = 3 , r = 1 ): \"\"\" Expected percentage of examples falling within multiples of a standard deviation of a multivariate Gaussian distribution. Reference for the used formulae: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118537 . Parameters ---------- n : positive int Dimensionality of the Gaussian. r : float greater than 1 Factor for standard deviation radius. Numerical issues(?) if radius too close to zero. We probably never want require r < 1, though, so this is probably fine. \"\"\" if r < 1 : raise ValueError ( f \"r = { r } < 1 may result in numerical issues\" ) if n == 1 : return pr_in_sd1 ( r = r ) elif n == 2 : return pr_in_sd2 ( r = r ) elif n >= 3 : ci = pr_in_sd ( n = n - 2 , r = r ) - ( r / np . sqrt ( 2 )) ** ( n - 2 ) * np . exp ( - ( r ** 2 ) / 2 ) / sp . gamma ( n / 2 ) if ci < 0 and np . isclose ( ci , 0 ): return 0 else : return ci else : raise ValueError ( \"n must be positive\" )","title":"pr_in_sd()"},{"location":"reference/berbl/utils/#berbl.utils.pr_in_sd1","text":"Expected percentage of examples falling within one standard deviation of a one-dimensional Gaussian distribution. See pr_in_sd . Parameters: Name Type Description Default r float Radius (in multiples of standard deviation). 1 Source code in berbl/utils.py 79 80 81 82 83 84 85 86 87 88 89 90 def pr_in_sd1 ( r = 1 ): \"\"\" Expected percentage of examples falling within one standard deviation of a one-dimensional Gaussian distribution. See ``pr_in_sd``. Parameters ---------- r : float Radius (in multiples of standard deviation). \"\"\" # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erf.html return sp . erf ( r / np . sqrt ( 2 ))","title":"pr_in_sd1()"},{"location":"reference/berbl/utils/#berbl.utils.pr_in_sd2","text":"Expected percentage of examples falling within one standard deviation of a two-dimensional Gaussian distribution. See pr_in_sd . Parameters: Name Type Description Default r float Radius (in multiples of standard deviation). 1 Source code in berbl/utils.py 93 94 95 96 97 98 99 100 101 102 103 def pr_in_sd2 ( r = 1 ): \"\"\" Expected percentage of examples falling within one standard deviation of a two-dimensional Gaussian distribution. See ``pr_in_sd``. Parameters ---------- r : float Radius (in multiples of standard deviation). \"\"\" return 1 - np . exp ( - ( r ** 2 ) / 2 )","title":"pr_in_sd2()"},{"location":"reference/berbl/utils/#berbl.utils.radius_for_ci","text":"Calculate how many standard deviations are required to fulfill the given confidence interval for a multivariate Gaussian of the given dimensionality. Source code in berbl/utils.py 144 145 146 147 148 149 150 151 152 def radius_for_ci ( n = 3 , ci = 0.5 ): \"\"\" Calculate how many standard deviations are required to fulfill the given confidence interval for a multivariate Gaussian of the given dimensionality. \"\"\" # Other than in this German Wikipedia article we actually need to use # SciPy's inverse of the *lower* incomplete gamma function: # https://de.wikipedia.org/wiki/Mehrdimensionale_Normalverteilung return np . sqrt ( 2 * sp . gammaincinv ( n / 2 , ci ))","title":"radius_for_ci()"},{"location":"reference/berbl/utils/#berbl.utils.randseed","text":"Sometimes we need to generate a new random seed from a RandomState due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy RandomState API, DEAP uses random.random ). Source code in berbl/utils.py 15 16 17 18 19 20 21 22 def randseed ( random_state : np . random . RandomState ): \"\"\" Sometimes we need to generate a new random seed from a ``RandomState`` due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy ``RandomState`` API, DEAP uses ``random.random``). \"\"\" # Highest possible seed is `2**32 - 1` for NumPy legacy generators. return random_state . randint ( 2 ** 32 - 1 )","title":"randseed()"},{"location":"reference/berbl/utils/#berbl.utils.randseed_legacy","text":"Sometimes we need to generate a new random seed for a RandomState from the standard random library due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy RandomState API, DEAP uses random.random ). Source code in berbl/utils.py 25 26 27 28 29 30 31 32 33 def randseed_legacy (): \"\"\" Sometimes we need to generate a new random seed for a ``RandomState`` from the standard random library due to different APIs (e.g. NumPy wants the new rng API, scikit-learn uses the legacy NumPy ``RandomState`` API, DEAP uses ``random.random``). \"\"\" # Highest possible seed is `2**32 - 1` for NumPy legacy generators. return random . randint ( 0 , 2 ** 32 - 1 )","title":"randseed_legacy()"},{"location":"reference/berbl/utils/#berbl.utils.space_vol","text":"The volume of an [-1, 1]^dim space. Source code in berbl/utils.py 191 192 193 194 195 def space_vol ( dim ): \"\"\" The volume of an ``[-1, 1]^dim`` space. \"\"\" return 2. ** dim","title":"space_vol()"},{"location":"reference/berbl/utils/#berbl.utils.t","text":"Alternative form of the Student's t distribution used by Drugowtisch (see e.g. (Bishop, 2006)). Parameters: Name Type Description Default mu float or array Mean of the distribution. required prec float or array \u201cPrecision\u201d of the distribution (although \u201cnot in general equal to the inverse of the variance\u201d, see (Bishop, 2006)). required df float or array Degrees of freedom. required Returns: Type Description callable A probability density function. Source code in berbl/utils.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def t ( mu , prec , df ): \"\"\" Alternative form of the Student's t distribution used by Drugowtisch (see e.g. (Bishop, 2006)). Parameters ---------- mu : float or array Mean of the distribution. prec : float or array \u201cPrecision\u201d of the distribution (although \u201cnot in general equal to the inverse of the variance\u201d, see (Bishop, 2006)). df : float or array Degrees of freedom. Returns ------- callable A probability density function. \"\"\" def pdf ( X ): # Repeat X so that we can perform vectorized calculation. X = X [:, np . newaxis ] . repeat ( len ( mu ), axis = 1 ) return sp . gamma (( df + 1 ) / 2 ) / sp . gamma ( df / 2 ) * np . sqrt ( prec / ( np . pi * df )) * ( 1 + ( prec * ( X - mu ) ** 2 ) / df ) ** ( - ( df + 1 ) / 2 ) return pdf","title":"t()"},{"location":"reference/berbl/literal/","text":"Module implementing the algorithm presented in \u2018Design and Analysis of Learning Classifier Systems \u2013 A Probabilistic Approach\u2019 by Jan Drugowitsch. This implementation intentionally breaks with several Python conventions (e.g. PEP8 regarding variable naming) in order to stay as close as possible to the formulation of the algorithm in aforementioned work. Also, other than in the remainder of this implementation, we do not as strictly avoid the overloaded term \u201cclassifier\u201d within this module since it is sometimes used by the original algorithms. The only deviations from the book are: model_probability returns L(q) - ln K! instead of L(q) + ln K! as the latter is presumably a typographical error in the book (the corresponding formula in Section 7 uses - as well, which seems to be correct). We initialize the mixing model parameters V using the correct scale of b_beta / a_beta (there is a typographical error in the TrainMixing algorithm in Drugowitsch's book). We always use Moore-Penrose pseudo-inverses instead of actual inverses due to (very seldomly) matrices being invertible\u2014probably due to numerical inaccuracies. This is also done in the code that Jan Drugowitsch published to accompany his book: 1 <https://github.com/jdrugo/LCSBookCode/blob/master/cl.py#L120> , 2 <https://github.com/jdrugo/LCSBookCode/blob/master/cl.py#L385> , 3 <https://github.com/jdrugo/LCSBookCode/blob/master/cl.py#L409> _. Since the IRLS training of the mixing weights sometimes starts to oscillate in an infinite loop between several weight values, we add a maximum number of iterations to the three main training loops: submodel training ( train_classifier ) mixing model training ( train_mixing ) mixing weight training ( train_mix_weights ) This seems reasonable, especially since Jan Drugowitsch's code does the same (a behaviour that is not documented in the book ). * Since the oscillations in train_mix_weights are (at least sometimes) caused by the Kullback-Leibler divergence between G and R being optimal followed by another unnecessary execution of the loop thereafter we also abort if that is the case (i.e. if the Kullback-Leibler divergence is zero) and always compute the divergence before starting the loop first time. * We deal with minor numerical issues in a few places (e.g. in train_mix_priors , responsibilities ). Within the code, comments referring to \u201cLCSBookCode\u201d refer to Jan Drugowitsch's code <https://github.com/jdrugo/LCSBookCode> _. _kl ( R , G ) Computes the negative Kullback-Leibler divergence between the given arrays. Drugowitsch does not introduce this subroutine. We do so to reduce code duplication in train_mix_weights (where we deviated from the original text by one additional calculation of _kl(R, G) ). Source code in berbl/literal/__init__.py 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 def _kl ( R , G ): \"\"\" Computes the negative Kullback-Leibler divergence between the given arrays. Drugowitsch does not introduce this subroutine. We do so to reduce code duplication in ``train_mix_weights`` (where we deviated from the original text by one additional calculation of ``_kl(R, G)``). \"\"\" # NumPy uses subnormal numbers \u201cto fill the gap between 0 and # [np.finfo(None).tiny]\u201d. This means that R may contain elements that do not # silently lead to inf but raise a \u201cFloatingPointError: overflow encountered # in true_divide\u201d instead. We simply set anything smaller than tiny to 0. R [ np . where ( R < np . finfo ( None ) . tiny )] = 0 # ``responsibilities`` performs a ``nan_to_num(\u2026, nan=0, \u2026)``, so we might # divide by 0 here due to 0's in ``R``. The intended behaviour is to # silently get a NaN that can then be replaced by 0 again (this is how # Drugowitsch does it [PDF p. 213]). Drugowitsch expects dividing ``x`` by # 0 to result in NaN, however, in Python this is only true for ``x == 0``; # for any other ``x`` this instead results in ``inf`` (with sign depending # on the sign of x). The two cases also throw different errors (\u2018invalid # value encountered\u2019 for ``x == 0`` and \u2018divide by zero\u2019 otherwise). # # I don't think the neginf is strictly required but let's be safe. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): KLRG = np . sum ( R * np . nan_to_num ( np . log ( G / R ), nan = 0 , posinf = 0 , neginf = 0 )) # This fixes(?) some numerical problems. Note that KLRG is actually the # negative Kullback-Leibler divergence (hence the > 0 comparison instead # of < 0). if KLRG > 0 and np . isclose ( KLRG , 0 ): KLRG = 0 assert KLRG <= 0 , ( f \"Kullback-Leibler divergence less than zero: \" f \"KLRG = { - KLRG } \" ) return KLRG hessian ( Phi , G , a_beta , b_beta ) [PDF p. 243] Parameters: Name Type Description Default Phi array of shape (N, DV) Mixing feature matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Type Description array of shape (K Hessian matrix. Source code in berbl/literal/__init__.py 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def hessian ( Phi : np . ndarray , G : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 243] Parameters ---------- Phi : array of shape (N, DV) Mixing feature matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- array of shape (K * DV, K * DV) Hessian matrix. \"\"\" N , DV = Phi . shape K , = a_beta . shape assert G . shape == ( N , K ) assert a_beta . shape == b_beta . shape H = np . zeros (( K * DV , K * DV )) for k in range ( K ): for j in range ( k ): lk = k * DV uk = ( k + 1 ) * DV lj = j * DV uj = ( j + 1 ) * DV H_kj = - Phi . T @ ( Phi * ( G [:, [ k ]] * G [:, [ j ]])) H [ lk : uk : 1 , lj : uj : 1 ] = H_kj H [ lj : uj : 1 , lk : uk : 1 ] = H_kj l = k * DV u = ( k + 1 ) * DV H [ l : u : 1 , l : u : 1 ] = Phi . T @ ( Phi * ( G [:, [ k ]] * ( 1 - G [:, [ k ]]))) + a_beta [ k ] / b_beta [ k ] * np . identity ( DV ) return H mixing ( M , Phi , V ) [PDF p. 239] Is zero wherever a rule does not match. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required Returns: Name Type Description G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Source code in berbl/literal/__init__.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def mixing ( M : np . ndarray , Phi : np . ndarray , V : np . ndarray ): \"\"\" [PDF p. 239] Is zero wherever a rule does not match. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. Returns ------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. \"\"\" DV , K = V . shape # If Phi is standard, this simply broadcasts V to a matrix [V, V, V, \u2026] of # shape (N, DV). G = Phi @ V G = np . clip ( G , HParams () . EXP_MIN , HParams () . LN_MAX - np . log ( K )) G = np . exp ( G ) * M # The sum can be 0 meaning we do 0/0 (== NaN) but we ignore it because it is # fixed one line later (this is how Drugowitsch does it). Drugowitsch does, # however, also say that: \u201cUsually, this should never happen as only model # structures are accepted where [(np.sum(G, 1) > 0).all()]. Nonetheless, # this check was added to ensure that even these cases are handled # gracefully.\u201d # # The also sometimes overflows which we fix as well. # TODO Why does the sum overflow sometimes? Is V *that* large? with np . errstate ( invalid = \"ignore\" , over = \"ignore\" ): G = G / np . sum ( G , axis = 1 )[:, np . newaxis ] G = np . nan_to_num ( G , nan = 1 / K , posinf = 1 / K ) return G model_probability ( matchs , X , Y , Phi , random_state , exp_min = np . log ( np . finfo ( None ) . tiny ), ln_max = np . log ( np . finfo ( None ) . max )) [PDF p. 235] Note that this deviates from [PDF p. 235] in that we return p(M | D) = L(q) - ln K! instead of L(q) + ln K! because the latter is not consistent with (7.3). We also compute the matching matrix within this function instead of providing it to it. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required Returns: Type Description metrics , params Model metrics and model parameters. Source code in berbl/literal/__init__.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def model_probability ( matchs : List , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , random_state : np . random . RandomState , exp_min : float = np . log ( np . finfo ( None ) . tiny ), ln_max : float = np . log ( np . finfo ( None ) . max )): \"\"\" [PDF p. 235] Note that this deviates from [PDF p. 235] in that we return ``p(M | D) = L(q) - ln K!`` instead of ``L(q) + ln K!`` because the latter is not consistent with (7.3). We also compute the matching matrix *within* this function instead of providing it to it. Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. Returns ------- metrics, params : pair of dict Model metrics and model parameters. \"\"\" # Underflows may occur in many places, e.g. if X contains values very close to # 0. However, they mostly occur in the very first training iterations so they # should be OK to ignore for now. We want to stop (for now) if any other # floating point error occurs, though. with np . errstate ( all = \"raise\" , under = \"ignore\" ): N , _ = X . shape K = len ( matchs ) M = matching_matrix ( matchs , X ) W = [ None ] * K Lambda_1 = [ None ] * K a_tau = [ None ] * K b_tau = [ None ] * K a_alpha = [ None ] * K b_alpha = [ None ] * K for k in range ( K ): W [ k ], Lambda_1 [ k ], a_tau [ k ], b_tau [ k ], a_alpha [ k ], b_alpha [ k ] = train_classifier ( M [:, [ k ]], X , Y ) V , Lambda_V_1 , a_beta , b_beta = train_mixing ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , exp_min = exp_min , ln_max = ln_max , random_state = random_state ) # We rename L_k_q to L_C_q. L_q , L_C_q , L_M_q = var_bound ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , a_alpha = a_alpha , b_alpha = b_alpha , V = V , Lambda_V_1 = Lambda_V_1 , a_beta = a_beta , b_beta = b_beta ) ln_p_M = - np . log ( float ( np . math . factorial ( K ))) p_M_D = L_q + ln_p_M return { \"p_M_D\" : p_M_D , \"L_q\" : L_q , \"ln_p_M\" : ln_p_M , \"L_C_q\" : L_C_q , \"L_M_q\" : L_M_q }, { \"matchs\" : matchs , \"W\" : W , \"Lambda_1\" : Lambda_1 , \"a_tau\" : a_tau , \"b_tau\" : b_tau , \"a_alpha\" : a_alpha , \"b_alpha\" : b_alpha , \"V\" : V , \"Lambda_V_1\" : Lambda_V_1 , \"a_beta\" : a_beta , \"b_beta\" : b_beta , } responsibilities ( X , Y , G , W , Lambda_1 , a_tau , b_tau ) [PDF p. 240] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param G: mixing (\u201cgating\u201d) matrix (N \u00d7 K) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :returns: responsibility matrix (N \u00d7 K) Source code in berbl/literal/__init__.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def responsibilities ( X : np . ndarray , Y : np . ndarray , G : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray ): \"\"\" [PDF p. 240] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param G: mixing (\u201cgating\u201d) matrix (N \u00d7 K) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :returns: responsibility matrix (N \u00d7 K) \"\"\" N , K = G . shape N , DY = Y . shape # This fixes instabilities that occur if there is only a single rule. if K == 1 : return np . ones (( N , K )) # We first create the transpose of R because indexing is easier (assigning # to rows instead of columns is rather straightforward). We then transpose # before multiplying elementwise with G. R_T = np . zeros (( K , N )) for k in range ( K ): R_T [ k ] = np . exp ( DY / 2 * ( ss . digamma ( a_tau [ k ]) - np . log ( b_tau [ k ])) - 0.5 * ( a_tau [ k ] / b_tau [ k ] * np . sum (( Y - X @ W [ k ] . T ) ** 2 , 1 ) + DY * np . sum ( X * ( X @ Lambda_1 [ k ]), 1 ))) R = R_T . T * G # Make a copy of the reference for checking for nans a few lines later. R_ = R # The sum can be 0 meaning we do 0/0 (== NaN in Python) but we ignore it # because it is fixed one line later (this is how Drugowitsch does it). with np . errstate ( invalid = \"ignore\" ): R = R / np . sum ( R , 1 )[:, np . newaxis ] # This is safer than Drugowitsch's plain `R = np.nan_to_num(R, nan=0)` # (i.e. we check whether the nan really came from the cause described above # at the cost of an additional run over R to check for zeroes). R [ np . where ( np . logical_and ( R_ == 0 , np . isnan ( R )))] = 0 return R train_classifier ( m_k , X , Y ) [PDF p. 238] Parameters: Name Type Description Default m_k array of shape (N,) Matching vector of rule k. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Returns: Type Description W_k , Lambda_k_1 , a_tau_k , b_tau_k , a_alpha_k , b_alpha_k Weight matrix (DY \u00d7 DX), covariance matrix (DX \u00d7 DX), noise precision parameters, weight vector parameters. Source code in berbl/literal/__init__.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def train_classifier ( m_k , X , Y ): \"\"\" [PDF p. 238] Parameters ---------- m_k : array of shape (N,) Matching vector of rule k. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Returns ------- W_k, Lambda_k_1, a_tau_k, b_tau_k, a_alpha_k, b_alpha_k : arrays of shapes (DY, DX) and (DX, DX) and float Weight matrix (DY \u00d7 DX), covariance matrix (DX \u00d7 DX), noise precision parameters, weight vector parameters. \"\"\" N , DX = X . shape N , DY = Y . shape X_k = X * np . sqrt ( m_k ) Y_k = Y * np . sqrt ( m_k ) a_alpha_k , b_alpha_k = HParams () . A_ALPHA , HParams () . B_ALPHA a_tau_k , b_tau_k = HParams () . A_TAU , HParams () . B_TAU L_k_q = - np . inf delta_L_k_q = HParams () . DELTA_S_L_K_Q + 1 # Drugowitsch reaches convergence usually after 3-4 iterations [PDF p. 237]. # NOTE Deviation from the original text (but not from LCSBookCode) since we # add a maximum number of iterations (see module doc string). i = 0 while delta_L_k_q > HParams () . DELTA_S_L_K_Q and i < HParams () . MAX_ITER_CLS : i += 1 E_alpha_alpha_k = a_alpha_k / b_alpha_k Lambda_k = np . diag ([ E_alpha_alpha_k ] * DX ) + X_k . T @ X_k # While, in theory, Lambda_k is always invertible here and we thus # should be able to use inv (as it is described in the algorithm we # implement), we (seldomly) get a singular matrix, probably due to # numerical issues. Thus we simply use pinv which yields the same result # as inv anyways if H is non-singular. Also, in his own code, # Drugowitsch always uses pseudo inverse here. Lambda_k_1 = np . linalg . pinv ( Lambda_k ) W_k = Y_k . T @ X_k @ Lambda_k_1 a_tau_k = HParams () . A_TAU + 0.5 * np . sum ( m_k ) b_tau_k = HParams () . B_TAU + 1 / ( 2 * DY ) * ( np . sum ( Y_k * Y_k ) - np . sum ( W_k * ( W_k @ Lambda_k ))) E_tau_tau_k = a_tau_k / b_tau_k a_alpha_k = HParams () . A_ALPHA + DX * DY / 2 # DY factor in front of trace due to sum over DY elements (7.100). b_alpha_k = HParams () . B_ALPHA + 0.5 * ( E_tau_tau_k * np . sum ( W_k * W_k ) + DY * np . trace ( Lambda_k_1 )) L_k_q_prev = L_k_q L_k_q = var_cl_bound ( X = X , Y = Y , W_k = W_k , Lambda_k_1 = Lambda_k_1 , a_tau_k = a_tau_k , b_tau_k = b_tau_k , a_alpha_k = a_alpha_k , b_alpha_k = b_alpha_k , # Substitute r_k by m_k in order to train submodels independently # (see [PDF p. 219]). r_k = m_k ) delta_L_k_q = L_k_q - L_k_q_prev # \u201cEach parameter update either increases L_k_q or leaves it unchanged # (\u2026). If this is not the case, then the implementation is faulty and/or # suffers from numerical instabilities.\u201d [PDF p. 237] assert delta_L_k_q >= 0 or np . isclose ( delta_L_k_q , 0 ), ( \"Error: L_k(q) not increasing monotonically! \" \"Often caused by data not being standardized \" \"(both X and y should be standardized). \" f \"Iteration: { i } ; \u0394 L_k(q) = { delta_L_k_q } ; L_k(q) = { L_k_q } .\" ) if i >= HParams () . MAX_ITER_CLS : mlflow . set_tag ( \"MAX_ITER_CLS\" , \"reached\" ) return W_k , Lambda_k_1 , a_tau_k , b_tau_k , a_alpha_k , b_alpha_k train_mix_priors ( V , Lambda_V_1 ) [PDF p. 244] :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :returns: mixing weight vector prior parameters a_beta, b_beta Source code in berbl/literal/__init__.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def train_mix_priors ( V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :returns: mixing weight vector prior parameters a_beta, b_beta \"\"\" DV , K = V . shape assert Lambda_V_1 . shape == ( K * DV , K * DV ) a_beta = np . repeat ( HParams () . A_BETA , ( K , )) b_beta = np . repeat ( HParams () . B_BETA , ( K , )) Lambda_V_1_diag = np . diag ( Lambda_V_1 ) for k in range ( K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV a_beta [ k ] += DV / 2 try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , f \"v_k = { v_k } , K = { K } , V = { V } , Lambda_V_1 = { Lambda_V_1 } \" , report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return a_beta , b_beta train_mix_weights ( M , X , Y , Phi , W , Lambda_1 , a_tau , b_tau , V , a_beta , b_beta ) Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book [PDF p. 241]). Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required W list (length K) of arrays of shape (DY, DX) Submodel weight matrices. required Lambda_1 list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. required a_tau array of shape (K,) Submodel noise precision parameter. required b_tau array of shape (K,) Submodel noise precision parameter. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required lxi array of shape (N, K) Parameter of Bouchard's bound. required alpha array of shape (N, 1) Parameter of Bouchard's bound. required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrix. Source code in berbl/literal/__init__.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def train_mix_weights ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , V : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book [PDF p. 241]). Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. W : list (length K) of arrays of shape (DY, DX) Submodel weight matrices. Lambda_1 : list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. a_tau : array of shape (K,) Submodel noise precision parameter. b_tau : array of shape (K,) Submodel noise precision parameter. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). lxi : array of shape (N, K) Parameter of Bouchard's bound. alpha : array of shape (N, 1) Parameter of Bouchard's bound. Returns ------- V, Lambda_V_1 : tuple of arrays of shapes (DV, K) and (K * DV, K * DV) Updated mixing weight matrix and mixing weight covariance matrix. \"\"\" DV , K = V . shape E_beta_beta = a_beta / b_beta G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) # NOTE Deviation from the original text since we do not always perform at # least TWO runs of the loop (Drugowitsch sets KLRG = np.inf). We instead # use its real value which solves a certain numerical issue that occurrs # very seldomly: if the actual KLRG is almost 0 and the loop is forced to # execute (which is the case for KLRG = np.inf) then V becomes unstable and # very large. KLRG = _kl ( R , G ) delta_KLRG = HParams () . DELTA_S_KLRG + 1 # NOTE Deviation from the original text since we add a maximum number of # iterations (see module doc string). In addition, we abort as soon as KLRG # is 0 (and do not wait until it was 0 twice, as would be the case if we # used the delta-based test only) since numerical issues arise in the loop # which then leads to endless oscillations between bad values, one of which # is then used in the remainder of the algorithm causing further bad things # to happen. i = 0 while delta_KLRG > HParams () . DELTA_S_KLRG and i < HParams ( ) . MAX_ITER_MIXING and not np . isclose ( KLRG , 0 ): i += 1 # Actually, this should probably be named nabla_E. E = Phi . T @ ( G - R ) + V * E_beta_beta e = E . T . ravel () H = hessian ( Phi = Phi , G = G , a_beta = a_beta , b_beta = b_beta ) # While, in theory, H is always invertible here and we thus should be able # to use inv (as it is described in the algorithm we implement), we # (seldomly) get a singular H, probably due to numerical issues. Thus we # simply use pinv which yields the same result as inv anyways if H is # non-singular. Also, in his own code, Drugowitsch always uses pseudo # inverse here. delta_v = - np . linalg . pinv ( H ) @ e # \u201cDV \u00d7 K matrix with jk'th element given by ((k - 1) K + j)'th element # of v.\u201d (Probably means \u201cdelta_v\u201d.) delta_V = delta_v . reshape (( K , DV )) . T V = V + delta_V G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) # Note that KLRG is actually the *negative* Kullback-Leibler divergence # (other than is stated in the book). KLRG_prev = KLRG KLRG = _kl ( R , G ) delta_KLRG = np . abs ( KLRG_prev - KLRG ) H = hessian ( Phi = Phi , G = G , a_beta = a_beta , b_beta = b_beta ) # While, in theory, H is always invertible here and we thus should be able # to use inv (as it is described in the algorithm we implement), we # (seldomly) get a singular H, probably due to numerical issues. Thus we # simply use pinv which yields the same result as inv anyways if H is # non-singular. Also, in his own code, Drugowitsch always uses pseudo # inverse here. Lambda_V_1 = np . linalg . pinv ( H ) # Note that instead of returning/storing Lambda_V_1, Drugowitsch's # LCSBookCode computes and stores np.slogdet(Lambda_V_1) and cov_Tr (the # latter of which is used in his update_gating). return V , Lambda_V_1 train_mixing ( M , X , Y , Phi , W , Lambda_1 , a_tau , b_tau , exp_min , ln_max , random_state ) [PDF p. 238] Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required W list (length K) of arrays of shape (DY, DX) Submodel weight matrices. required Lambda_1 list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. required a_tau array of shape (K,) Submodel noise precision parameter. required b_tau array of shape (K,) Submodel noise precision parameter. required Returns: Type Description V , Lambda_V_1 , a_beta , b_beta Mixing weight matrix, mixing weight covariance matrix, mixing weight prior parameter vectors. Source code in berbl/literal/__init__.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def train_mixing ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , exp_min : float , ln_max : float , random_state : np . random . RandomState ): \"\"\" [PDF p. 238] Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. W : list (length K) of arrays of shape (DY, DX) Submodel weight matrices. Lambda_1 : list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. a_tau : array of shape (K,) Submodel noise precision parameter. b_tau : array of shape (K,) Submodel noise precision parameter. Returns ------- V, Lambda_V_1, a_beta, b_beta : tuple of arrays of shapes (DV, K), (K * DV, K * DV), (K,) and (K,) Mixing weight matrix, mixing weight covariance matrix, mixing weight prior parameter vectors. \"\"\" N , K = M . shape N , DX = X . shape N , DY = Y . shape N , DV = Phi . shape # NOTE LCSBookCode initializes this with np.ones(\u2026). # NOTE The scale of this normal is wrong in TrainMixing in Drugowitsch's # book (but correct in the text accompanying that algorithm). V = random_state . normal ( loc = 0 , scale = HParams () . B_BETA / HParams () . A_BETA , size = ( DV , K )) a_beta = np . repeat ( HParams () . A_BETA , K ) b_beta = np . repeat ( HParams () . B_BETA , K ) L_M_q = - np . inf delta_L_M_q = HParams () . DELTA_S_L_M_Q + 1 # NOTE Deviation from the original text (but not from LCSBookCode) since we # add a maximum number of iterations (see module doc string). i = 0 while delta_L_M_q > HParams () . DELTA_S_L_M_Q and i < HParams ( ) . MAX_ITER_MIXING : i += 1 # This is not monotonous due to the Laplace approximation used [PDF p. # 202, 160]. Also: \u201cThis desirable monotonicity property is unlikely to # arise with other types of approximation methods, such as the Laplace # approximation.\u201d (Bayesian parameter estimation via variational methods # (Jaakkola, Jordan), p. 10) V , Lambda_V_1 = train_mix_weights ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , V = V , a_beta = a_beta , b_beta = b_beta ) # NOTE LCSBookCode only updates b_beta here as a_beta is constant. a_beta , b_beta = train_mix_priors ( V , Lambda_V_1 ) G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) L_M_q_prev = L_M_q L_M_q = var_mix_bound ( G = G , R = R , V = V , Lambda_V_1 = Lambda_V_1 , a_beta = a_beta , b_beta = b_beta ) try : # LCSBookCode states: \u201cas we are using a [Laplace] approximation, # the variational bound might decrease, so we're not checking and # need to take the abs()\u201d. delta_L_M_q = np . abs ( L_M_q - L_M_q_prev ) except FloatingPointError as e : # ``L_M_q`` and ``L_M_q_prev`` are sometimes ``-inf`` which results # in a FloatingPointError (as a nan is generated from ``-inf - # (-inf)``). # # However, ``delta_L_M_q`` being ``nan`` makes the loop abort # anyway, so we should be fine. We'll log to stdout and mlflow that # this happened, anyway. known_issue ( \"FloatingPointError in train_mixing\" , f \"L_M_q: { L_M_q } , L_M_q_prev: { L_M_q_prev } \" ) mlflow . set_tag ( \"FloatingPointError delta_L_M_q\" , \"occurred\" ) if i >= HParams () . MAX_ITER_MIXING : mlflow . set_tag ( \"MAX_ITER_MIXING\" , \"reached\" ) return V , Lambda_V_1 , a_beta , b_beta var_bound ( M , X , Y , Phi , W , Lambda_1 , a_tau , b_tau , a_alpha , b_alpha , V , Lambda_V_1 , a_beta , b_beta ) [PDF p. 244] :param M: matching matrix (N \u00d7 K) :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param Phi: mixing feature matrix (N \u00d7 DV) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :param a_alpha: weight vector prior parameters :param b_alpha: weight vector prior parameters :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :param a_beta: mixing weight prior parameter (row vector of length K) :param b_beta: mixing weight prior parameter (row vector of length K) :returns: variational bound L(q) Source code in berbl/literal/__init__.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 def var_bound ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , a_alpha : np . ndarray , b_alpha : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta , b_beta ): \"\"\" [PDF p. 244] :param M: matching matrix (N \u00d7 K) :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param Phi: mixing feature matrix (N \u00d7 DV) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :param a_alpha: weight vector prior parameters :param b_alpha: weight vector prior parameters :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :param a_beta: mixing weight prior parameter (row vector of length K) :param b_beta: mixing weight prior parameter (row vector of length K) :returns: variational bound L(q) \"\"\" DV , K = V . shape assert Lambda_V_1 . shape == ( K * DV , K * DV ) assert a_beta . shape == b_beta . shape assert a_beta . shape == ( K , ) G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) L_K_q = 0 for k in range ( K ): L_K_q = L_K_q + var_cl_bound ( X = X , Y = Y , W_k = W [ k ], Lambda_k_1 = Lambda_1 [ k ], a_tau_k = a_tau [ k ], b_tau_k = b_tau [ k ], a_alpha_k = a_alpha [ k ], b_alpha_k = b_alpha [ k ], r_k = R [:, [ k ]]) L_M_q = var_mix_bound ( G , R , V , Lambda_V_1 , a_beta , b_beta ) return L_K_q + L_M_q , L_K_q , L_M_q var_cl_bound ( X , Y , W_k , Lambda_k_1 , a_tau_k , b_tau_k , a_alpha_k , b_alpha_k , r_k ) [PDF p. 245] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param W_k: submodel weight matrix (DY \u00d7 DX) :param Lambda_k_1: submodel covariance matrix (DX \u00d7 DX) :param a_tau_k: submodel noise precision parameter :param b_tau_k: submodel noise precision parameter :param a_alpha_k: weight vector prior parameter :param b_alpha_k: weight vector prior parameter :param r_k: responsibility vector (NumPy row or column vector, we reshape to (-1) anyways) :returns: rule component L_k(q) of variational bound Source code in berbl/literal/__init__.py 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 def var_cl_bound ( X : np . ndarray , Y : np . ndarray , W_k : np . ndarray , Lambda_k_1 : np . ndarray , a_tau_k : float , b_tau_k : float , a_alpha_k : float , b_alpha_k : float , r_k : np . ndarray ): \"\"\" [PDF p. 245] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param W_k: submodel weight matrix (DY \u00d7 DX) :param Lambda_k_1: submodel covariance matrix (DX \u00d7 DX) :param a_tau_k: submodel noise precision parameter :param b_tau_k: submodel noise precision parameter :param a_alpha_k: weight vector prior parameter :param b_alpha_k: weight vector prior parameter :param r_k: responsibility vector (NumPy row or column vector, we reshape to (-1) anyways) :returns: rule component L_k(q) of variational bound \"\"\" DY , DX = W_k . shape E_tau_tau_k = a_tau_k / b_tau_k L_k_1_q = DY / 2 * ( ss . digamma ( a_tau_k ) - np . log ( b_tau_k ) - np . log ( 2 * np . pi )) * np . sum ( r_k ) # We reshape r_k to a NumPy row vector since NumPy seems to understand what # we want to do when we multiply two row vectors (i.e. a^T a). L_k_2_q = ( - 0.5 * r_k ) . reshape (( - 1 )) @ ( E_tau_tau_k * np . sum ( ( Y - X @ W_k . T ) ** 2 , axis = 1 ) + DY * np . sum ( X * ( X @ Lambda_k_1 ), axis = 1 )) L_k_3_q = - ss . gammaln ( HParams () . A_ALPHA ) + HParams () . A_ALPHA * np . log ( HParams () . B_ALPHA ) + ss . gammaln ( a_alpha_k ) - a_alpha_k * np . log ( b_alpha_k ) + DX * DY / 2 + DY / 2 * np . log ( np . linalg . det ( Lambda_k_1 )) L_k_4_q = DY * ( - ss . gammaln ( HParams () . A_TAU ) + HParams () . A_TAU * np . log ( HParams () . B_TAU ) + ( HParams () . A_TAU - a_tau_k ) * ss . digamma ( a_tau_k ) - HParams () . A_TAU * np . log ( b_tau_k ) - HParams () . B_TAU * E_tau_tau_k + ss . gammaln ( a_tau_k ) + a_tau_k ) return L_k_1_q + L_k_2_q + L_k_3_q + L_k_4_q var_mix_bound ( G , R , V , Lambda_V_1 , a_beta , b_beta ) [PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/literal/__init__.py 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 def var_mix_bound ( G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" DV , K = V . shape assert G . shape == R . shape assert G . shape [ 1 ] == K assert Lambda_V_1 . shape == ( K * DV , K * DV ) assert a_beta . shape == ( K , ) assert b_beta . shape == ( K , ) L_M1q = K * ( - ss . gammaln ( HParams () . A_BETA ) + HParams () . A_BETA * np . log ( HParams () . B_BETA )) for k in range ( K ): try : L_M1q = L_M1q + ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) except FloatingPointError as e : known_issue ( \"FloatingPointError in var_mix_bound\" , ( f \"Lambda_V_1: { Lambda_V_1 } \" f \"V: { V } \" f \"K: { K } \" f \"k: { k } \" f \"b_beta[k]: { b_beta [ k ] } \" ), report = True ) raise e # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. L_M2q = _kl ( R , G ) # L_M3q may be -inf after the following line but that is probably OK since # the ``train_mixing`` loop then aborts (also see comment in # ``train_mixing``). L_M3q = 0.5 * np . linalg . slogdet ( Lambda_V_1 )[ 1 ] + K * DV / 2 if np . any ( ~ np . isfinite ([ L_M1q , L_M2q , L_M3q ])): known_issue ( \"Infinite var_mix_bound\" , ( f \"Lambda_V_1 = { Lambda_V_1 } , \" f \"L_M1q = { L_M1q } , \" f \"L_M2q = { L_M2q } , \" f \"L_M3q = { L_M3q } \" )) return L_M1q + L_M2q + L_M3q","title":"literal"},{"location":"reference/berbl/literal/#berbl.literal._kl","text":"Computes the negative Kullback-Leibler divergence between the given arrays. Drugowitsch does not introduce this subroutine. We do so to reduce code duplication in train_mix_weights (where we deviated from the original text by one additional calculation of _kl(R, G) ). Source code in berbl/literal/__init__.py 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 def _kl ( R , G ): \"\"\" Computes the negative Kullback-Leibler divergence between the given arrays. Drugowitsch does not introduce this subroutine. We do so to reduce code duplication in ``train_mix_weights`` (where we deviated from the original text by one additional calculation of ``_kl(R, G)``). \"\"\" # NumPy uses subnormal numbers \u201cto fill the gap between 0 and # [np.finfo(None).tiny]\u201d. This means that R may contain elements that do not # silently lead to inf but raise a \u201cFloatingPointError: overflow encountered # in true_divide\u201d instead. We simply set anything smaller than tiny to 0. R [ np . where ( R < np . finfo ( None ) . tiny )] = 0 # ``responsibilities`` performs a ``nan_to_num(\u2026, nan=0, \u2026)``, so we might # divide by 0 here due to 0's in ``R``. The intended behaviour is to # silently get a NaN that can then be replaced by 0 again (this is how # Drugowitsch does it [PDF p. 213]). Drugowitsch expects dividing ``x`` by # 0 to result in NaN, however, in Python this is only true for ``x == 0``; # for any other ``x`` this instead results in ``inf`` (with sign depending # on the sign of x). The two cases also throw different errors (\u2018invalid # value encountered\u2019 for ``x == 0`` and \u2018divide by zero\u2019 otherwise). # # I don't think the neginf is strictly required but let's be safe. with np . errstate ( divide = \"ignore\" , invalid = \"ignore\" ): KLRG = np . sum ( R * np . nan_to_num ( np . log ( G / R ), nan = 0 , posinf = 0 , neginf = 0 )) # This fixes(?) some numerical problems. Note that KLRG is actually the # negative Kullback-Leibler divergence (hence the > 0 comparison instead # of < 0). if KLRG > 0 and np . isclose ( KLRG , 0 ): KLRG = 0 assert KLRG <= 0 , ( f \"Kullback-Leibler divergence less than zero: \" f \"KLRG = { - KLRG } \" ) return KLRG","title":"_kl()"},{"location":"reference/berbl/literal/#berbl.literal.hessian","text":"[PDF p. 243] Parameters: Name Type Description Default Phi array of shape (N, DV) Mixing feature matrix. required G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required Returns: Type Description array of shape (K Hessian matrix. Source code in berbl/literal/__init__.py 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def hessian ( Phi : np . ndarray , G : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 243] Parameters ---------- Phi : array of shape (N, DV) Mixing feature matrix. G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). Returns ------- array of shape (K * DV, K * DV) Hessian matrix. \"\"\" N , DV = Phi . shape K , = a_beta . shape assert G . shape == ( N , K ) assert a_beta . shape == b_beta . shape H = np . zeros (( K * DV , K * DV )) for k in range ( K ): for j in range ( k ): lk = k * DV uk = ( k + 1 ) * DV lj = j * DV uj = ( j + 1 ) * DV H_kj = - Phi . T @ ( Phi * ( G [:, [ k ]] * G [:, [ j ]])) H [ lk : uk : 1 , lj : uj : 1 ] = H_kj H [ lj : uj : 1 , lk : uk : 1 ] = H_kj l = k * DV u = ( k + 1 ) * DV H [ l : u : 1 , l : u : 1 ] = Phi . T @ ( Phi * ( G [:, [ k ]] * ( 1 - G [:, [ k ]]))) + a_beta [ k ] / b_beta [ k ] * np . identity ( DV ) return H","title":"hessian()"},{"location":"reference/berbl/literal/#berbl.literal.mixing","text":"[PDF p. 239] Is zero wherever a rule does not match. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required Phi array of shape (N, DV) Mixing feature matrix. required V array of shape (DV, K) Mixing weight matrix. required Returns: Name Type Description G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. Source code in berbl/literal/__init__.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def mixing ( M : np . ndarray , Phi : np . ndarray , V : np . ndarray ): \"\"\" [PDF p. 239] Is zero wherever a rule does not match. Parameters ---------- M : array of shape (N, K) Matching matrix. Phi : array of shape (N, DV) Mixing feature matrix. V : array of shape (DV, K) Mixing weight matrix. Returns ------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. \"\"\" DV , K = V . shape # If Phi is standard, this simply broadcasts V to a matrix [V, V, V, \u2026] of # shape (N, DV). G = Phi @ V G = np . clip ( G , HParams () . EXP_MIN , HParams () . LN_MAX - np . log ( K )) G = np . exp ( G ) * M # The sum can be 0 meaning we do 0/0 (== NaN) but we ignore it because it is # fixed one line later (this is how Drugowitsch does it). Drugowitsch does, # however, also say that: \u201cUsually, this should never happen as only model # structures are accepted where [(np.sum(G, 1) > 0).all()]. Nonetheless, # this check was added to ensure that even these cases are handled # gracefully.\u201d # # The also sometimes overflows which we fix as well. # TODO Why does the sum overflow sometimes? Is V *that* large? with np . errstate ( invalid = \"ignore\" , over = \"ignore\" ): G = G / np . sum ( G , axis = 1 )[:, np . newaxis ] G = np . nan_to_num ( G , nan = 1 / K , posinf = 1 / K ) return G","title":"mixing()"},{"location":"reference/berbl/literal/#berbl.literal.model_probability","text":"[PDF p. 235] Note that this deviates from [PDF p. 235] in that we return p(M | D) = L(q) - ln K! instead of L(q) + ln K! because the latter is not consistent with (7.3). We also compute the matching matrix within this function instead of providing it to it. Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required Returns: Type Description metrics , params Model metrics and model parameters. Source code in berbl/literal/__init__.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def model_probability ( matchs : List , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , random_state : np . random . RandomState , exp_min : float = np . log ( np . finfo ( None ) . tiny ), ln_max : float = np . log ( np . finfo ( None ) . max )): \"\"\" [PDF p. 235] Note that this deviates from [PDF p. 235] in that we return ``p(M | D) = L(q) - ln K!`` instead of ``L(q) + ln K!`` because the latter is not consistent with (7.3). We also compute the matching matrix *within* this function instead of providing it to it. Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. Returns ------- metrics, params : pair of dict Model metrics and model parameters. \"\"\" # Underflows may occur in many places, e.g. if X contains values very close to # 0. However, they mostly occur in the very first training iterations so they # should be OK to ignore for now. We want to stop (for now) if any other # floating point error occurs, though. with np . errstate ( all = \"raise\" , under = \"ignore\" ): N , _ = X . shape K = len ( matchs ) M = matching_matrix ( matchs , X ) W = [ None ] * K Lambda_1 = [ None ] * K a_tau = [ None ] * K b_tau = [ None ] * K a_alpha = [ None ] * K b_alpha = [ None ] * K for k in range ( K ): W [ k ], Lambda_1 [ k ], a_tau [ k ], b_tau [ k ], a_alpha [ k ], b_alpha [ k ] = train_classifier ( M [:, [ k ]], X , Y ) V , Lambda_V_1 , a_beta , b_beta = train_mixing ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , exp_min = exp_min , ln_max = ln_max , random_state = random_state ) # We rename L_k_q to L_C_q. L_q , L_C_q , L_M_q = var_bound ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , a_alpha = a_alpha , b_alpha = b_alpha , V = V , Lambda_V_1 = Lambda_V_1 , a_beta = a_beta , b_beta = b_beta ) ln_p_M = - np . log ( float ( np . math . factorial ( K ))) p_M_D = L_q + ln_p_M return { \"p_M_D\" : p_M_D , \"L_q\" : L_q , \"ln_p_M\" : ln_p_M , \"L_C_q\" : L_C_q , \"L_M_q\" : L_M_q }, { \"matchs\" : matchs , \"W\" : W , \"Lambda_1\" : Lambda_1 , \"a_tau\" : a_tau , \"b_tau\" : b_tau , \"a_alpha\" : a_alpha , \"b_alpha\" : b_alpha , \"V\" : V , \"Lambda_V_1\" : Lambda_V_1 , \"a_beta\" : a_beta , \"b_beta\" : b_beta , }","title":"model_probability()"},{"location":"reference/berbl/literal/#berbl.literal.responsibilities","text":"[PDF p. 240] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param G: mixing (\u201cgating\u201d) matrix (N \u00d7 K) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :returns: responsibility matrix (N \u00d7 K) Source code in berbl/literal/__init__.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def responsibilities ( X : np . ndarray , Y : np . ndarray , G : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray ): \"\"\" [PDF p. 240] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param G: mixing (\u201cgating\u201d) matrix (N \u00d7 K) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :returns: responsibility matrix (N \u00d7 K) \"\"\" N , K = G . shape N , DY = Y . shape # This fixes instabilities that occur if there is only a single rule. if K == 1 : return np . ones (( N , K )) # We first create the transpose of R because indexing is easier (assigning # to rows instead of columns is rather straightforward). We then transpose # before multiplying elementwise with G. R_T = np . zeros (( K , N )) for k in range ( K ): R_T [ k ] = np . exp ( DY / 2 * ( ss . digamma ( a_tau [ k ]) - np . log ( b_tau [ k ])) - 0.5 * ( a_tau [ k ] / b_tau [ k ] * np . sum (( Y - X @ W [ k ] . T ) ** 2 , 1 ) + DY * np . sum ( X * ( X @ Lambda_1 [ k ]), 1 ))) R = R_T . T * G # Make a copy of the reference for checking for nans a few lines later. R_ = R # The sum can be 0 meaning we do 0/0 (== NaN in Python) but we ignore it # because it is fixed one line later (this is how Drugowitsch does it). with np . errstate ( invalid = \"ignore\" ): R = R / np . sum ( R , 1 )[:, np . newaxis ] # This is safer than Drugowitsch's plain `R = np.nan_to_num(R, nan=0)` # (i.e. we check whether the nan really came from the cause described above # at the cost of an additional run over R to check for zeroes). R [ np . where ( np . logical_and ( R_ == 0 , np . isnan ( R )))] = 0 return R","title":"responsibilities()"},{"location":"reference/berbl/literal/#berbl.literal.train_classifier","text":"[PDF p. 238] Parameters: Name Type Description Default m_k array of shape (N,) Matching vector of rule k. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Returns: Type Description W_k , Lambda_k_1 , a_tau_k , b_tau_k , a_alpha_k , b_alpha_k Weight matrix (DY \u00d7 DX), covariance matrix (DX \u00d7 DX), noise precision parameters, weight vector parameters. Source code in berbl/literal/__init__.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def train_classifier ( m_k , X , Y ): \"\"\" [PDF p. 238] Parameters ---------- m_k : array of shape (N,) Matching vector of rule k. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Returns ------- W_k, Lambda_k_1, a_tau_k, b_tau_k, a_alpha_k, b_alpha_k : arrays of shapes (DY, DX) and (DX, DX) and float Weight matrix (DY \u00d7 DX), covariance matrix (DX \u00d7 DX), noise precision parameters, weight vector parameters. \"\"\" N , DX = X . shape N , DY = Y . shape X_k = X * np . sqrt ( m_k ) Y_k = Y * np . sqrt ( m_k ) a_alpha_k , b_alpha_k = HParams () . A_ALPHA , HParams () . B_ALPHA a_tau_k , b_tau_k = HParams () . A_TAU , HParams () . B_TAU L_k_q = - np . inf delta_L_k_q = HParams () . DELTA_S_L_K_Q + 1 # Drugowitsch reaches convergence usually after 3-4 iterations [PDF p. 237]. # NOTE Deviation from the original text (but not from LCSBookCode) since we # add a maximum number of iterations (see module doc string). i = 0 while delta_L_k_q > HParams () . DELTA_S_L_K_Q and i < HParams () . MAX_ITER_CLS : i += 1 E_alpha_alpha_k = a_alpha_k / b_alpha_k Lambda_k = np . diag ([ E_alpha_alpha_k ] * DX ) + X_k . T @ X_k # While, in theory, Lambda_k is always invertible here and we thus # should be able to use inv (as it is described in the algorithm we # implement), we (seldomly) get a singular matrix, probably due to # numerical issues. Thus we simply use pinv which yields the same result # as inv anyways if H is non-singular. Also, in his own code, # Drugowitsch always uses pseudo inverse here. Lambda_k_1 = np . linalg . pinv ( Lambda_k ) W_k = Y_k . T @ X_k @ Lambda_k_1 a_tau_k = HParams () . A_TAU + 0.5 * np . sum ( m_k ) b_tau_k = HParams () . B_TAU + 1 / ( 2 * DY ) * ( np . sum ( Y_k * Y_k ) - np . sum ( W_k * ( W_k @ Lambda_k ))) E_tau_tau_k = a_tau_k / b_tau_k a_alpha_k = HParams () . A_ALPHA + DX * DY / 2 # DY factor in front of trace due to sum over DY elements (7.100). b_alpha_k = HParams () . B_ALPHA + 0.5 * ( E_tau_tau_k * np . sum ( W_k * W_k ) + DY * np . trace ( Lambda_k_1 )) L_k_q_prev = L_k_q L_k_q = var_cl_bound ( X = X , Y = Y , W_k = W_k , Lambda_k_1 = Lambda_k_1 , a_tau_k = a_tau_k , b_tau_k = b_tau_k , a_alpha_k = a_alpha_k , b_alpha_k = b_alpha_k , # Substitute r_k by m_k in order to train submodels independently # (see [PDF p. 219]). r_k = m_k ) delta_L_k_q = L_k_q - L_k_q_prev # \u201cEach parameter update either increases L_k_q or leaves it unchanged # (\u2026). If this is not the case, then the implementation is faulty and/or # suffers from numerical instabilities.\u201d [PDF p. 237] assert delta_L_k_q >= 0 or np . isclose ( delta_L_k_q , 0 ), ( \"Error: L_k(q) not increasing monotonically! \" \"Often caused by data not being standardized \" \"(both X and y should be standardized). \" f \"Iteration: { i } ; \u0394 L_k(q) = { delta_L_k_q } ; L_k(q) = { L_k_q } .\" ) if i >= HParams () . MAX_ITER_CLS : mlflow . set_tag ( \"MAX_ITER_CLS\" , \"reached\" ) return W_k , Lambda_k_1 , a_tau_k , b_tau_k , a_alpha_k , b_alpha_k","title":"train_classifier()"},{"location":"reference/berbl/literal/#berbl.literal.train_mix_priors","text":"[PDF p. 244] :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :returns: mixing weight vector prior parameters a_beta, b_beta Source code in berbl/literal/__init__.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def train_mix_priors ( V : np . ndarray , Lambda_V_1 : np . ndarray ): \"\"\" [PDF p. 244] :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :returns: mixing weight vector prior parameters a_beta, b_beta \"\"\" DV , K = V . shape assert Lambda_V_1 . shape == ( K * DV , K * DV ) a_beta = np . repeat ( HParams () . A_BETA , ( K , )) b_beta = np . repeat ( HParams () . B_BETA , ( K , )) Lambda_V_1_diag = np . diag ( Lambda_V_1 ) for k in range ( K ): v_k = V [:, [ k ]] l = k * DV u = ( k + 1 ) * DV a_beta [ k ] += DV / 2 try : b_beta [ k ] += 0.5 * ( np . sum ( Lambda_V_1_diag [ l : u : 1 ]) + v_k . T @ v_k ) except FloatingPointError as e : known_issue ( \"FloatingPointError in train_mix_priors\" , f \"v_k = { v_k } , K = { K } , V = { V } , Lambda_V_1 = { Lambda_V_1 } \" , report = True ) mlflow . set_tag ( \"FloatingPointError_train_mix_priors\" , \"occurred\" ) raise e return a_beta , b_beta","title":"train_mix_priors()"},{"location":"reference/berbl/literal/#berbl.literal.train_mix_weights","text":"Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book [PDF p. 241]). Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required y array of shape (N, Dy) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required W list (length K) of arrays of shape (DY, DX) Submodel weight matrices. required Lambda_1 list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. required a_tau array of shape (K,) Submodel noise precision parameter. required b_tau array of shape (K,) Submodel noise precision parameter. required V array of shape (DV, K) Mixing weight matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) Mixing weight prior parameter (row vector). required lxi array of shape (N, K) Parameter of Bouchard's bound. required alpha array of shape (N, 1) Parameter of Bouchard's bound. required Returns: Type Description V , Lambda_V_1 Updated mixing weight matrix and mixing weight covariance matrix. Source code in berbl/literal/__init__.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def train_mix_weights ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , V : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" Training routine for mixing weights based on a Laplace approximation (see Drugowitsch's book [PDF p. 241]). Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. y : array of shape (N, Dy) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. W : list (length K) of arrays of shape (DY, DX) Submodel weight matrices. Lambda_1 : list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. a_tau : array of shape (K,) Submodel noise precision parameter. b_tau : array of shape (K,) Submodel noise precision parameter. V : array of shape (DV, K) Mixing weight matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Mixing weight prior parameter (row vector). lxi : array of shape (N, K) Parameter of Bouchard's bound. alpha : array of shape (N, 1) Parameter of Bouchard's bound. Returns ------- V, Lambda_V_1 : tuple of arrays of shapes (DV, K) and (K * DV, K * DV) Updated mixing weight matrix and mixing weight covariance matrix. \"\"\" DV , K = V . shape E_beta_beta = a_beta / b_beta G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) # NOTE Deviation from the original text since we do not always perform at # least TWO runs of the loop (Drugowitsch sets KLRG = np.inf). We instead # use its real value which solves a certain numerical issue that occurrs # very seldomly: if the actual KLRG is almost 0 and the loop is forced to # execute (which is the case for KLRG = np.inf) then V becomes unstable and # very large. KLRG = _kl ( R , G ) delta_KLRG = HParams () . DELTA_S_KLRG + 1 # NOTE Deviation from the original text since we add a maximum number of # iterations (see module doc string). In addition, we abort as soon as KLRG # is 0 (and do not wait until it was 0 twice, as would be the case if we # used the delta-based test only) since numerical issues arise in the loop # which then leads to endless oscillations between bad values, one of which # is then used in the remainder of the algorithm causing further bad things # to happen. i = 0 while delta_KLRG > HParams () . DELTA_S_KLRG and i < HParams ( ) . MAX_ITER_MIXING and not np . isclose ( KLRG , 0 ): i += 1 # Actually, this should probably be named nabla_E. E = Phi . T @ ( G - R ) + V * E_beta_beta e = E . T . ravel () H = hessian ( Phi = Phi , G = G , a_beta = a_beta , b_beta = b_beta ) # While, in theory, H is always invertible here and we thus should be able # to use inv (as it is described in the algorithm we implement), we # (seldomly) get a singular H, probably due to numerical issues. Thus we # simply use pinv which yields the same result as inv anyways if H is # non-singular. Also, in his own code, Drugowitsch always uses pseudo # inverse here. delta_v = - np . linalg . pinv ( H ) @ e # \u201cDV \u00d7 K matrix with jk'th element given by ((k - 1) K + j)'th element # of v.\u201d (Probably means \u201cdelta_v\u201d.) delta_V = delta_v . reshape (( K , DV )) . T V = V + delta_V G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) # Note that KLRG is actually the *negative* Kullback-Leibler divergence # (other than is stated in the book). KLRG_prev = KLRG KLRG = _kl ( R , G ) delta_KLRG = np . abs ( KLRG_prev - KLRG ) H = hessian ( Phi = Phi , G = G , a_beta = a_beta , b_beta = b_beta ) # While, in theory, H is always invertible here and we thus should be able # to use inv (as it is described in the algorithm we implement), we # (seldomly) get a singular H, probably due to numerical issues. Thus we # simply use pinv which yields the same result as inv anyways if H is # non-singular. Also, in his own code, Drugowitsch always uses pseudo # inverse here. Lambda_V_1 = np . linalg . pinv ( H ) # Note that instead of returning/storing Lambda_V_1, Drugowitsch's # LCSBookCode computes and stores np.slogdet(Lambda_V_1) and cov_Tr (the # latter of which is used in his update_gating). return V , Lambda_V_1","title":"train_mix_weights()"},{"location":"reference/berbl/literal/#berbl.literal.train_mixing","text":"[PDF p. 238] Parameters: Name Type Description Default M array of shape (N, K) Matching matrix. required X array of shape (N, DX) Input matrix. required Y array of shape (N, DY) Output matrix. required Phi array of shape (N, DV) Mixing feature matrix. required W list (length K) of arrays of shape (DY, DX) Submodel weight matrices. required Lambda_1 list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. required a_tau array of shape (K,) Submodel noise precision parameter. required b_tau array of shape (K,) Submodel noise precision parameter. required Returns: Type Description V , Lambda_V_1 , a_beta , b_beta Mixing weight matrix, mixing weight covariance matrix, mixing weight prior parameter vectors. Source code in berbl/literal/__init__.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def train_mixing ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , exp_min : float , ln_max : float , random_state : np . random . RandomState ): \"\"\" [PDF p. 238] Parameters ---------- M : array of shape (N, K) Matching matrix. X : array of shape (N, DX) Input matrix. Y : array of shape (N, DY) Output matrix. Phi : array of shape (N, DV) Mixing feature matrix. W : list (length K) of arrays of shape (DY, DX) Submodel weight matrices. Lambda_1 : list (length K) of arrays of shape (DX, DX) Submodel covariance matrices. a_tau : array of shape (K,) Submodel noise precision parameter. b_tau : array of shape (K,) Submodel noise precision parameter. Returns ------- V, Lambda_V_1, a_beta, b_beta : tuple of arrays of shapes (DV, K), (K * DV, K * DV), (K,) and (K,) Mixing weight matrix, mixing weight covariance matrix, mixing weight prior parameter vectors. \"\"\" N , K = M . shape N , DX = X . shape N , DY = Y . shape N , DV = Phi . shape # NOTE LCSBookCode initializes this with np.ones(\u2026). # NOTE The scale of this normal is wrong in TrainMixing in Drugowitsch's # book (but correct in the text accompanying that algorithm). V = random_state . normal ( loc = 0 , scale = HParams () . B_BETA / HParams () . A_BETA , size = ( DV , K )) a_beta = np . repeat ( HParams () . A_BETA , K ) b_beta = np . repeat ( HParams () . B_BETA , K ) L_M_q = - np . inf delta_L_M_q = HParams () . DELTA_S_L_M_Q + 1 # NOTE Deviation from the original text (but not from LCSBookCode) since we # add a maximum number of iterations (see module doc string). i = 0 while delta_L_M_q > HParams () . DELTA_S_L_M_Q and i < HParams ( ) . MAX_ITER_MIXING : i += 1 # This is not monotonous due to the Laplace approximation used [PDF p. # 202, 160]. Also: \u201cThis desirable monotonicity property is unlikely to # arise with other types of approximation methods, such as the Laplace # approximation.\u201d (Bayesian parameter estimation via variational methods # (Jaakkola, Jordan), p. 10) V , Lambda_V_1 = train_mix_weights ( M = M , X = X , Y = Y , Phi = Phi , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau , V = V , a_beta = a_beta , b_beta = b_beta ) # NOTE LCSBookCode only updates b_beta here as a_beta is constant. a_beta , b_beta = train_mix_priors ( V , Lambda_V_1 ) G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) L_M_q_prev = L_M_q L_M_q = var_mix_bound ( G = G , R = R , V = V , Lambda_V_1 = Lambda_V_1 , a_beta = a_beta , b_beta = b_beta ) try : # LCSBookCode states: \u201cas we are using a [Laplace] approximation, # the variational bound might decrease, so we're not checking and # need to take the abs()\u201d. delta_L_M_q = np . abs ( L_M_q - L_M_q_prev ) except FloatingPointError as e : # ``L_M_q`` and ``L_M_q_prev`` are sometimes ``-inf`` which results # in a FloatingPointError (as a nan is generated from ``-inf - # (-inf)``). # # However, ``delta_L_M_q`` being ``nan`` makes the loop abort # anyway, so we should be fine. We'll log to stdout and mlflow that # this happened, anyway. known_issue ( \"FloatingPointError in train_mixing\" , f \"L_M_q: { L_M_q } , L_M_q_prev: { L_M_q_prev } \" ) mlflow . set_tag ( \"FloatingPointError delta_L_M_q\" , \"occurred\" ) if i >= HParams () . MAX_ITER_MIXING : mlflow . set_tag ( \"MAX_ITER_MIXING\" , \"reached\" ) return V , Lambda_V_1 , a_beta , b_beta","title":"train_mixing()"},{"location":"reference/berbl/literal/#berbl.literal.var_bound","text":"[PDF p. 244] :param M: matching matrix (N \u00d7 K) :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param Phi: mixing feature matrix (N \u00d7 DV) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :param a_alpha: weight vector prior parameters :param b_alpha: weight vector prior parameters :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :param a_beta: mixing weight prior parameter (row vector of length K) :param b_beta: mixing weight prior parameter (row vector of length K) :returns: variational bound L(q) Source code in berbl/literal/__init__.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 def var_bound ( M : np . ndarray , X : np . ndarray , Y : np . ndarray , Phi : np . ndarray , W : List [ np . ndarray ], Lambda_1 : List [ np . ndarray ], a_tau : np . ndarray , b_tau : np . ndarray , a_alpha : np . ndarray , b_alpha : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta , b_beta ): \"\"\" [PDF p. 244] :param M: matching matrix (N \u00d7 K) :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param Phi: mixing feature matrix (N \u00d7 DV) :param W: submodel weight matrices (list of DY \u00d7 DX) :param Lambda_1: submodel covariance matrices (list of DX \u00d7 DX) :param a_tau: submodel noise precision parameters :param b_tau: submodel noise precision parameters :param a_alpha: weight vector prior parameters :param b_alpha: weight vector prior parameters :param V: mixing weight matrix (DV \u00d7 K) :param Lambda_V_1: mixing covariance matrix (K DV \u00d7 K DV) :param a_beta: mixing weight prior parameter (row vector of length K) :param b_beta: mixing weight prior parameter (row vector of length K) :returns: variational bound L(q) \"\"\" DV , K = V . shape assert Lambda_V_1 . shape == ( K * DV , K * DV ) assert a_beta . shape == b_beta . shape assert a_beta . shape == ( K , ) G = mixing ( M , Phi , V ) R = responsibilities ( X = X , Y = Y , G = G , W = W , Lambda_1 = Lambda_1 , a_tau = a_tau , b_tau = b_tau ) L_K_q = 0 for k in range ( K ): L_K_q = L_K_q + var_cl_bound ( X = X , Y = Y , W_k = W [ k ], Lambda_k_1 = Lambda_1 [ k ], a_tau_k = a_tau [ k ], b_tau_k = b_tau [ k ], a_alpha_k = a_alpha [ k ], b_alpha_k = b_alpha [ k ], r_k = R [:, [ k ]]) L_M_q = var_mix_bound ( G , R , V , Lambda_V_1 , a_beta , b_beta ) return L_K_q + L_M_q , L_K_q , L_M_q","title":"var_bound()"},{"location":"reference/berbl/literal/#berbl.literal.var_cl_bound","text":"[PDF p. 245] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param W_k: submodel weight matrix (DY \u00d7 DX) :param Lambda_k_1: submodel covariance matrix (DX \u00d7 DX) :param a_tau_k: submodel noise precision parameter :param b_tau_k: submodel noise precision parameter :param a_alpha_k: weight vector prior parameter :param b_alpha_k: weight vector prior parameter :param r_k: responsibility vector (NumPy row or column vector, we reshape to (-1) anyways) :returns: rule component L_k(q) of variational bound Source code in berbl/literal/__init__.py 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 def var_cl_bound ( X : np . ndarray , Y : np . ndarray , W_k : np . ndarray , Lambda_k_1 : np . ndarray , a_tau_k : float , b_tau_k : float , a_alpha_k : float , b_alpha_k : float , r_k : np . ndarray ): \"\"\" [PDF p. 245] :param X: input matrix (N \u00d7 DX) :param Y: output matrix (N \u00d7 DY) :param W_k: submodel weight matrix (DY \u00d7 DX) :param Lambda_k_1: submodel covariance matrix (DX \u00d7 DX) :param a_tau_k: submodel noise precision parameter :param b_tau_k: submodel noise precision parameter :param a_alpha_k: weight vector prior parameter :param b_alpha_k: weight vector prior parameter :param r_k: responsibility vector (NumPy row or column vector, we reshape to (-1) anyways) :returns: rule component L_k(q) of variational bound \"\"\" DY , DX = W_k . shape E_tau_tau_k = a_tau_k / b_tau_k L_k_1_q = DY / 2 * ( ss . digamma ( a_tau_k ) - np . log ( b_tau_k ) - np . log ( 2 * np . pi )) * np . sum ( r_k ) # We reshape r_k to a NumPy row vector since NumPy seems to understand what # we want to do when we multiply two row vectors (i.e. a^T a). L_k_2_q = ( - 0.5 * r_k ) . reshape (( - 1 )) @ ( E_tau_tau_k * np . sum ( ( Y - X @ W_k . T ) ** 2 , axis = 1 ) + DY * np . sum ( X * ( X @ Lambda_k_1 ), axis = 1 )) L_k_3_q = - ss . gammaln ( HParams () . A_ALPHA ) + HParams () . A_ALPHA * np . log ( HParams () . B_ALPHA ) + ss . gammaln ( a_alpha_k ) - a_alpha_k * np . log ( b_alpha_k ) + DX * DY / 2 + DY / 2 * np . log ( np . linalg . det ( Lambda_k_1 )) L_k_4_q = DY * ( - ss . gammaln ( HParams () . A_TAU ) + HParams () . A_TAU * np . log ( HParams () . B_TAU ) + ( HParams () . A_TAU - a_tau_k ) * ss . digamma ( a_tau_k ) - HParams () . A_TAU * np . log ( b_tau_k ) - HParams () . B_TAU * E_tau_tau_k + ss . gammaln ( a_tau_k ) + a_tau_k ) return L_k_1_q + L_k_2_q + L_k_3_q + L_k_4_q","title":"var_cl_bound()"},{"location":"reference/berbl/literal/#berbl.literal.var_mix_bound","text":"[PDF p. 245] Parameters: Name Type Description Default G array of shape (N, K) Mixing (\u201cgating\u201d) matrix. required R array of shape (N, K) Responsibility matrix. required V array of shape (DV, K) Mixing weight matrix. required Lambda_V_1 array of shape (K Mixing weight covariance matrix. required a_beta array of shape (K,) Mixing weight prior parameter (row vector). required b_beta array of shape (K,) required Returns: Name Type Description L_M_q float Mixing component L_M(q) of variational bound. Source code in berbl/literal/__init__.py 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 def var_mix_bound ( G : np . ndarray , R : np . ndarray , V : np . ndarray , Lambda_V_1 : np . ndarray , a_beta : np . ndarray , b_beta : np . ndarray ): \"\"\" [PDF p. 245] Parameters ---------- G : array of shape (N, K) Mixing (\u201cgating\u201d) matrix. R : array of shape (N, K) Responsibility matrix. V : array of shape (DV, K) Mixing weight matrix. Lambda_V_1 : array of shape (K * DV, K * DV) Mixing weight covariance matrix. a_beta : array of shape (K,) Mixing weight prior parameter (row vector). b_beta : array of shape (K,) Returns ------- L_M_q : float Mixing component L_M(q) of variational bound. \"\"\" DV , K = V . shape assert G . shape == R . shape assert G . shape [ 1 ] == K assert Lambda_V_1 . shape == ( K * DV , K * DV ) assert a_beta . shape == ( K , ) assert b_beta . shape == ( K , ) L_M1q = K * ( - ss . gammaln ( HParams () . A_BETA ) + HParams () . A_BETA * np . log ( HParams () . B_BETA )) for k in range ( K ): try : L_M1q = L_M1q + ss . gammaln ( a_beta [ k ]) - a_beta [ k ] * np . log ( b_beta [ k ]) except FloatingPointError as e : known_issue ( \"FloatingPointError in var_mix_bound\" , ( f \"Lambda_V_1: { Lambda_V_1 } \" f \"V: { V } \" f \"K: { K } \" f \"k: { k } \" f \"b_beta[k]: { b_beta [ k ] } \" ), report = True ) raise e # L_M2q is the negative Kullback-Leibler divergence [PDF p. 246]. L_M2q = _kl ( R , G ) # L_M3q may be -inf after the following line but that is probably OK since # the ``train_mixing`` loop then aborts (also see comment in # ``train_mixing``). L_M3q = 0.5 * np . linalg . slogdet ( Lambda_V_1 )[ 1 ] + K * DV / 2 if np . any ( ~ np . isfinite ([ L_M1q , L_M2q , L_M3q ])): known_issue ( \"Infinite var_mix_bound\" , ( f \"Lambda_V_1 = { Lambda_V_1 } , \" f \"L_M1q = { L_M1q } , \" f \"L_M2q = { L_M2q } , \" f \"L_M3q = { L_M3q } \" )) return L_M1q + L_M2q + L_M3q","title":"var_mix_bound()"},{"location":"reference/berbl/literal/hyperparams/","text":"HParams () We use Alex Martelli's Borg pattern for not having to add Drugowitsch's hyper parameters to all signatures. See this <https://code.activestate.com/recipes/66531-singleton-we-dont-need-no-stinkin-singleton-the-bo/> _. Source code in berbl/literal/hyperparams.py 33 34 def __init__ ( self ): self . __dict__ = self . __shared_state","title":"hyperparams"},{"location":"reference/berbl/literal/hyperparams/#berbl.literal.hyperparams.HParams","text":"We use Alex Martelli's Borg pattern for not having to add Drugowitsch's hyper parameters to all signatures. See this <https://code.activestate.com/recipes/66531-singleton-we-dont-need-no-stinkin-singleton-the-bo/> _. Source code in berbl/literal/hyperparams.py 33 34 def __init__ ( self ): self . __dict__ = self . __shared_state","title":"HParams"},{"location":"reference/berbl/literal/model/","text":"Model ( matchs , random_state , add_bias = True , phi = None ) A model based on mixing localized linear submodels using the given model structure. Parameters: Name Type Description Default matchs List A list of matching functions (i.e. objects implementing a match attribute) defining the structure of this mixture. required random_state RandomState object required add_bias bool Whether to add an all-ones bias column to the input data. True phi mixing feature extractor (N \u00d7 Dx \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 None Source code in berbl/literal/model.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , matchs : List , random_state , add_bias = True , phi = None ): \"\"\" A model based on mixing localized linear submodels using the given model structure. Parameters ---------- matchs A list of matching functions (i.e. objects implementing a ``match`` attribute) defining the structure of this mixture. random_state : RandomState object add_bias : bool Whether to add an all-ones bias column to the input data. phi mixing feature extractor (N \u00d7 Dx \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1`` \"\"\" self . matchs = matchs self . add_bias = add_bias self . phi = phi self . random_state = random_state _predict_vars ( X ) No bias is added. Source code in berbl/literal/model.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def _predict_vars ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y_var = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): # A submodel's prediction variance. var = 2 * self . b_tau_ [ k ] / ( self . a_tau_ [ k ] - 1 ) * ( 1 + np . sum ( X * X @ self . Lambda_1_ [ k ], axis = 1 )) y_var [ k ] = var . reshape (( len ( X ), self . Dy_ )) . repeat ( self . Dy_ , axis = 1 ) return y_var _predicts ( X ) No bias is added. Source code in berbl/literal/model.py 196 197 198 199 200 201 202 203 204 205 206 207 def _predicts ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y = np . zeros (( self . K_ , N , self . Dy_ )) # TODO Maybe more efficient: np.sum(W[k] * X, axis=1) for k in range ( self . K_ ): # A submodel's prediction. y [ k ] = X @ self . W_ [ k ] . T return y predict_mean_var ( X ) Calculates prediction means and variances of the model for the provided inputs. Source code in berbl/literal/model.py 89 90 91 92 93 94 95 96 97 98 99 def predict_mean_var ( self , X : np . ndarray ): \"\"\" Calculates prediction means and variances of the model for the provided inputs. \"\"\" y = np . zeros (( len ( X ), self . Dy_ )) y_var = np . zeros (( len ( X ), self . Dy_ )) for i in range ( len ( X )): y [ i ], y_var [ i ] = self . predict_mean_var1 ( X [ i ]) return y , y_var predict_mean_var1 ( x ) Calculates prediction mean and variance of the model for the provided input. Literal (and inefficient) version given in Drugowitsch's book. Source code in berbl/literal/model.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def predict_mean_var1 ( self , x : np . ndarray ): \"\"\" Calculates prediction mean and variance of the model for the provided input. Literal (and inefficient) version given in Drugowitsch's book. \"\"\" Dy , Dx = self . W_ [ 0 ] . shape if self . add_bias : x = np . append ( 1 , x ) X = np . array ([ x ]) Phi = check_phi ( self . phi , X ) M = matching_matrix ( self . matchs , X ) G = mixing ( M , Phi , self . V_ ) # shape ((N=1), K) # assert G.shape == (1, self.K_) g = G [ 0 ] # Mean (7.108). gW = 0 for k in range ( len ( self . W_ )): gW += g [ k ] * self . W_ [ k ] y = gW @ x # Variance (7.109). var = np . zeros ( Dy ) for j in range ( Dy ): for k in range ( self . K_ ): var [ j ] += g [ k ] * ( 2 * self . b_tau_ [ k ] / ( self . a_tau_ [ k ] - 1 ) * ( 1 + x @ self . Lambda_1_ [ k ] @ x ) + ( self . W_ [ k ][ j ] @ x ) ** 2 ) var [ j ] -= y [ j ] ** 2 return y , var predict_mean_var_ ( X ) [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] :param X: input vector (N \u00d7 Dx) :returns: mean output vector (N \u00d7 Dy), variance of output (N \u00d7 Dy) Source code in berbl/literal/model.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def predict_mean_var_ ( self , X : np . ndarray ): \"\"\" [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] :param X: input vector (N \u00d7 Dx) :returns: mean output vector (N \u00d7 Dy), variance of output (N \u00d7 Dy) \"\"\" N , _ = X . shape Dy , Dx = self . W_ [ 0 ] . shape if self . add_bias : X = add_bias ( X ) # Collect the independent predictions and variances of each submodel. We # use the definitions of those that do neither perform input checking # nor bias adding to save some time. ys = self . _predicts ( X ) y_vars = self . _predict_vars ( X ) # Next, mix the predictions. Phi = check_phi ( self . phi , X ) M = matching_matrix ( self . matchs , X ) G_ = mixing ( M , Phi , self . V_ ) # For each rule's prediction, we weigh every dimension of the output # vector by the same amount, thus we simply repeat the G values over Dy. G = G_ . reshape ( ys . shape ) . repeat ( Dy , axis = 2 ) # K \u00d7 N \u00d7 Dy y = np . sum ( G * ys , axis = 0 ) y_var = np . sum ( G * ( y_vars + ys ** 2 ), axis = 0 ) - y ** 2 return y , y_var predict_vars ( X ) Returns this model's submodels' prediction variances, one by one, without mixing them. Returns: Type Description array of shape (K, N) Prediction variances of each submodel. Source code in berbl/literal/model.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def predict_vars ( self , X ): \"\"\" Returns this model's submodels' prediction variances, one by one, without mixing them. Returns ------- array of shape (K, N) Prediction variances of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predict_vars ( X ) predicts ( X ) Returns this model's submodels' predictions, one by one, without mixing them. Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/literal/model.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def predicts ( self , X ): \"\"\" Returns this model's submodels' predictions, one by one, without mixing them. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predicts ( X )","title":"model"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model","text":"A model based on mixing localized linear submodels using the given model structure. Parameters: Name Type Description Default matchs List A list of matching functions (i.e. objects implementing a match attribute) defining the structure of this mixture. required random_state RandomState object required add_bias bool Whether to add an all-ones bias column to the input data. True phi mixing feature extractor (N \u00d7 Dx \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 None Source code in berbl/literal/model.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , matchs : List , random_state , add_bias = True , phi = None ): \"\"\" A model based on mixing localized linear submodels using the given model structure. Parameters ---------- matchs A list of matching functions (i.e. objects implementing a ``match`` attribute) defining the structure of this mixture. random_state : RandomState object add_bias : bool Whether to add an all-ones bias column to the input data. phi mixing feature extractor (N \u00d7 Dx \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1`` \"\"\" self . matchs = matchs self . add_bias = add_bias self . phi = phi self . random_state = random_state","title":"Model"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model._predict_vars","text":"No bias is added. Source code in berbl/literal/model.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def _predict_vars ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y_var = np . zeros (( self . K_ , N , self . Dy_ )) for k in range ( self . K_ ): # A submodel's prediction variance. var = 2 * self . b_tau_ [ k ] / ( self . a_tau_ [ k ] - 1 ) * ( 1 + np . sum ( X * X @ self . Lambda_1_ [ k ], axis = 1 )) y_var [ k ] = var . reshape (( len ( X ), self . Dy_ )) . repeat ( self . Dy_ , axis = 1 ) return y_var","title":"_predict_vars()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model._predicts","text":"No bias is added. Source code in berbl/literal/model.py 196 197 198 199 200 201 202 203 204 205 206 207 def _predicts ( self , X ): \"\"\" No bias is added. \"\"\" N = len ( X ) y = np . zeros (( self . K_ , N , self . Dy_ )) # TODO Maybe more efficient: np.sum(W[k] * X, axis=1) for k in range ( self . K_ ): # A submodel's prediction. y [ k ] = X @ self . W_ [ k ] . T return y","title":"_predicts()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model.predict_mean_var","text":"Calculates prediction means and variances of the model for the provided inputs. Source code in berbl/literal/model.py 89 90 91 92 93 94 95 96 97 98 99 def predict_mean_var ( self , X : np . ndarray ): \"\"\" Calculates prediction means and variances of the model for the provided inputs. \"\"\" y = np . zeros (( len ( X ), self . Dy_ )) y_var = np . zeros (( len ( X ), self . Dy_ )) for i in range ( len ( X )): y [ i ], y_var [ i ] = self . predict_mean_var1 ( X [ i ]) return y , y_var","title":"predict_mean_var()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model.predict_mean_var1","text":"Calculates prediction mean and variance of the model for the provided input. Literal (and inefficient) version given in Drugowitsch's book. Source code in berbl/literal/model.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def predict_mean_var1 ( self , x : np . ndarray ): \"\"\" Calculates prediction mean and variance of the model for the provided input. Literal (and inefficient) version given in Drugowitsch's book. \"\"\" Dy , Dx = self . W_ [ 0 ] . shape if self . add_bias : x = np . append ( 1 , x ) X = np . array ([ x ]) Phi = check_phi ( self . phi , X ) M = matching_matrix ( self . matchs , X ) G = mixing ( M , Phi , self . V_ ) # shape ((N=1), K) # assert G.shape == (1, self.K_) g = G [ 0 ] # Mean (7.108). gW = 0 for k in range ( len ( self . W_ )): gW += g [ k ] * self . W_ [ k ] y = gW @ x # Variance (7.109). var = np . zeros ( Dy ) for j in range ( Dy ): for k in range ( self . K_ ): var [ j ] += g [ k ] * ( 2 * self . b_tau_ [ k ] / ( self . a_tau_ [ k ] - 1 ) * ( 1 + x @ self . Lambda_1_ [ k ] @ x ) + ( self . W_ [ k ][ j ] @ x ) ** 2 ) var [ j ] -= y [ j ] ** 2 return y , var","title":"predict_mean_var1()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model.predict_mean_var_","text":"[PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] :param X: input vector (N \u00d7 Dx) :returns: mean output vector (N \u00d7 Dy), variance of output (N \u00d7 Dy) Source code in berbl/literal/model.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def predict_mean_var_ ( self , X : np . ndarray ): \"\"\" [PDF p. 224] The mean and variance of the predictive density described by this model for each of the provided data points. \u201cAs the mixture of Student\u2019s t distributions might be multimodal, there exists no clear definition for the 95% confidence intervals, but a mixture density-related study that deals with this problem can be found in [118]. Here, we take the variance as a sufficient indicator of the prediction\u2019s confidence.\u201d [PDF p. 224] :param X: input vector (N \u00d7 Dx) :returns: mean output vector (N \u00d7 Dy), variance of output (N \u00d7 Dy) \"\"\" N , _ = X . shape Dy , Dx = self . W_ [ 0 ] . shape if self . add_bias : X = add_bias ( X ) # Collect the independent predictions and variances of each submodel. We # use the definitions of those that do neither perform input checking # nor bias adding to save some time. ys = self . _predicts ( X ) y_vars = self . _predict_vars ( X ) # Next, mix the predictions. Phi = check_phi ( self . phi , X ) M = matching_matrix ( self . matchs , X ) G_ = mixing ( M , Phi , self . V_ ) # For each rule's prediction, we weigh every dimension of the output # vector by the same amount, thus we simply repeat the G values over Dy. G = G_ . reshape ( ys . shape ) . repeat ( Dy , axis = 2 ) # K \u00d7 N \u00d7 Dy y = np . sum ( G * ys , axis = 0 ) y_var = np . sum ( G * ( y_vars + ys ** 2 ), axis = 0 ) - y ** 2 return y , y_var","title":"predict_mean_var_()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model.predict_vars","text":"Returns this model's submodels' prediction variances, one by one, without mixing them. Returns: Type Description array of shape (K, N) Prediction variances of each submodel. Source code in berbl/literal/model.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def predict_vars ( self , X ): \"\"\" Returns this model's submodels' prediction variances, one by one, without mixing them. Returns ------- array of shape (K, N) Prediction variances of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predict_vars ( X )","title":"predict_vars()"},{"location":"reference/berbl/literal/model/#berbl.literal.model.Model.predicts","text":"Returns this model's submodels' predictions, one by one, without mixing them. Returns: Type Description array of shape (K, N, Dy) Mean output vectors of each submodel. Source code in berbl/literal/model.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def predicts ( self , X ): \"\"\" Returns this model's submodels' predictions, one by one, without mixing them. Returns ------- array of shape (K, N, Dy) Mean output vectors of each submodel. \"\"\" if self . add_bias : X = add_bias ( X ) return self . _predicts ( X )","title":"predicts()"},{"location":"reference/berbl/match/","text":"Families of matching function.","title":"match"},{"location":"reference/berbl/match/allmatch/","text":"AllMatch () self.match is a matching function that matches all inputs. Source code in berbl/match/allmatch.py 8 9 def __init__ ( self ): pass match ( X ) Since this matching function matches all inputs, this always returns an all-ones matrix of shape (N, 1) (with each entry corresponding to one of the rows of the input matrix). Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array Matching vector (N) of this matching function Source code in berbl/match/allmatch.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def match ( self , X : np . ndarray ): \"\"\" Since this matching function matches all inputs, this always returns an all-ones matrix of shape `(N, 1)` (with each entry corresponding to one of the rows of the input matrix). Parameters ---------- X : array of shape `(N, DX)` Input matrix. Returns ------- array Matching vector ``(N)`` of this matching function \"\"\" return np . ones (( len ( X ), 1 ))","title":"allmatch"},{"location":"reference/berbl/match/allmatch/#berbl.match.allmatch.AllMatch","text":"self.match is a matching function that matches all inputs. Source code in berbl/match/allmatch.py 8 9 def __init__ ( self ): pass","title":"AllMatch"},{"location":"reference/berbl/match/allmatch/#berbl.match.allmatch.AllMatch.match","text":"Since this matching function matches all inputs, this always returns an all-ones matrix of shape (N, 1) (with each entry corresponding to one of the rows of the input matrix). Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array Matching vector (N) of this matching function Source code in berbl/match/allmatch.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def match ( self , X : np . ndarray ): \"\"\" Since this matching function matches all inputs, this always returns an all-ones matrix of shape `(N, 1)` (with each entry corresponding to one of the rows of the input matrix). Parameters ---------- X : array of shape `(N, DX)` Input matrix. Returns ------- array Matching vector ``(N)`` of this matching function \"\"\" return np . ones (( len ( X ), 1 ))","title":"match()"},{"location":"reference/berbl/match/init/","text":"Tools for initializing lists of matching functions (i.e. model structures). binomial_init ( n , p , allel_init , kmin = None , kmax = None , ** kwargs ) Creates a distribution over lists of RadialMatch based on a binomial distribution over the lists' lengths. List lengths are drawn from the distribution np.clip(binomial(n, p, size=size), kmin, kmax) . (Drugowitsch problem-dependently samples individual sizes from such distribution as well [PDF p. 221, 3rd paragraph]). Parameters: Name Type Description Default n int n-parameter of the underlying binomial distribution. required p float p-parameter of the underlying binomial distribution. required allel_init callable Callable for initializing a single random allele (e.g. RadialMatch.random_ball ), receives required kmin positive int Minimum value for individual lengths. If None (the default), assume kmin = 1 . None kmax int Maximum value for individual lengths. If None (the default), assume kmax = 10 * n . None **kwargs Passed through to func unchanged. {} Returns: Type Description callable receiving a A distribution over lists of random RadialMatch objects. Source code in berbl/match/init.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def binomial_init ( n , p , allel_init , kmin = None , kmax = None , ** kwargs ): \"\"\" Creates a distribution over lists of ``RadialMatch`` based on a binomial distribution over the lists' lengths. List lengths are drawn from the distribution ``np.clip(binomial(n, p, size=size), kmin, kmax)``. (Drugowitsch problem-dependently samples individual sizes from such distribution as well [PDF p. 221, 3rd paragraph]). Parameters ---------- n : int n-parameter of the underlying binomial distribution. p : float p-parameter of the underlying binomial distribution. allel_init : callable Callable for initializing a single random allele (e.g. ``RadialMatch.random_ball``), receives kmin : positive int Minimum value for individual lengths. If ``None`` (the default), assume ``kmin = 1``. kmax : int Maximum value for individual lengths. If ``None`` (the default), assume ``kmax = 10 * n``. **kwargs Passed through to ``func`` unchanged. Returns ------- callable receiving a ``np.random.RandomState`` A distribution over lists of random ``RadialMatch`` objects. \"\"\" if kmin is None : kmin = 1 if kmax is None : kmax = 10 * n # TODO Consider adding a parameter size (passed through to binomial etc.) to # be more efficient def init ( random_state ): # Draw a solution size. K = np . clip ( random_state . binomial ( n , p ), a_min = kmin , a_max = kmax ) # Initialize a solution. return [ allel_init ( random_state = random_state , ** kwargs ) for k in range ( K ) ] return init","title":"init"},{"location":"reference/berbl/match/init/#berbl.match.init.binomial_init","text":"Creates a distribution over lists of RadialMatch based on a binomial distribution over the lists' lengths. List lengths are drawn from the distribution np.clip(binomial(n, p, size=size), kmin, kmax) . (Drugowitsch problem-dependently samples individual sizes from such distribution as well [PDF p. 221, 3rd paragraph]). Parameters: Name Type Description Default n int n-parameter of the underlying binomial distribution. required p float p-parameter of the underlying binomial distribution. required allel_init callable Callable for initializing a single random allele (e.g. RadialMatch.random_ball ), receives required kmin positive int Minimum value for individual lengths. If None (the default), assume kmin = 1 . None kmax int Maximum value for individual lengths. If None (the default), assume kmax = 10 * n . None **kwargs Passed through to func unchanged. {} Returns: Type Description callable receiving a A distribution over lists of random RadialMatch objects. Source code in berbl/match/init.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def binomial_init ( n , p , allel_init , kmin = None , kmax = None , ** kwargs ): \"\"\" Creates a distribution over lists of ``RadialMatch`` based on a binomial distribution over the lists' lengths. List lengths are drawn from the distribution ``np.clip(binomial(n, p, size=size), kmin, kmax)``. (Drugowitsch problem-dependently samples individual sizes from such distribution as well [PDF p. 221, 3rd paragraph]). Parameters ---------- n : int n-parameter of the underlying binomial distribution. p : float p-parameter of the underlying binomial distribution. allel_init : callable Callable for initializing a single random allele (e.g. ``RadialMatch.random_ball``), receives kmin : positive int Minimum value for individual lengths. If ``None`` (the default), assume ``kmin = 1``. kmax : int Maximum value for individual lengths. If ``None`` (the default), assume ``kmax = 10 * n``. **kwargs Passed through to ``func`` unchanged. Returns ------- callable receiving a ``np.random.RandomState`` A distribution over lists of random ``RadialMatch`` objects. \"\"\" if kmin is None : kmin = 1 if kmax is None : kmax = 10 * n # TODO Consider adding a parameter size (passed through to binomial etc.) to # be more efficient def init ( random_state ): # Draw a solution size. K = np . clip ( random_state . binomial ( n , p ), a_min = kmin , a_max = kmax ) # Initialize a solution. return [ allel_init ( random_state = random_state , ** kwargs ) for k in range ( K ) ] return init","title":"binomial_init()"},{"location":"reference/berbl/match/nomatch/","text":"NoMatch () self.match is a matching function that doesn't match any of the inputs given to it. Not matching meaning here that the smallest positive non-zero number is returned (i.e. not matching in a fuzzy matching sense). Source code in berbl/match/nomatch.py 10 11 def __init__ ( self ): pass match ( X ) Since this matching function matches no inputs, this always returns an all-ones (N \u00d7 1) matrix (with each entry corresponding to one of the rows of the input matrix). :param X: input matrix (N \u00d7 D_X) with D_X == 1 :returns: matching vector (N) of this matching function Source code in berbl/match/nomatch.py 13 14 15 16 17 18 19 20 21 22 def match ( self , X : np . ndarray ): \"\"\" Since this matching function matches no inputs, this always returns an all-ones (N \u00d7 1) matrix (with each entry corresponding to one of the rows of the input matrix). :param X: input matrix ``(N \u00d7 D_X)`` with ``D_X == 1`` :returns: matching vector ``(N)`` of this matching function \"\"\" return np . repeat ( np . finfo ( None ) . tiny , len ( X ))[:, np . newaxis ]","title":"nomatch"},{"location":"reference/berbl/match/nomatch/#berbl.match.nomatch.NoMatch","text":"self.match is a matching function that doesn't match any of the inputs given to it. Not matching meaning here that the smallest positive non-zero number is returned (i.e. not matching in a fuzzy matching sense). Source code in berbl/match/nomatch.py 10 11 def __init__ ( self ): pass","title":"NoMatch"},{"location":"reference/berbl/match/nomatch/#berbl.match.nomatch.NoMatch.match","text":"Since this matching function matches no inputs, this always returns an all-ones (N \u00d7 1) matrix (with each entry corresponding to one of the rows of the input matrix). :param X: input matrix (N \u00d7 D_X) with D_X == 1 :returns: matching vector (N) of this matching function Source code in berbl/match/nomatch.py 13 14 15 16 17 18 19 20 21 22 def match ( self , X : np . ndarray ): \"\"\" Since this matching function matches no inputs, this always returns an all-ones (N \u00d7 1) matrix (with each entry corresponding to one of the rows of the input matrix). :param X: input matrix ``(N \u00d7 D_X)`` with ``D_X == 1`` :returns: matching vector ``(N)`` of this matching function \"\"\" return np . repeat ( np . finfo ( None ) . tiny , len ( X ))[:, np . newaxis ]","title":"match()"},{"location":"reference/berbl/match/radial1d_drugowitsch/","text":"RadialMatch1D ( * , a = None , b = None , mu = None , sigma_2 = None , has_bias = True , input_bounds = None ) self.match is a radial basis function\u2013based matching function as defined in Drugowitsch's book [PDF p. 256]. Parameters: Name Type Description Default a float Evolving parameter from which the position of the Gaussian is inferred ( 0 <= a <= 100 ). [PDF p. 256] Exactly one of a and mu has to be given (the other one can be inferred); the same goes for b and sigma_2 . None b float Evolving parameter from which the standard deviation of the Gaussian is inferred ( 0 <= b <= 50 ). See a . None mu float Position of the Gaussian. See a . None sigma_2 float Standard deviation. See a . None has_bias bool Whether to expect 2D data where we always match the first dimension (e.g. because it is all ones as a bias to implicitly fit the intercept). True input_bounds pair of two floats or None If None (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs should be standardized for everything else to work properly. None Source code in berbl/match/radial1d_drugowitsch.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , * , a : float = None , b : float = None , mu : float = None , sigma_2 : float = None , has_bias = True , # TODO Detect input_bounds automatedly input_bounds = None ): \"\"\" ``self.match`` is a radial basis function\u2013based matching function as defined in Drugowitsch's book [PDF p. 256]. Parameters ---------- a : float Evolving parameter from which the position of the Gaussian is inferred (``0 <= a <= 100``). [PDF p. 256] Exactly one of ``a`` and ``mu`` has to be given (the other one can be inferred); the same goes for ``b`` and ``sigma_2``. b : float Evolving parameter from which the standard deviation of the Gaussian is inferred (``0 <= b <= 50``). See ``a``. mu : float Position of the Gaussian. See ``a``. sigma_2 : float Standard deviation. See ``a``. has_bias : bool Whether to expect 2D data where we always match the first dimension (e.g. because it is all ones as a bias to implicitly fit the intercept). input_bounds : pair of two floats or None If ``None`` (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs *should* be standardized for everything else to work properly. \"\"\" self . has_bias = has_bias if input_bounds is not None : self . _l , self . _u = input_bounds print ( \"Warning: Changed matching function input bounds \" f \"to { input_bounds } \" ) else : # TODO This is not ideal: Standardized does not imply [-1, 1]. self . _l , self . _u = - 1 , 1 if a is not None and mu is None : self . a = a elif a is None and mu is not None : self . a = 100 * ( mu - self . _l ) / ( self . _u - self . _l ) else : raise ValueError ( \"Exactly one of a and mu has to be given\" ) if b is not None and sigma_2 is None : self . b = b elif b is None and sigma_2 is not None : self . b = - 10 * np . log10 ( sigma_2 ) if not ( 0 <= self . b <= 50 ): raise ValueError ( \"sigma_2 is too small (i.e. probably too close to zero)\" ) else : raise ValueError ( \"Exactly one of b and sigma_2 has to be given\" ) _match_wo_bias ( X ) Compute matching vector for given input assuming that the input doesn't have bias column. We vectorize the following (i.e. feed the whole input through at once):: for n in range(len(X)): M[n] = np.exp(-0.5 / sigma_2 * (x - mu)**2) :param X: input matrix (N \u00d7 D_X) with D_X == 1 :returns: matching vector (N) of this matching function (i.e. of this rule) Source code in berbl/match/radial1d_drugowitsch.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def _match_wo_bias ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Compute matching vector for given input assuming that the input doesn't have bias column. We vectorize the following (i.e. feed the whole input through at once):: for n in range(len(X)): M[n] = np.exp(-0.5 / sigma_2 * (x - mu)**2) :param X: input matrix ``(N \u00d7 D_X)`` with ``D_X == 1`` :returns: matching vector ``(N)`` of this matching function (i.e. of this rule) \"\"\" # We have to clip this so we don't return 0 here (0 should never be # returned because every match function matches everywhere at least a # little bit). Also, we clip from above such that this function never # returns a value larger than 1 (it's a probability, after all), meaning # that m should not be larger than 0. m_min = np . log ( np . finfo ( None ) . tiny ) m_max = 0 # NOTE If ``self.sigma_2()`` is very close to 0 then the next line may # result in ``nan`` due to ``-inf * 0 = nan``. However, if we use ``b`` # to set ``self.sigma_2()`` this problem will not occur. m = np . clip ( - 0.5 / self . sigma_2 () * ( X - self . mu ()) ** 2 , m_min , m_max ) return np . exp ( m ) match ( X ) Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute self.has_bias ), remove that beforehand. Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/radial1d_drugowitsch.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def match ( self , X : np . ndarray ): \"\"\" Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute ``self.has_bias``), remove that beforehand. Parameters ---------- X : array of shape ``(N, 1)`` or ``(N, 2)`` if ``self.has_bias`` Input matrix. Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" if self . has_bias : assert X . shape [ 1 ] == 2 , f \"X should have 2 columns but has { X . shape [ 1 ] } \" X = X . T [ 1 :] . T return self . _match_wo_bias ( X ) mutate ( random_state ) [PDF p. 256] Source code in berbl/match/radial1d_drugowitsch.py 91 92 93 94 95 96 97 98 def mutate ( self , random_state : np . random . RandomState ): \"\"\" [PDF p. 256] \"\"\" # NOTE LCSBookCode puts int(...) around the normal. self . a = np . clip ( random_state . normal ( loc = self . a , scale = 10 ), 0 , 100 ) self . b = np . clip ( random_state . normal ( loc = self . b , scale = 5 ), 0 , 50 ) return self random ( random_state , input_bounds = None ) [PDF p. 256] Source code in berbl/match/radial1d_drugowitsch.py 81 82 83 84 85 86 87 88 89 @classmethod def random ( cls , random_state : np . random . RandomState , input_bounds = None ): \"\"\" [PDF p. 256] \"\"\" random_state = check_random_state ( random_state ) return RadialMatch1D ( a = random_state . uniform ( 0 , 100 ), b = random_state . uniform ( 0 , 50 ), input_bounds = input_bounds )","title":"radial1d_drugowitsch"},{"location":"reference/berbl/match/radial1d_drugowitsch/#berbl.match.radial1d_drugowitsch.RadialMatch1D","text":"self.match is a radial basis function\u2013based matching function as defined in Drugowitsch's book [PDF p. 256]. Parameters: Name Type Description Default a float Evolving parameter from which the position of the Gaussian is inferred ( 0 <= a <= 100 ). [PDF p. 256] Exactly one of a and mu has to be given (the other one can be inferred); the same goes for b and sigma_2 . None b float Evolving parameter from which the standard deviation of the Gaussian is inferred ( 0 <= b <= 50 ). See a . None mu float Position of the Gaussian. See a . None sigma_2 float Standard deviation. See a . None has_bias bool Whether to expect 2D data where we always match the first dimension (e.g. because it is all ones as a bias to implicitly fit the intercept). True input_bounds pair of two floats or None If None (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs should be standardized for everything else to work properly. None Source code in berbl/match/radial1d_drugowitsch.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , * , a : float = None , b : float = None , mu : float = None , sigma_2 : float = None , has_bias = True , # TODO Detect input_bounds automatedly input_bounds = None ): \"\"\" ``self.match`` is a radial basis function\u2013based matching function as defined in Drugowitsch's book [PDF p. 256]. Parameters ---------- a : float Evolving parameter from which the position of the Gaussian is inferred (``0 <= a <= 100``). [PDF p. 256] Exactly one of ``a`` and ``mu`` has to be given (the other one can be inferred); the same goes for ``b`` and ``sigma_2``. b : float Evolving parameter from which the standard deviation of the Gaussian is inferred (``0 <= b <= 50``). See ``a``. mu : float Position of the Gaussian. See ``a``. sigma_2 : float Standard deviation. See ``a``. has_bias : bool Whether to expect 2D data where we always match the first dimension (e.g. because it is all ones as a bias to implicitly fit the intercept). input_bounds : pair of two floats or None If ``None`` (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs *should* be standardized for everything else to work properly. \"\"\" self . has_bias = has_bias if input_bounds is not None : self . _l , self . _u = input_bounds print ( \"Warning: Changed matching function input bounds \" f \"to { input_bounds } \" ) else : # TODO This is not ideal: Standardized does not imply [-1, 1]. self . _l , self . _u = - 1 , 1 if a is not None and mu is None : self . a = a elif a is None and mu is not None : self . a = 100 * ( mu - self . _l ) / ( self . _u - self . _l ) else : raise ValueError ( \"Exactly one of a and mu has to be given\" ) if b is not None and sigma_2 is None : self . b = b elif b is None and sigma_2 is not None : self . b = - 10 * np . log10 ( sigma_2 ) if not ( 0 <= self . b <= 50 ): raise ValueError ( \"sigma_2 is too small (i.e. probably too close to zero)\" ) else : raise ValueError ( \"Exactly one of b and sigma_2 has to be given\" )","title":"RadialMatch1D"},{"location":"reference/berbl/match/radial1d_drugowitsch/#berbl.match.radial1d_drugowitsch.RadialMatch1D._match_wo_bias","text":"Compute matching vector for given input assuming that the input doesn't have bias column. We vectorize the following (i.e. feed the whole input through at once):: for n in range(len(X)): M[n] = np.exp(-0.5 / sigma_2 * (x - mu)**2) :param X: input matrix (N \u00d7 D_X) with D_X == 1 :returns: matching vector (N) of this matching function (i.e. of this rule) Source code in berbl/match/radial1d_drugowitsch.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def _match_wo_bias ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Compute matching vector for given input assuming that the input doesn't have bias column. We vectorize the following (i.e. feed the whole input through at once):: for n in range(len(X)): M[n] = np.exp(-0.5 / sigma_2 * (x - mu)**2) :param X: input matrix ``(N \u00d7 D_X)`` with ``D_X == 1`` :returns: matching vector ``(N)`` of this matching function (i.e. of this rule) \"\"\" # We have to clip this so we don't return 0 here (0 should never be # returned because every match function matches everywhere at least a # little bit). Also, we clip from above such that this function never # returns a value larger than 1 (it's a probability, after all), meaning # that m should not be larger than 0. m_min = np . log ( np . finfo ( None ) . tiny ) m_max = 0 # NOTE If ``self.sigma_2()`` is very close to 0 then the next line may # result in ``nan`` due to ``-inf * 0 = nan``. However, if we use ``b`` # to set ``self.sigma_2()`` this problem will not occur. m = np . clip ( - 0.5 / self . sigma_2 () * ( X - self . mu ()) ** 2 , m_min , m_max ) return np . exp ( m )","title":"_match_wo_bias()"},{"location":"reference/berbl/match/radial1d_drugowitsch/#berbl.match.radial1d_drugowitsch.RadialMatch1D.match","text":"Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute self.has_bias ), remove that beforehand. Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/radial1d_drugowitsch.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def match ( self , X : np . ndarray ): \"\"\" Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute ``self.has_bias``), remove that beforehand. Parameters ---------- X : array of shape ``(N, 1)`` or ``(N, 2)`` if ``self.has_bias`` Input matrix. Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" if self . has_bias : assert X . shape [ 1 ] == 2 , f \"X should have 2 columns but has { X . shape [ 1 ] } \" X = X . T [ 1 :] . T return self . _match_wo_bias ( X )","title":"match()"},{"location":"reference/berbl/match/radial1d_drugowitsch/#berbl.match.radial1d_drugowitsch.RadialMatch1D.mutate","text":"[PDF p. 256] Source code in berbl/match/radial1d_drugowitsch.py 91 92 93 94 95 96 97 98 def mutate ( self , random_state : np . random . RandomState ): \"\"\" [PDF p. 256] \"\"\" # NOTE LCSBookCode puts int(...) around the normal. self . a = np . clip ( random_state . normal ( loc = self . a , scale = 10 ), 0 , 100 ) self . b = np . clip ( random_state . normal ( loc = self . b , scale = 5 ), 0 , 50 ) return self","title":"mutate()"},{"location":"reference/berbl/match/radial1d_drugowitsch/#berbl.match.radial1d_drugowitsch.RadialMatch1D.random","text":"[PDF p. 256] Source code in berbl/match/radial1d_drugowitsch.py 81 82 83 84 85 86 87 88 89 @classmethod def random ( cls , random_state : np . random . RandomState , input_bounds = None ): \"\"\" [PDF p. 256] \"\"\" random_state = check_random_state ( random_state ) return RadialMatch1D ( a = random_state . uniform ( 0 , 100 ), b = random_state . uniform ( 0 , 50 ), input_bounds = input_bounds )","title":"random()"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/","text":"SoftInterval1D ( l , u , has_bias = True , input_bounds = None ) self.match is a soft interval\u2013based matching function as defined in Drugowitsch's book [PDF p. 260]. \u201cWhen specifying the interval for [rule] k by its lower bound l_k and upper bound u_k, we want exactly one standard deviation of the Gaussian to lie inside this interval, and additionally require 95% of the area underneath the matching function to be inside this interval.\u201d Parameters: Name Type Description Default input_bounds pair of two floats or None If None (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs should be standardized for everything else to work properly. None Source code in berbl/match/softinterval1d_drugowitsch.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , l : float , u : float , has_bias = True , input_bounds = None ): \"\"\" ``self.match`` is a soft interval\u2013based matching function as defined in Drugowitsch's book [PDF p. 260]. \u201cWhen specifying the interval for [rule] k by its lower bound l_k and upper bound u_k, we want exactly one standard deviation of the Gaussian to lie inside this interval, and additionally require 95% of the area underneath the matching function to be inside this interval.\u201d Parameters ---------- input_bounds : pair of two floats or None If ``None`` (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs *should* be standardized for everything else to work properly. \"\"\" self . has_bias = has_bias if input_bounds is not None : self . _l , self . _u = input_bounds print ( \"Warning: Changed matching function input bounds \" f \"to { input_bounds } \" ) else : self . _l , self . _u = - 1 , 1 # Unordered bound representation, we swap if necessary. [PDF p. 261] self . l , self . u = tuple ( sorted ([ l , u ])) _match_wo_bias ( X ) Compute matching vector for given input assuming that the input doesn't have a bias column. Parameters: Name Type Description Default X input matrix of shape required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/softinterval1d_drugowitsch.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def _match_wo_bias ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Compute matching vector for given input assuming that the input doesn't have a bias column. Parameters ---------- X : input matrix of shape ``(N, DX)`` with ``DX == 1`` Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" sigma2 = self . sigma2 () # The interval may be trivial. if sigma2 == 0 : return np . where ( X == self . u , 1 , np . finfo ( None ) . tiny ) else : # We have to clip this so we don't return 0 here (0 should never be # returned because every match function matches everywhere at least a # little bit). conds = [ X < self . l , X > self . u , ] cases = [ np . exp ( - 1 / ( 2 * sigma2 ) * ( X - self . l ) ** 2 ), np . exp ( - 1 / ( 2 * sigma2 ) * ( X - self . u ) ** 2 ), ] default = 1 m = np . select ( conds , cases , default = default ) m_min = np . finfo ( None ) . tiny m_max = 1 return np . clip ( m , m_min , m_max ) match ( X ) Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute self.has_bias ), remove that beforehand. Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/softinterval1d_drugowitsch.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , X : np . ndarray ): \"\"\" Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute ``self.has_bias``), remove that beforehand. Parameters ---------- X : array of shape ``(N, 1)`` or ``(N, 2)`` if ``self.has_bias`` Input matrix. Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" if self . has_bias : assert X . shape [ 1 ] == 2 , f \"X should have 2 columns but has { X . shape [ 1 ] } \" X = X . T [ 1 :] . T return self . _match_wo_bias ( X ) mutate ( random_state ) Note: Unordered bound representation, we swap if necessary. [PDF p. 261] Source code in berbl/match/softinterval1d_drugowitsch.py 60 61 62 63 64 65 66 67 68 69 70 71 72 def mutate ( self , random_state : np . random . RandomState ): \"\"\" Note: Unordered bound representation, we swap if necessary. [PDF p. 261] \"\"\" self . l , self . u = tuple ( sorted ( np . clip ( random_state . normal ( loc = ( self . l , self . u ), scale = ( self . _u - self . _l ) / 10 ), self . _l , self . _u ))) return self random ( random_state , input_bounds = None ) [PDF p. 260] Parameters: Name Type Description Default input_bounds pair of two floats or None See constructor documentation. None Source code in berbl/match/softinterval1d_drugowitsch.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @classmethod def random ( cls , random_state : np . random . RandomState , input_bounds = None ): \"\"\" [PDF p. 260] Parameters ---------- input_bounds : pair of two floats or None See constructor documentation. \"\"\" if input_bounds is not None : _l , _u = input_bounds else : _l , _u = - 1 , 1 l , u = tuple ( random_state . uniform ( _l , _u , size = 2 )) return SoftInterval1D ( l , u , input_bounds = input_bounds )","title":"softinterval1d_drugowitsch"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/#berbl.match.softinterval1d_drugowitsch.SoftInterval1D","text":"self.match is a soft interval\u2013based matching function as defined in Drugowitsch's book [PDF p. 260]. \u201cWhen specifying the interval for [rule] k by its lower bound l_k and upper bound u_k, we want exactly one standard deviation of the Gaussian to lie inside this interval, and additionally require 95% of the area underneath the matching function to be inside this interval.\u201d Parameters: Name Type Description Default input_bounds pair of two floats or None If None (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs should be standardized for everything else to work properly. None Source code in berbl/match/softinterval1d_drugowitsch.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , l : float , u : float , has_bias = True , input_bounds = None ): \"\"\" ``self.match`` is a soft interval\u2013based matching function as defined in Drugowitsch's book [PDF p. 260]. \u201cWhen specifying the interval for [rule] k by its lower bound l_k and upper bound u_k, we want exactly one standard deviation of the Gaussian to lie inside this interval, and additionally require 95% of the area underneath the matching function to be inside this interval.\u201d Parameters ---------- input_bounds : pair of two floats or None If ``None`` (the default), input is assumed to be standardized. Otherwise, input is assumed to lie within the interval described by the two floats. Note that inputs *should* be standardized for everything else to work properly. \"\"\" self . has_bias = has_bias if input_bounds is not None : self . _l , self . _u = input_bounds print ( \"Warning: Changed matching function input bounds \" f \"to { input_bounds } \" ) else : self . _l , self . _u = - 1 , 1 # Unordered bound representation, we swap if necessary. [PDF p. 261] self . l , self . u = tuple ( sorted ([ l , u ]))","title":"SoftInterval1D"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/#berbl.match.softinterval1d_drugowitsch.SoftInterval1D._match_wo_bias","text":"Compute matching vector for given input assuming that the input doesn't have a bias column. Parameters: Name Type Description Default X input matrix of shape required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/softinterval1d_drugowitsch.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def _match_wo_bias ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Compute matching vector for given input assuming that the input doesn't have a bias column. Parameters ---------- X : input matrix of shape ``(N, DX)`` with ``DX == 1`` Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" sigma2 = self . sigma2 () # The interval may be trivial. if sigma2 == 0 : return np . where ( X == self . u , 1 , np . finfo ( None ) . tiny ) else : # We have to clip this so we don't return 0 here (0 should never be # returned because every match function matches everywhere at least a # little bit). conds = [ X < self . l , X > self . u , ] cases = [ np . exp ( - 1 / ( 2 * sigma2 ) * ( X - self . l ) ** 2 ), np . exp ( - 1 / ( 2 * sigma2 ) * ( X - self . u ) ** 2 ), ] default = 1 m = np . select ( conds , cases , default = default ) m_min = np . finfo ( None ) . tiny m_max = 1 return np . clip ( m , m_min , m_max )","title":"_match_wo_bias()"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/#berbl.match.softinterval1d_drugowitsch.SoftInterval1D.match","text":"Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute self.has_bias ), remove that beforehand. Parameters: Name Type Description Default X array of shape Input matrix. required Returns: Type Description array of shape Matching vector of this matching function for the given input. Source code in berbl/match/softinterval1d_drugowitsch.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , X : np . ndarray ): \"\"\" Compute matching vector for given input. Depending on whether the input is expected to have a bias column (see attribute ``self.has_bias``), remove that beforehand. Parameters ---------- X : array of shape ``(N, 1)`` or ``(N, 2)`` if ``self.has_bias`` Input matrix. Returns ------- array of shape ``(N)`` Matching vector of this matching function for the given input. \"\"\" if self . has_bias : assert X . shape [ 1 ] == 2 , f \"X should have 2 columns but has { X . shape [ 1 ] } \" X = X . T [ 1 :] . T return self . _match_wo_bias ( X )","title":"match()"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/#berbl.match.softinterval1d_drugowitsch.SoftInterval1D.mutate","text":"Note: Unordered bound representation, we swap if necessary. [PDF p. 261] Source code in berbl/match/softinterval1d_drugowitsch.py 60 61 62 63 64 65 66 67 68 69 70 71 72 def mutate ( self , random_state : np . random . RandomState ): \"\"\" Note: Unordered bound representation, we swap if necessary. [PDF p. 261] \"\"\" self . l , self . u = tuple ( sorted ( np . clip ( random_state . normal ( loc = ( self . l , self . u ), scale = ( self . _u - self . _l ) / 10 ), self . _l , self . _u ))) return self","title":"mutate()"},{"location":"reference/berbl/match/softinterval1d_drugowitsch/#berbl.match.softinterval1d_drugowitsch.SoftInterval1D.random","text":"[PDF p. 260] Parameters: Name Type Description Default input_bounds pair of two floats or None See constructor documentation. None Source code in berbl/match/softinterval1d_drugowitsch.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @classmethod def random ( cls , random_state : np . random . RandomState , input_bounds = None ): \"\"\" [PDF p. 260] Parameters ---------- input_bounds : pair of two floats or None See constructor documentation. \"\"\" if input_bounds is not None : _l , _u = input_bounds else : _l , _u = - 1 , 1 l , u = tuple ( random_state . uniform ( _l , _u , size = 2 )) return SoftInterval1D ( l , u , input_bounds = input_bounds )","title":"random()"},{"location":"reference/berbl/search/","text":"","title":"search"},{"location":"reference/berbl/search/ga/","text":"Genetic algorithms that can be used for searching model structure space.","title":"ga"},{"location":"reference/berbl/search/ga/drugowitsch/","text":"GADrugowitsch ( toolbox , random_state , pop_size = 20 , cxpb = 0.4 , mupb = 0.4 , n_iter = 250 , add_bias = True ) A DEAP-based implementation of the GA algorithm found in Drugowitsch's book. The genotypes aren't fixed to be of the same form as Drugowitsch's (i.e. this mimics only the general algorithmic part of the GA which can be applied to many different forms of individuals). The exact operator instances used are expected to be given as part of the toolbox object (just as it is the case for the algorithms implementations that are part of DEAP). Parameters: Name Type Description Default toolbox object A DEAP Toolbox object that specifies all the operators required by this metaheuristic. required random_state int , NumPy ( legacy ) Due to scikit-learn compatibility, we use NumPy's legacy API. required pop_size int Population size. 20 cxpb float in [0, 1] Crossover probability. 0.4 mupb float in [0, 1] Mutation probability. 0.4 n_iter positive int Number of iterations to run. 250 Source code in berbl/search/ga/drugowitsch.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , toolbox , random_state , pop_size = 20 , cxpb = 0.4 , mupb = 0.4 , n_iter = 250 , add_bias = True ): \"\"\" Parameters ---------- toolbox : object A DEAP ``Toolbox`` object that specifies all the operators required by this metaheuristic. random_state : int, NumPy (legacy) ``RandomState`` object Due to scikit-learn compatibility, we use NumPy's legacy API. pop_size : int Population size. cxpb : float in [0, 1] Crossover probability. mupb : float in [0, 1] Mutation probability. n_iter : positive int Number of iterations to run. \"\"\" self . toolbox = toolbox self . pop_size = pop_size self . cxpb = cxpb self . mupb = mupb self . n_iter = n_iter self . random_state = random_state self . add_bias = add_bias fit ( X , y ) Fit this to the data. This gets called by berbl.BERBL.fit . Source code in berbl/search/ga/drugowitsch.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fit this to the data. This gets called by [berbl.BERBL.fit][]. \"\"\" random_state = check_random_state ( self . random_state ) # DEAP uses the global ``random.random`` RNG. seed = randseed ( random_state ) random . seed ( seed ) self . pop_ = self . toolbox . population ( n = self . pop_size ) fitnesses = [ self . toolbox . evaluate ( i , X , y ) for i in self . pop_ ] for ind , fit in zip ( self . pop_ , fitnesses ): ind . fitness . values = fit self . elitist_ = tools . HallOfFame ( 1 ) self . elitist_ . update ( self . pop_ ) for i in range ( self . n_iter ): elitist = self . elitist_ [ 0 ] # TODO Consider a more modular setup for logging log_metric ( \"elitist.p_M_D\" , elitist . fitness . values [ 0 ], i ) print ( f \"Generation { i } . Elitist of size { len ( elitist ) } with p(M | D) \" f \"= { elitist . fitness . values [ 0 ] : .2 } \" ) pop_new = [] while len ( pop_new ) < self . pop_size : # \u201cWe create a new population by selecting two individuals from # the current population. \u2026 is repeated until the new population # again holds P individuals. Then, the new population replaces # the current one and the next iteration begins.\u201d offspring = self . toolbox . select ( self . pop_ ) offspring = [ self . toolbox . clone ( ind ) for ind in offspring ] offspring_ = [] for c1 , c2 in zip ( offspring [:: 2 ], offspring [ 1 :: 2 ]): # \u201cApply crossover with probability pc \u2026\u201d if random_state . random () < self . cxpb : # TODO I don't yet modify c1 and c2 in-place. Must I? c1_ , c2_ = self . toolbox . mate ( c1 , c2 , random_state = random_state ) c1_ = creator . Genotype ( c1_ ) c2_ = creator . Genotype ( c2_ ) del c1_ . fitness . values del c2_ . fitness . values offspring_ += [ c1_ , c2_ ] offspring = offspring_ for c in offspring : # \u201c\u2026 and mutation with probability pm.\u201d if random_state . random () < self . mupb : self . toolbox . mutate ( c , random_state = random_state ) del c . fitness . values invalids = [ ind for ind in offspring if not ind . fitness . valid ] fitnesses = [ self . toolbox . evaluate ( ind , X , y ) for ind in invalids ] for ind , fit in zip ( invalids , fitnesses ): ind . fitness . values = fit pop_new += offspring self . pop_ [:] = pop_new self . elitist_ . update ( self . pop_ ) # TODO Doc those self . size_ = [ len ( i ) for i in self . elitist_ ] self . p_M_D_ = [ i . phenotype . p_M_D_ for i in self . elitist_ ] return self frozen () Returns a picklable copy of this object (we simply remove the toolbox). Source code in berbl/search/ga/drugowitsch.py 168 169 170 171 172 173 174 def frozen ( self ): \"\"\" Returns a picklable copy of this object (we simply remove the toolbox). \"\"\" copy = deepcopy ( self ) del copy . toolbox return copy predict ( X ) Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict . Source code in berbl/search/ga/drugowitsch.py 136 137 138 139 140 141 142 def predict ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict ( X ) predict_distribution ( x ) Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict_distribution . Source code in berbl/search/ga/drugowitsch.py 160 161 162 163 164 165 166 def predict_distribution ( self , x ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict_distribution][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict_distribution ( x ) predict_mean_var ( X ) Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict_mean_var . Source code in berbl/search/ga/drugowitsch.py 144 145 146 147 148 149 150 def predict_mean_var ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict_mean_var][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict_mean_var ( X ) predicts ( X ) Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predicts . Source code in berbl/search/ga/drugowitsch.py 152 153 154 155 156 157 158 def predicts ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predicts][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predicts ( X )","title":"drugowitsch"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch","text":"A DEAP-based implementation of the GA algorithm found in Drugowitsch's book. The genotypes aren't fixed to be of the same form as Drugowitsch's (i.e. this mimics only the general algorithmic part of the GA which can be applied to many different forms of individuals). The exact operator instances used are expected to be given as part of the toolbox object (just as it is the case for the algorithms implementations that are part of DEAP). Parameters: Name Type Description Default toolbox object A DEAP Toolbox object that specifies all the operators required by this metaheuristic. required random_state int , NumPy ( legacy ) Due to scikit-learn compatibility, we use NumPy's legacy API. required pop_size int Population size. 20 cxpb float in [0, 1] Crossover probability. 0.4 mupb float in [0, 1] Mutation probability. 0.4 n_iter positive int Number of iterations to run. 250 Source code in berbl/search/ga/drugowitsch.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , toolbox , random_state , pop_size = 20 , cxpb = 0.4 , mupb = 0.4 , n_iter = 250 , add_bias = True ): \"\"\" Parameters ---------- toolbox : object A DEAP ``Toolbox`` object that specifies all the operators required by this metaheuristic. random_state : int, NumPy (legacy) ``RandomState`` object Due to scikit-learn compatibility, we use NumPy's legacy API. pop_size : int Population size. cxpb : float in [0, 1] Crossover probability. mupb : float in [0, 1] Mutation probability. n_iter : positive int Number of iterations to run. \"\"\" self . toolbox = toolbox self . pop_size = pop_size self . cxpb = cxpb self . mupb = mupb self . n_iter = n_iter self . random_state = random_state self . add_bias = add_bias","title":"GADrugowitsch"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.fit","text":"Fit this to the data. This gets called by berbl.BERBL.fit . Source code in berbl/search/ga/drugowitsch.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Fit this to the data. This gets called by [berbl.BERBL.fit][]. \"\"\" random_state = check_random_state ( self . random_state ) # DEAP uses the global ``random.random`` RNG. seed = randseed ( random_state ) random . seed ( seed ) self . pop_ = self . toolbox . population ( n = self . pop_size ) fitnesses = [ self . toolbox . evaluate ( i , X , y ) for i in self . pop_ ] for ind , fit in zip ( self . pop_ , fitnesses ): ind . fitness . values = fit self . elitist_ = tools . HallOfFame ( 1 ) self . elitist_ . update ( self . pop_ ) for i in range ( self . n_iter ): elitist = self . elitist_ [ 0 ] # TODO Consider a more modular setup for logging log_metric ( \"elitist.p_M_D\" , elitist . fitness . values [ 0 ], i ) print ( f \"Generation { i } . Elitist of size { len ( elitist ) } with p(M | D) \" f \"= { elitist . fitness . values [ 0 ] : .2 } \" ) pop_new = [] while len ( pop_new ) < self . pop_size : # \u201cWe create a new population by selecting two individuals from # the current population. \u2026 is repeated until the new population # again holds P individuals. Then, the new population replaces # the current one and the next iteration begins.\u201d offspring = self . toolbox . select ( self . pop_ ) offspring = [ self . toolbox . clone ( ind ) for ind in offspring ] offspring_ = [] for c1 , c2 in zip ( offspring [:: 2 ], offspring [ 1 :: 2 ]): # \u201cApply crossover with probability pc \u2026\u201d if random_state . random () < self . cxpb : # TODO I don't yet modify c1 and c2 in-place. Must I? c1_ , c2_ = self . toolbox . mate ( c1 , c2 , random_state = random_state ) c1_ = creator . Genotype ( c1_ ) c2_ = creator . Genotype ( c2_ ) del c1_ . fitness . values del c2_ . fitness . values offspring_ += [ c1_ , c2_ ] offspring = offspring_ for c in offspring : # \u201c\u2026 and mutation with probability pm.\u201d if random_state . random () < self . mupb : self . toolbox . mutate ( c , random_state = random_state ) del c . fitness . values invalids = [ ind for ind in offspring if not ind . fitness . valid ] fitnesses = [ self . toolbox . evaluate ( ind , X , y ) for ind in invalids ] for ind , fit in zip ( invalids , fitnesses ): ind . fitness . values = fit pop_new += offspring self . pop_ [:] = pop_new self . elitist_ . update ( self . pop_ ) # TODO Doc those self . size_ = [ len ( i ) for i in self . elitist_ ] self . p_M_D_ = [ i . phenotype . p_M_D_ for i in self . elitist_ ] return self","title":"fit()"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.frozen","text":"Returns a picklable copy of this object (we simply remove the toolbox). Source code in berbl/search/ga/drugowitsch.py 168 169 170 171 172 173 174 def frozen ( self ): \"\"\" Returns a picklable copy of this object (we simply remove the toolbox). \"\"\" copy = deepcopy ( self ) del copy . toolbox return copy","title":"frozen()"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.predict","text":"Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict . Source code in berbl/search/ga/drugowitsch.py 136 137 138 139 140 141 142 def predict ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict ( X )","title":"predict()"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.predict_distribution","text":"Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict_distribution . Source code in berbl/search/ga/drugowitsch.py 160 161 162 163 164 165 166 def predict_distribution ( self , x ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict_distribution][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict_distribution ( x )","title":"predict_distribution()"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.predict_mean_var","text":"Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predict_mean_var . Source code in berbl/search/ga/drugowitsch.py 144 145 146 147 148 149 150 def predict_mean_var ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predict_mean_var][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predict_mean_var ( X )","title":"predict_mean_var()"},{"location":"reference/berbl/search/ga/drugowitsch/#berbl.search.ga.drugowitsch.GADrugowitsch.predicts","text":"Uses the current elitist to perform a prediction. This gets called by berbl.BERBL.predicts . Source code in berbl/search/ga/drugowitsch.py 152 153 154 155 156 157 158 def predicts ( self , X ): \"\"\" Uses the current elitist to perform a prediction. This gets called by [berbl.BERBL.predicts][]. \"\"\" return self . elitist_ [ 0 ] . phenotype . predicts ( X )","title":"predicts()"},{"location":"reference/berbl/search/operators/","text":"Base definitions for all toolboxes. Toolbox ( random_state , literal = False , add_bias = True , phi = None , fit_mixing = 'laplace' , ** kwargs ) Bases: base . Toolbox Base class for toolboxes that are used to perform model structure search. Defines and registers evaluate depending on the provided parameters. Parameters: Name Type Description Default random_state int, NumPy (legacy) RandomState object Due to scikit-learn compatibility, we use NumPy's legacy API. required literal bool Whether to use the literal backend (unoptimized but close to the main reference, uses the Laplace approximation\u2013based mixing training). May be used to check whether a new implementation's/idea's behaviour is still close to the original reference. False add_bias bool Whether to add an all-ones bias column to the input data. True phi callable Mixing feature extractor (N \u00d7 D_X \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . None fit_mixing str Only applies if literal is False . How mixing weights should be fitted. As of now, only \"laplace\" is suported (the original method from Drugowitsch's book , very slow and possibly suboptimal in terms of the variational bound) with a replacement being in the works. 'laplace' **kwargs kwargs Passed to the Mixture , Mixing and Rule constructors. May be used to override their default parameters. Can be accessed later on via the kwargs attribute. {} Source code in berbl/search/operators/__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , random_state , literal = False , add_bias = True , phi = None , fit_mixing = \"laplace\" , ** kwargs ): \"\"\" Parameters ---------- random_state : int, NumPy (legacy) RandomState object Due to scikit-learn compatibility, we use NumPy's legacy API. literal : bool Whether to use the literal backend (unoptimized but close to the main reference, uses the Laplace approximation\u2013based mixing training). May be used to check whether a new implementation's/idea's behaviour is still close to the original reference. add_bias : bool Whether to add an all-ones bias column to the input data. phi : callable Mixing feature extractor (N \u00d7 D_X \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. fit_mixing : str Only applies if ``literal`` is ``False``. How mixing weights should be fitted. As of now, only ``\"laplace\"`` is suported (the original method from [Drugowitsch's book](/), very slow and possibly suboptimal in terms of the variational bound) with a replacement being in the works. **kwargs : kwargs Passed to the ``Mixture``, ``Mixing`` and ``Rule`` constructors. May be used to override their default parameters. Can be accessed later on via the ``kwargs`` attribute. \"\"\" super () . __init__ () self . literal = literal self . random_state = check_random_state ( random_state ) self . kwargs = kwargs if self . literal : def _evaluate ( genotype , X , y ): genotype . phenotype = Model ( matchs = genotype , random_state = self . random_state , add_bias = add_bias , phi = phi ) . fit ( X , y ) return ( genotype . phenotype . p_M_D_ , ) else : def _evaluate ( genotype , X , y ): genotype . phenotype = Mixture ( matchs = genotype , random_state = self . random_state , add_bias = add_bias , phi = phi , fit_mixing = fit_mixing , ** self . kwargs ) . fit ( X , y ) return ( genotype . phenotype . p_M_D_ , ) self . register ( \"evaluate\" , _evaluate )","title":"operators"},{"location":"reference/berbl/search/operators/#berbl.search.operators.Toolbox","text":"Bases: base . Toolbox Base class for toolboxes that are used to perform model structure search. Defines and registers evaluate depending on the provided parameters. Parameters: Name Type Description Default random_state int, NumPy (legacy) RandomState object Due to scikit-learn compatibility, we use NumPy's legacy API. required literal bool Whether to use the literal backend (unoptimized but close to the main reference, uses the Laplace approximation\u2013based mixing training). May be used to check whether a new implementation's/idea's behaviour is still close to the original reference. False add_bias bool Whether to add an all-ones bias column to the input data. True phi callable Mixing feature extractor (N \u00d7 D_X \u2192 N \u00d7 DV); if None uses the default LCS mixing feature matrix based on phi(x) = 1 . None fit_mixing str Only applies if literal is False . How mixing weights should be fitted. As of now, only \"laplace\" is suported (the original method from Drugowitsch's book , very slow and possibly suboptimal in terms of the variational bound) with a replacement being in the works. 'laplace' **kwargs kwargs Passed to the Mixture , Mixing and Rule constructors. May be used to override their default parameters. Can be accessed later on via the kwargs attribute. {} Source code in berbl/search/operators/__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , random_state , literal = False , add_bias = True , phi = None , fit_mixing = \"laplace\" , ** kwargs ): \"\"\" Parameters ---------- random_state : int, NumPy (legacy) RandomState object Due to scikit-learn compatibility, we use NumPy's legacy API. literal : bool Whether to use the literal backend (unoptimized but close to the main reference, uses the Laplace approximation\u2013based mixing training). May be used to check whether a new implementation's/idea's behaviour is still close to the original reference. add_bias : bool Whether to add an all-ones bias column to the input data. phi : callable Mixing feature extractor (N \u00d7 D_X \u2192 N \u00d7 DV); if ``None`` uses the default LCS mixing feature matrix based on ``phi(x) = 1``. fit_mixing : str Only applies if ``literal`` is ``False``. How mixing weights should be fitted. As of now, only ``\"laplace\"`` is suported (the original method from [Drugowitsch's book](/), very slow and possibly suboptimal in terms of the variational bound) with a replacement being in the works. **kwargs : kwargs Passed to the ``Mixture``, ``Mixing`` and ``Rule`` constructors. May be used to override their default parameters. Can be accessed later on via the ``kwargs`` attribute. \"\"\" super () . __init__ () self . literal = literal self . random_state = check_random_state ( random_state ) self . kwargs = kwargs if self . literal : def _evaluate ( genotype , X , y ): genotype . phenotype = Model ( matchs = genotype , random_state = self . random_state , add_bias = add_bias , phi = phi ) . fit ( X , y ) return ( genotype . phenotype . p_M_D_ , ) else : def _evaluate ( genotype , X , y ): genotype . phenotype = Mixture ( matchs = genotype , random_state = self . random_state , add_bias = add_bias , phi = phi , fit_mixing = fit_mixing , ** self . kwargs ) . fit ( X , y ) return ( genotype . phenotype . p_M_D_ , ) self . register ( \"evaluate\" , _evaluate )","title":"Toolbox"},{"location":"reference/berbl/search/operators/drugowitsch/","text":"Search operators as defined in Drugowitsch's book. DefaultToolbox ( random_state , matchcls = SoftInterval1D , n = 100 , p = 0.5 , literal = False , add_bias = True , phi = None , tournsize = 5 , fit_mixing = 'laplace' , match_args = {}, ** kwargs ) Bases: Toolbox Toolbox specified in Drugowitsch's book . Extends the base toolbox (containing evaluate ) by providing gene , genotype , population , select , mate and mutate . Initializes this toolbox by creating and registering operators. Individuals are created by drawing their size from a binomial distribution and then . Parameters: Name Type Description Default random_state int, NumPy (legacy) RandomState object See berbl.search.operators.Toolbox. init . required matchcls object Matching function class to be used. By default, SoftInterval1D . SoftInterval1D n int n parameter (number of independent experiments) of the binomial distribution from which initial individual sizes are drawn. 100 p float p parameter (success rate) of the binomial distribution from which initial individual sizes are drawn. 0.5 literal bool See berbl.search.operators.Toolbox. init . False add_bias bool See berbl.search.operators.Toolbox. init . True phi callable See berbl.search.operators.Toolbox. init . None tournsize int Size of the tournaments used in the select operator. 5 fit_mixing str See berbl.search.operators.Toolbox. init . 'laplace' Source code in berbl/search/operators/drugowitsch.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , random_state , matchcls = SoftInterval1D , n = 100 , p = 0.5 , literal = False , add_bias = True , phi = None , tournsize = 5 , fit_mixing = \"laplace\" , match_args = {}, ** kwargs ): \"\"\" Initializes this toolbox by creating and registering operators. Individuals are created by drawing their size from a binomial distribution and then . Parameters ---------- random_state : int, NumPy (legacy) RandomState object See [berbl.search.operators.Toolbox.__init__][]. matchcls : object Matching function class to be used. By default, [``SoftInterval1D``](/berbl.match.softinterval1d_drugowitsch.SoftInterval1D). n : int, > 0 n parameter (number of independent experiments) of the binomial distribution from which initial individual sizes are drawn. p : float p parameter (success rate) of the binomial distribution from which initial individual sizes are drawn. literal : bool See [berbl.search.operators.Toolbox.__init__][]. add_bias : bool See [berbl.search.operators.Toolbox.__init__][]. phi : callable See [berbl.search.operators.Toolbox.__init__][]. tournsize : int, > 1 Size of the tournaments used in the ``select`` operator. fit_mixing : str See [berbl.search.operators.Toolbox.__init__][]. \"\"\" super () . __init__ ( literal = literal , add_bias = add_bias , phi = phi , fit_mixing = fit_mixing , random_state = random_state , ** kwargs ) self . register ( \"gene\" , matchcls . random , random_state = self . random_state , ** match_args ) self . register ( \"genotype\" , initRepeat_binom , creator . Genotype , self . gene , n = n , p = p , random_state = self . random_state ) self . register ( \"population\" , tools . initRepeat , list , self . genotype ) # \u201cWe create a new population by selecting two individuals (\u2026) To avoid # the influence of fitness scaling, we select individuals from the # current population by deterministic tournament selection with # tournament size ts.\u201d # [PDF p. 249] self . register ( \"select\" , tools . selTournament , k = 2 , tournsize = tournsize ) self . register ( \"mate\" , crossover ) self . register ( \"mutate\" , mutate ) crossover ( M_a , M_b , random_state ) Drugowitsch's simple diadic crossover operator. See Drugowitsch's book . Draw two new sizes for the offspring individuals and then randomly distribute the parent's genes among them. Parameters: Name Type Description Default M_a list First individual to crossover. required M_b list Second individual to crossover. required Returns: Type Description pair of objects Two new (unfitted) models resulting from crossover of the inputs. Source code in berbl/search/operators/drugowitsch.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def crossover ( M_a : List , M_b : List , random_state : np . random . RandomState ): \"\"\" Drugowitsch's simple diadic crossover operator. See [Drugowitsch's book](/). Draw two new sizes for the offspring individuals and then randomly distribute the parent's genes among them. Parameters ---------- M_a : list First individual to crossover. M_b : list Second individual to crossover. Returns ------- pair of objects Two new (unfitted) models resulting from crossover of the inputs. \"\"\" # \u201cAs two selected individuals can be of different length, we cannot apply # standard uniform cross-over but have to use different means: we want the # total number of [rules] to remain unchanged, but as the location of # the [rules] in the genome of an individual do not provide us with # any information, we allow their location to change.\u201d K_a = len ( M_a ) K_b = len ( M_b ) M_a_ = M_a + M_b random_state . shuffle ( M_a_ ) K_b_ = random_state . randint ( low = 1 , high = K_a + K_b ) M_b_ = [] for k in range ( K_b_ ): i = random_state . randint ( low = 0 , high = len ( M_a_ )) M_b_ . append ( M_a_ [ i ]) del M_a_ [ i ] assert K_a + K_b - K_b_ == len ( M_a_ ) assert K_b_ == len ( M_b_ ) assert K_a + K_b == len ( M_b_ ) + len ( M_a_ ) # TODO To use this with DEAP builtin algorithms, modify in place # Use M_a.extend([1] * \u2026) and del M_b[k:], then M_a[:] = M_a_ etc. return ( M_a_ , M_b_ ) mutate ( matchs , random_state ) Drugowitsch's individual-level mutation operator. See Drugowitsch's book . Go once over the individual and call each gene's mutate method with the provided random state. Note that the genes are not copied which means that in-place alterations of them cannot be ruled out. Parameters: Name Type Description Default matchs list of objects An individual consisting of genes, each gene having a mutate method that expects an np.random.RandomState . required Returns: Type Description tuple of object The mutated individual wrapped in a one-tuple (DEAP specification). Source code in berbl/search/operators/drugowitsch.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def mutate ( matchs , random_state : np . random . RandomState ): \"\"\" Drugowitsch's individual-level mutation operator. See [Drugowitsch's book](/). Go once over the individual and call each gene's `mutate` method with the provided random state. Note that the genes are *not* copied which means that in-place alterations of them cannot be ruled out. Parameters ---------- matchs : list of objects An individual consisting of genes, each gene having a `mutate` method that expects an `np.random.RandomState`. Returns ------- tuple of object The mutated individual wrapped in a one-tuple (DEAP specification). \"\"\" return [ m . mutate ( random_state ) for m in matchs ],","title":"drugowitsch"},{"location":"reference/berbl/search/operators/drugowitsch/#berbl.search.operators.drugowitsch.DefaultToolbox","text":"Bases: Toolbox Toolbox specified in Drugowitsch's book . Extends the base toolbox (containing evaluate ) by providing gene , genotype , population , select , mate and mutate . Initializes this toolbox by creating and registering operators. Individuals are created by drawing their size from a binomial distribution and then . Parameters: Name Type Description Default random_state int, NumPy (legacy) RandomState object See berbl.search.operators.Toolbox. init . required matchcls object Matching function class to be used. By default, SoftInterval1D . SoftInterval1D n int n parameter (number of independent experiments) of the binomial distribution from which initial individual sizes are drawn. 100 p float p parameter (success rate) of the binomial distribution from which initial individual sizes are drawn. 0.5 literal bool See berbl.search.operators.Toolbox. init . False add_bias bool See berbl.search.operators.Toolbox. init . True phi callable See berbl.search.operators.Toolbox. init . None tournsize int Size of the tournaments used in the select operator. 5 fit_mixing str See berbl.search.operators.Toolbox. init . 'laplace' Source code in berbl/search/operators/drugowitsch.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , random_state , matchcls = SoftInterval1D , n = 100 , p = 0.5 , literal = False , add_bias = True , phi = None , tournsize = 5 , fit_mixing = \"laplace\" , match_args = {}, ** kwargs ): \"\"\" Initializes this toolbox by creating and registering operators. Individuals are created by drawing their size from a binomial distribution and then . Parameters ---------- random_state : int, NumPy (legacy) RandomState object See [berbl.search.operators.Toolbox.__init__][]. matchcls : object Matching function class to be used. By default, [``SoftInterval1D``](/berbl.match.softinterval1d_drugowitsch.SoftInterval1D). n : int, > 0 n parameter (number of independent experiments) of the binomial distribution from which initial individual sizes are drawn. p : float p parameter (success rate) of the binomial distribution from which initial individual sizes are drawn. literal : bool See [berbl.search.operators.Toolbox.__init__][]. add_bias : bool See [berbl.search.operators.Toolbox.__init__][]. phi : callable See [berbl.search.operators.Toolbox.__init__][]. tournsize : int, > 1 Size of the tournaments used in the ``select`` operator. fit_mixing : str See [berbl.search.operators.Toolbox.__init__][]. \"\"\" super () . __init__ ( literal = literal , add_bias = add_bias , phi = phi , fit_mixing = fit_mixing , random_state = random_state , ** kwargs ) self . register ( \"gene\" , matchcls . random , random_state = self . random_state , ** match_args ) self . register ( \"genotype\" , initRepeat_binom , creator . Genotype , self . gene , n = n , p = p , random_state = self . random_state ) self . register ( \"population\" , tools . initRepeat , list , self . genotype ) # \u201cWe create a new population by selecting two individuals (\u2026) To avoid # the influence of fitness scaling, we select individuals from the # current population by deterministic tournament selection with # tournament size ts.\u201d # [PDF p. 249] self . register ( \"select\" , tools . selTournament , k = 2 , tournsize = tournsize ) self . register ( \"mate\" , crossover ) self . register ( \"mutate\" , mutate )","title":"DefaultToolbox"},{"location":"reference/berbl/search/operators/drugowitsch/#berbl.search.operators.drugowitsch.crossover","text":"Drugowitsch's simple diadic crossover operator. See Drugowitsch's book . Draw two new sizes for the offspring individuals and then randomly distribute the parent's genes among them. Parameters: Name Type Description Default M_a list First individual to crossover. required M_b list Second individual to crossover. required Returns: Type Description pair of objects Two new (unfitted) models resulting from crossover of the inputs. Source code in berbl/search/operators/drugowitsch.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def crossover ( M_a : List , M_b : List , random_state : np . random . RandomState ): \"\"\" Drugowitsch's simple diadic crossover operator. See [Drugowitsch's book](/). Draw two new sizes for the offspring individuals and then randomly distribute the parent's genes among them. Parameters ---------- M_a : list First individual to crossover. M_b : list Second individual to crossover. Returns ------- pair of objects Two new (unfitted) models resulting from crossover of the inputs. \"\"\" # \u201cAs two selected individuals can be of different length, we cannot apply # standard uniform cross-over but have to use different means: we want the # total number of [rules] to remain unchanged, but as the location of # the [rules] in the genome of an individual do not provide us with # any information, we allow their location to change.\u201d K_a = len ( M_a ) K_b = len ( M_b ) M_a_ = M_a + M_b random_state . shuffle ( M_a_ ) K_b_ = random_state . randint ( low = 1 , high = K_a + K_b ) M_b_ = [] for k in range ( K_b_ ): i = random_state . randint ( low = 0 , high = len ( M_a_ )) M_b_ . append ( M_a_ [ i ]) del M_a_ [ i ] assert K_a + K_b - K_b_ == len ( M_a_ ) assert K_b_ == len ( M_b_ ) assert K_a + K_b == len ( M_b_ ) + len ( M_a_ ) # TODO To use this with DEAP builtin algorithms, modify in place # Use M_a.extend([1] * \u2026) and del M_b[k:], then M_a[:] = M_a_ etc. return ( M_a_ , M_b_ )","title":"crossover()"},{"location":"reference/berbl/search/operators/drugowitsch/#berbl.search.operators.drugowitsch.mutate","text":"Drugowitsch's individual-level mutation operator. See Drugowitsch's book . Go once over the individual and call each gene's mutate method with the provided random state. Note that the genes are not copied which means that in-place alterations of them cannot be ruled out. Parameters: Name Type Description Default matchs list of objects An individual consisting of genes, each gene having a mutate method that expects an np.random.RandomState . required Returns: Type Description tuple of object The mutated individual wrapped in a one-tuple (DEAP specification). Source code in berbl/search/operators/drugowitsch.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def mutate ( matchs , random_state : np . random . RandomState ): \"\"\" Drugowitsch's individual-level mutation operator. See [Drugowitsch's book](/). Go once over the individual and call each gene's `mutate` method with the provided random state. Note that the genes are *not* copied which means that in-place alterations of them cannot be ruled out. Parameters ---------- matchs : list of objects An individual consisting of genes, each gene having a `mutate` method that expects an `np.random.RandomState`. Returns ------- tuple of object The mutated individual wrapped in a one-tuple (DEAP specification). \"\"\" return [ m . mutate ( random_state ) for m in matchs ],","title":"mutate()"}]}